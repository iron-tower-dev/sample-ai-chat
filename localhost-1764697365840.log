 [MarkdownContent] Content changed: Thinking
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "\n<think>"
 [LLM API] tagBuffer: 
<think>
 [LLM API] Entered <think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 0, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(1), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 0, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "The"
 [LLM API] tagBuffer: The
 [LLM API] Accumulated thinking: 3 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(2), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3, toolingLength: 0, responseLength: 0}
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " user"
 [LLM API] tagBuffer:  user
 [LLM API] Accumulated thinking: 8 chars
 [LLM API] Calling onChunk with: {thinkingLength: 8, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(3), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 8, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated thinking: 11 chars
 [LLM API] Calling onChunk with: {thinkingLength: 11, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(4), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 11, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " asking"
 [LLM API] tagBuffer:  asking
 [LLM API] Accumulated thinking: 18 chars
 [LLM API] Calling onChunk with: {thinkingLength: 18, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(5), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 18, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " about"
 [LLM API] tagBuffer:  about
 [LLM API] Accumulated thinking: 24 chars
 [LLM API] Calling onChunk with: {thinkingLength: 24, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(6), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 24, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated thinking: 30 chars
 [LLM API] Calling onChunk with: {thinkingLength: 30, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(7), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 30, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 40 chars
 [LLM API] Calling onChunk with: {thinkingLength: 40, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(8), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 40, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " manages"
 [LLM API] tagBuffer:  manages
 [LLM API] Accumulated thinking: 48 chars
 [LLM API] Calling onChunk with: {thinkingLength: 48, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(9), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 48, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 51 chars
 [LLM API] Calling onChunk with: {thinkingLength: 51, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(10), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 51, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 54 chars
 [LLM API] Calling onChunk with: {thinkingLength: 54, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(11), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 54, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 59 chars
 [LLM API] Calling onChunk with: {thinkingLength: 59, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(12), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 59, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 66 chars
 [LLM API] Calling onChunk with: {thinkingLength: 66, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(13), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 66, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 67 chars
 [LLM API] Calling onChunk with: {thinkingLength: 67, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(14), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 67, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " This"
 [LLM API] tagBuffer:  This
 [LLM API] Accumulated thinking: 72 chars
 [LLM API] Calling onChunk with: {thinkingLength: 72, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(15), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 72, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " seems"
 [LLM API] tagBuffer:  seems
 [LLM API] Accumulated thinking: 78 chars
 [LLM API] Calling onChunk with: {thinkingLength: 78, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(16), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 78, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 81 chars
 [LLM API] Calling onChunk with: {thinkingLength: 81, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(17), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 81, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated thinking: 84 chars
 [LLM API] Calling onChunk with: {thinkingLength: 84, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(18), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 84, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " a"
 [LLM API] tagBuffer:  a
 [LLM API] Accumulated thinking: 86 chars
 [LLM API] Calling onChunk with: {thinkingLength: 86, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(19), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 86, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " question"
 [LLM API] tagBuffer:  question
 [LLM API] Accumulated thinking: 95 chars
 [LLM API] Calling onChunk with: {thinkingLength: 95, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(20), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 95, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " about"
 [LLM API] tagBuffer:  about
 [LLM API] Accumulated thinking: 101 chars
 [LLM API] Calling onChunk with: {thinkingLength: 101, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(21), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 101, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated thinking: 112 chars
 [LLM API] Calling onChunk with: {thinkingLength: 112, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(22), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 112, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " at"
 [LLM API] tagBuffer:  at
 [LLM API] Accumulated thinking: 115 chars
 [LLM API] Calling onChunk with: {thinkingLength: 115, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(23), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 115, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Palo"
 [LLM API] tagBuffer:  Palo
 [LLM API] Accumulated thinking: 120 chars
 [LLM API] Calling onChunk with: {thinkingLength: 120, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(24), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 120, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated thinking: 126 chars
 [LLM API] Calling onChunk with: {thinkingLength: 126, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(25), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 126, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Nuclear"
 [LLM API] tagBuffer:  Nuclear
 [LLM API] Accumulated thinking: 134 chars
 [LLM API] Calling onChunk with: {thinkingLength: 134, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(26), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 134, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Generating"
 [LLM API] tagBuffer:  Generating
 [LLM API] Accumulated thinking: 145 chars
 [LLM API] Calling onChunk with: {thinkingLength: 145, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(27), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 145, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Station"
 [LLM API] tagBuffer:  Station
 [LLM API] Accumulated thinking: 153 chars
 [LLM API] Calling onChunk with: {thinkingLength: 153, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(28), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 153, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 154 chars
 [LLM API] Calling onChunk with: {thinkingLength: 154, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(29), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 154, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " This"
 [LLM API] tagBuffer:  This
 [LLM API] Accumulated thinking: 159 chars
 [LLM API] Calling onChunk with: {thinkingLength: 159, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(30), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 159, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " would"
 [LLM API] tagBuffer:  would
 [LLM API] Accumulated thinking: 165 chars
 [LLM API] Calling onChunk with: {thinkingLength: 165, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(31), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 165, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " likely"
 [LLM API] tagBuffer:  likely
 [LLM API] Accumulated thinking: 172 chars
 [LLM API] Calling onChunk with: {thinkingLength: 172, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(32), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 172, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated thinking: 175 chars
 [LLM API] Calling onChunk with: {thinkingLength: 175, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(33), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 175, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " found"
 [LLM API] tagBuffer:  found
 [LLM API] Accumulated thinking: 181 chars
 [LLM API] Calling onChunk with: {thinkingLength: 181, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(34), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 181, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " in"
 [LLM API] tagBuffer:  in
 [LLM API] Accumulated thinking: 184 chars
 [LLM API] Calling onChunk with: {thinkingLength: 184, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(35), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 184, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 188 chars
 [LLM API] Calling onChunk with: {thinkingLength: 188, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(36), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 188, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " e"
 [LLM API] tagBuffer:  e
 [LLM API] Accumulated thinking: 190 chars
 [LLM API] Calling onChunk with: {thinkingLength: 190, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(37), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 190, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Doc"
 [LLM API] tagBuffer: Doc
 [LLM API] Accumulated thinking: 193 chars
 [LLM API] Calling onChunk with: {thinkingLength: 193, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(38), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 193, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " collection"
 [LLM API] tagBuffer:  collection
 [LLM API] Accumulated thinking: 204 chars
 [LLM API] Calling onChunk with: {thinkingLength: 204, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(39), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 204, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " since"
 [LLM API] tagBuffer:  since
 [LLM API] Accumulated thinking: 210 chars
 [LLM API] Calling onChunk with: {thinkingLength: 210, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(40), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 210, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " they"
 [LLM API] tagBuffer:  they
 [LLM API] Accumulated thinking: 215 chars
 [LLM API] Calling onChunk with: {thinkingLength: 215, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(41), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 215, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "'re"
 [LLM API] tagBuffer: 're
 [LLM API] Accumulated thinking: 218 chars
 [LLM API] Calling onChunk with: {thinkingLength: 218, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(42), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 218, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " looking"
 [LLM API] tagBuffer:  looking
 [LLM API] Accumulated thinking: 226 chars
 [LLM API] Calling onChunk with: {thinkingLength: 226, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(43), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 226, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 230 chars
 [LLM API] Calling onChunk with: {thinkingLength: 230, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(44), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 230, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated thinking: 241 chars
 [LLM API] Calling onChunk with: {thinkingLength: 241, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(45), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 241, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 242 chars
 [LLM API] Calling onChunk with: {thinkingLength: 242, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(46), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 242, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " I"
 [LLM API] tagBuffer:  I
 [LLM API] Accumulated thinking: 244 chars
 [LLM API] Calling onChunk with: {thinkingLength: 244, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(47), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 244, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " should"
 [LLM API] tagBuffer:  should
 [LLM API] Accumulated thinking: 251 chars
 [LLM API] Calling onChunk with: {thinkingLength: 251, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(48), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 251, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " search"
 [LLM API] tagBuffer:  search
 [LLM API] Accumulated thinking: 258 chars
 [LLM API] Calling onChunk with: {thinkingLength: 258, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(49), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 258, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 262 chars
 [LLM API] Calling onChunk with: {thinkingLength: 262, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(50), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 262, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " documents"
 [LLM API] tagBuffer:  documents
 [LLM API] Accumulated thinking: 272 chars
 [LLM API] Calling onChunk with: {thinkingLength: 272, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(51), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 272, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " related"
 [LLM API] tagBuffer:  related
 [LLM API] Accumulated thinking: 280 chars
 [LLM API] Calling onChunk with: {thinkingLength: 280, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(52), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 280, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 283 chars
 [LLM API] Calling onChunk with: {thinkingLength: 283, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(53), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 283, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 286 chars
 [LLM API] Calling onChunk with: {thinkingLength: 286, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(54), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 286, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 289 chars
 [LLM API] Calling onChunk with: {thinkingLength: 289, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(55), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 289, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 294 chars
 [LLM API] Calling onChunk with: {thinkingLength: 294, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(56), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 294, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 301 chars
 [LLM API] Calling onChunk with: {thinkingLength: 301, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(57), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 301, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 308 chars
 [LLM API] Calling onChunk with: {thinkingLength: 308, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(58), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 308, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " control"
 [LLM API] tagBuffer:  control
 [LLM API] Accumulated thinking: 316 chars
 [LLM API] Calling onChunk with: {thinkingLength: 316, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(59), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 316, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated thinking: 327 chars
 [LLM API] Calling onChunk with: {thinkingLength: 327, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(60), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 327, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ".\n\n"
 [LLM API] tagBuffer: .


 [LLM API] Accumulated thinking: 330 chars
 [LLM API] Calling onChunk with: {thinkingLength: 330, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(61), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 330, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Let"
 [LLM API] tagBuffer: Let
 [LLM API] Accumulated thinking: 333 chars
 [LLM API] Calling onChunk with: {thinkingLength: 333, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(62), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 333, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " me"
 [LLM API] tagBuffer:  me
 [LLM API] Accumulated thinking: 336 chars
 [LLM API] Calling onChunk with: {thinkingLength: 336, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(63), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 336, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " search"
 [LLM API] tagBuffer:  search
 [LLM API] Accumulated thinking: 343 chars
 [LLM API] Calling onChunk with: {thinkingLength: 343, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(64), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 343, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 347 chars
 [LLM API] Calling onChunk with: {thinkingLength: 347, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(65), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 347, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " e"
 [LLM API] tagBuffer:  e
 [LLM API] Accumulated thinking: 349 chars
 [LLM API] Calling onChunk with: {thinkingLength: 349, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(66), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 349, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Doc"
 [LLM API] tagBuffer: Doc
 [LLM API] Accumulated thinking: 352 chars
 [LLM API] Calling onChunk with: {thinkingLength: 352, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(67), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 352, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " collection"
 [LLM API] tagBuffer:  collection
 [LLM API] Accumulated thinking: 363 chars
 [LLM API] Calling onChunk with: {thinkingLength: 363, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(68), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 363, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 367 chars
 [LLM API] Calling onChunk with: {thinkingLength: 367, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(69), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 367, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " documents"
 [LLM API] tagBuffer:  documents
 [LLM API] Accumulated thinking: 377 chars
 [LLM API] Calling onChunk with: {thinkingLength: 377, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(70), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 377, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " related"
 [LLM API] tagBuffer:  related
 [LLM API] Accumulated thinking: 385 chars
 [LLM API] Calling onChunk with: {thinkingLength: 385, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(71), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 385, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 388 chars
 [LLM API] Calling onChunk with: {thinkingLength: 388, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(72), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 388, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 391 chars
 [LLM API] Calling onChunk with: {thinkingLength: 391, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(73), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 391, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 394 chars
 [LLM API] Calling onChunk with: {thinkingLength: 394, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(74), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 394, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 399 chars
 [LLM API] Calling onChunk with: {thinkingLength: 399, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(75), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 399, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 406 chars
 [LLM API] Calling onChunk with: {thinkingLength: 406, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(76), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 406, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 407 chars
 [LLM API] Calling onChunk with: {thinkingLength: 407, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(77), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 407, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "</think>"
 [LLM API] tagBuffer: </think>
 [LLM API] Exited </think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 407, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(78), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 407, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Keeping potential partial tag in buffer: 

 [LLM API] Calling onChunk with: {thinkingLength: 407, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(79), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 407, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: tool: "{\"action\": \"searching for documents rela
 [LLM API] FOUND TOOL EVENT, line: tool: "{\"action\": \"searching for documents related to unescorted access\"}"
 [LLM API] Tool string to parse: "{\"action\": \"searching for documents related to unescorted access\"}"
 [LLM API] Parsed tool JSON: {"action": "searching for documents related to unescorted access"}
 [LLM API] Processing line: data: "\n<think>"
 [LLM API] tagBuffer: 

<think>
 [LLM API] Entered <think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 407, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(80), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 407, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Based"
 [LLM API] tagBuffer: Based
 [LLM API] Accumulated thinking: 412 chars
 [LLM API] Calling onChunk with: {thinkingLength: 412, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(81), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 412, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " on"
 [LLM API] tagBuffer:  on
 [LLM API] Accumulated thinking: 415 chars
 [LLM API] Calling onChunk with: {thinkingLength: 415, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(82), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 415, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 419 chars
 [LLM API] Calling onChunk with: {thinkingLength: 419, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(83), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 419, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " search"
 [LLM API] tagBuffer:  search
 [LLM API] Accumulated thinking: 426 chars
 [LLM API] Calling onChunk with: {thinkingLength: 426, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(84), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 426, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " results"
 [LLM API] tagBuffer:  results
 [LLM API] Accumulated thinking: 434 chars
 [LLM API] Calling onChunk with: {thinkingLength: 434, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(85), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 434, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 435 chars
 [LLM API] Calling onChunk with: {thinkingLength: 435, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(86), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 435, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " I"
 [LLM API] tagBuffer:  I
 [LLM API] Accumulated thinking: 437 chars
 [LLM API] Calling onChunk with: {thinkingLength: 437, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(87), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 437, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " can"
 [LLM API] tagBuffer:  can
 [LLM API] Accumulated thinking: 441 chars
 [LLM API] Calling onChunk with: {thinkingLength: 441, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(88), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 441, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " see"
 [LLM API] tagBuffer:  see
 [LLM API] Accumulated thinking: 445 chars
 [LLM API] Calling onChunk with: {thinkingLength: 445, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(89), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 445, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " several"
 [LLM API] tagBuffer:  several
 [LLM API] Accumulated thinking: 453 chars
 [LLM API] Calling onChunk with: {thinkingLength: 453, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(90), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 453, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " documents"
 [LLM API] tagBuffer:  documents
 [LLM API] Accumulated thinking: 463 chars
 [LLM API] Calling onChunk with: {thinkingLength: 463, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(91), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 463, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " related"
 [LLM API] tagBuffer:  related
 [LLM API] Accumulated thinking: 471 chars
 [LLM API] Calling onChunk with: {thinkingLength: 471, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(92), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 471, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 474 chars
 [LLM API] Calling onChunk with: {thinkingLength: 474, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(93), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 474, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 477 chars
 [LLM API] Calling onChunk with: {thinkingLength: 477, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(94), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 477, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 480 chars
 [LLM API] Calling onChunk with: {thinkingLength: 480, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(95), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 480, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 485 chars
 [LLM API] Calling onChunk with: {thinkingLength: 485, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(96), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 485, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 492 chars
 [LLM API] Calling onChunk with: {thinkingLength: 492, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(97), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 492, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " at"
 [LLM API] tagBuffer:  at
 [LLM API] Accumulated thinking: 495 chars
 [LLM API] Calling onChunk with: {thinkingLength: 495, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(98), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 495, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Palo"
 [LLM API] tagBuffer:  Palo
 [LLM API] Accumulated thinking: 500 chars
 [LLM API] Calling onChunk with: {thinkingLength: 500, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(99), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 500, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated thinking: 506 chars
 [LLM API] Calling onChunk with: {thinkingLength: 506, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(100), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 506, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 507 chars
 [LLM API] Calling onChunk with: {thinkingLength: 507, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(101), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 507, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " The"
 [LLM API] tagBuffer:  The
 [LLM API] Accumulated thinking: 511 chars
 [LLM API] Calling onChunk with: {thinkingLength: 511, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(102), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 511, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " most"
 [LLM API] tagBuffer:  most
 [LLM API] Accumulated thinking: 516 chars
 [LLM API] Calling onChunk with: {thinkingLength: 516, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(103), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 516, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " relevant"
 [LLM API] tagBuffer:  relevant
 [LLM API] Accumulated thinking: 525 chars
 [LLM API] Calling onChunk with: {thinkingLength: 525, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(104), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 525, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated thinking: 536 chars
 [LLM API] Calling onChunk with: {thinkingLength: 536, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(105), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 536, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " appear"
 [LLM API] tagBuffer:  appear
 [LLM API] Accumulated thinking: 543 chars
 [LLM API] Calling onChunk with: {thinkingLength: 543, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(106), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 543, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 546 chars
 [LLM API] Calling onChunk with: {thinkingLength: 546, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(107), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 546, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated thinking: 549 chars
 [LLM API] Calling onChunk with: {thinkingLength: 549, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(108), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 549, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ":\n\n"
 [LLM API] tagBuffer: :


 [LLM API] Accumulated thinking: 552 chars
 [LLM API] Calling onChunk with: {thinkingLength: 552, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(109), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 552, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "1"
 [LLM API] tagBuffer: 1
 [LLM API] Accumulated thinking: 553 chars
 [LLM API] Calling onChunk with: {thinkingLength: 553, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(110), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 553, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 554 chars
 [LLM API] Calling onChunk with: {thinkingLength: 554, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(111), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 554, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 555 chars
 [LLM API] Calling onChunk with: {thinkingLength: 555, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(112), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 555, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 557 chars
 [LLM API] Calling onChunk with: {thinkingLength: 557, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(113), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 557, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 559 chars
 [LLM API] Calling onChunk with: {thinkingLength: 559, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(114), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 559, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 560 chars
 [LLM API] Calling onChunk with: {thinkingLength: 560, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(115), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 560, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 561 chars
 [LLM API] Calling onChunk with: {thinkingLength: 561, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(116), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 561, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 563 chars
 [LLM API] Calling onChunk with: {thinkingLength: 563, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(117), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 563, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "11"
 [LLM API] tagBuffer: 11
 [LLM API] Accumulated thinking: 565 chars
 [LLM API] Calling onChunk with: {thinkingLength: 565, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(118), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 565, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 567 chars
 [LLM API] Calling onChunk with: {thinkingLength: 567, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(119), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 567, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 569 chars
 [LLM API] Calling onChunk with: {thinkingLength: 569, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(120), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 569, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "BACKGROUND"
 [LLM API] tagBuffer: BACKGROUND
 [LLM API] Accumulated thinking: 579 chars
 [LLM API] Calling onChunk with: {thinkingLength: 579, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(121), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 579, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " IN"
 [LLM API] tagBuffer:  IN
 [LLM API] Accumulated thinking: 582 chars
 [LLM API] Calling onChunk with: {thinkingLength: 582, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(122), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 582, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "VEST"
 [LLM API] tagBuffer: VEST
 [LLM API] Accumulated thinking: 586 chars
 [LLM API] Calling onChunk with: {thinkingLength: 586, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(123), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 586, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "IG"
 [LLM API] tagBuffer: IG
 [LLM API] Accumulated thinking: 588 chars
 [LLM API] Calling onChunk with: {thinkingLength: 588, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(124), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 588, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ATIONS"
 [LLM API] tagBuffer: ATIONS
 [LLM API] Accumulated thinking: 594 chars
 [LLM API] Calling onChunk with: {thinkingLength: 594, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(125), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 594, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " FOR"
 [LLM API] tagBuffer:  FOR
 [LLM API] Accumulated thinking: 598 chars
 [LLM API] Calling onChunk with: {thinkingLength: 598, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(126), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 598, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " UN"
 [LLM API] tagBuffer:  UN
 [LLM API] Accumulated thinking: 601 chars
 [LLM API] Calling onChunk with: {thinkingLength: 601, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(127), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 601, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ESC"
 [LLM API] tagBuffer: ESC
 [LLM API] Accumulated thinking: 604 chars
 [LLM API] Calling onChunk with: {thinkingLength: 604, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(128), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 604, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ORT"
 [LLM API] tagBuffer: ORT
 [LLM API] Accumulated thinking: 607 chars
 [LLM API] Calling onChunk with: {thinkingLength: 607, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(129), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 607, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ED"
 [LLM API] tagBuffer: ED
 [LLM API] Accumulated thinking: 609 chars
 [LLM API] Calling onChunk with: {thinkingLength: 609, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(130), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 609, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ACCESS"
 [LLM API] tagBuffer:  ACCESS
 [LLM API] Accumulated thinking: 616 chars
 [LLM API] Calling onChunk with: {thinkingLength: 616, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(131), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 616, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 617 chars
 [LLM API] Calling onChunk with: {thinkingLength: 617, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(132), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 617, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 619 chars
 [LLM API] Calling onChunk with: {thinkingLength: 619, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(133), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 619, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Rev"
 [LLM API] tagBuffer: Rev
 [LLM API] Accumulated thinking: 622 chars
 [LLM API] Calling onChunk with: {thinkingLength: 622, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(134), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 622, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 623 chars
 [LLM API] Calling onChunk with: {thinkingLength: 623, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(135), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 623, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 624 chars
 [LLM API] Calling onChunk with: {thinkingLength: 624, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(136), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 624, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 625 chars
 [LLM API] Calling onChunk with: {thinkingLength: 625, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(137), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 625, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "11"
 [LLM API] tagBuffer: 11
 [LLM API] Accumulated thinking: 627 chars
 [LLM API] Calling onChunk with: {thinkingLength: 627, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(138), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 627, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")"
 [LLM API] tagBuffer: )
 [LLM API] Accumulated thinking: 628 chars
 [LLM API] Calling onChunk with: {thinkingLength: 628, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(139), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 628, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 630 chars
 [LLM API] Calling onChunk with: {thinkingLength: 630, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(140), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 630, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " This"
 [LLM API] tagBuffer:  This
 [LLM API] Accumulated thinking: 635 chars
 [LLM API] Calling onChunk with: {thinkingLength: 635, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(141), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 635, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated thinking: 638 chars
 [LLM API] Calling onChunk with: {thinkingLength: 638, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(142), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 638, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " about"
 [LLM API] tagBuffer:  about
 [LLM API] Accumulated thinking: 644 chars
 [LLM API] Calling onChunk with: {thinkingLength: 644, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(143), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 644, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " background"
 [LLM API] tagBuffer:  background
 [LLM API] Accumulated thinking: 655 chars
 [LLM API] Calling onChunk with: {thinkingLength: 655, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(144), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 655, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " investigations"
 [LLM API] tagBuffer:  investigations
 [LLM API] Accumulated thinking: 670 chars
 [LLM API] Calling onChunk with: {thinkingLength: 670, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(145), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 670, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 674 chars
 [LLM API] Calling onChunk with: {thinkingLength: 674, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(146), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 674, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 677 chars
 [LLM API] Calling onChunk with: {thinkingLength: 677, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(147), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 677, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 680 chars
 [LLM API] Calling onChunk with: {thinkingLength: 680, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(148), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 680, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 685 chars
 [LLM API] Calling onChunk with: {thinkingLength: 685, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(149), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 685, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 692 chars
 [LLM API] Calling onChunk with: {thinkingLength: 692, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(150), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 692, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 693 chars
 [LLM API] Calling onChunk with: {thinkingLength: 693, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(151), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 693, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "2"
 [LLM API] tagBuffer: 2
 [LLM API] Accumulated thinking: 694 chars
 [LLM API] Calling onChunk with: {thinkingLength: 694, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(152), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 694, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 695 chars
 [LLM API] Calling onChunk with: {thinkingLength: 695, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(153), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 695, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 696 chars
 [LLM API] Calling onChunk with: {thinkingLength: 696, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(154), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 696, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 698 chars
 [LLM API] Calling onChunk with: {thinkingLength: 698, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(155), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 698, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 700 chars
 [LLM API] Calling onChunk with: {thinkingLength: 700, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(156), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 700, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 701 chars
 [LLM API] Calling onChunk with: {thinkingLength: 701, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(157), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 701, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 702 chars
 [LLM API] Calling onChunk with: {thinkingLength: 702, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(158), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 702, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 704 chars
 [LLM API] Calling onChunk with: {thinkingLength: 704, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(159), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 704, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated thinking: 706 chars
 [LLM API] Calling onChunk with: {thinkingLength: 706, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(160), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 706, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 708 chars
 [LLM API] Calling onChunk with: {thinkingLength: 708, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(161), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 708, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 710 chars
 [LLM API] Calling onChunk with: {thinkingLength: 710, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(162), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 710, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "P"
 [LLM API] tagBuffer: P
 [LLM API] Accumulated thinking: 711 chars
 [LLM API] Calling onChunk with: {thinkingLength: 711, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(163), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 711, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "alo"
 [LLM API] tagBuffer: alo
 [LLM API] Accumulated thinking: 714 chars
 [LLM API] Calling onChunk with: {thinkingLength: 714, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(164), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 714, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated thinking: 720 chars
 [LLM API] Calling onChunk with: {thinkingLength: 720, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(165), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 720, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Bad"
 [LLM API] tagBuffer:  Bad
 [LLM API] Accumulated thinking: 724 chars
 [LLM API] Calling onChunk with: {thinkingLength: 724, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(166), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 724, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ging"
 [LLM API] tagBuffer: ging
 [LLM API] Accumulated thinking: 728 chars
 [LLM API] Calling onChunk with: {thinkingLength: 728, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(167), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 728, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Procedure"
 [LLM API] tagBuffer:  Procedure
 [LLM API] Accumulated thinking: 738 chars
 [LLM API] Calling onChunk with: {thinkingLength: 738, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(168), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 738, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 739 chars
 [LLM API] Calling onChunk with: {thinkingLength: 739, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(169), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 739, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 741 chars
 [LLM API] Calling onChunk with: {thinkingLength: 741, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(170), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 741, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "multiple"
 [LLM API] tagBuffer: multiple
 [LLM API] Accumulated thinking: 749 chars
 [LLM API] Calling onChunk with: {thinkingLength: 749, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(171), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 749, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " revisions"
 [LLM API] tagBuffer:  revisions
 [LLM API] Accumulated thinking: 759 chars
 [LLM API] Calling onChunk with: {thinkingLength: 759, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(172), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 759, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")"
 [LLM API] tagBuffer: )
 [LLM API] Accumulated thinking: 760 chars
 [LLM API] Calling onChunk with: {thinkingLength: 760, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(173), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 760, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 762 chars
 [LLM API] Calling onChunk with: {thinkingLength: 762, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(174), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 762, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " This"
 [LLM API] tagBuffer:  This
 [LLM API] Accumulated thinking: 767 chars
 [LLM API] Calling onChunk with: {thinkingLength: 767, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(175), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 767, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " appears"
 [LLM API] tagBuffer:  appears
 [LLM API] Accumulated thinking: 775 chars
 [LLM API] Calling onChunk with: {thinkingLength: 775, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(176), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 775, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 778 chars
 [LLM API] Calling onChunk with: {thinkingLength: 778, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(177), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 778, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " manage"
 [LLM API] tagBuffer:  manage
 [LLM API] Accumulated thinking: 785 chars
 [LLM API] Calling onChunk with: {thinkingLength: 785, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(178), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 785, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 789 chars
 [LLM API] Calling onChunk with: {thinkingLength: 789, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(179), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 789, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " AC"
 [LLM API] tagBuffer:  AC
 [LLM API] Accumulated thinking: 792 chars
 [LLM API] Calling onChunk with: {thinkingLength: 792, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(180), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 792, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "AD"
 [LLM API] tagBuffer: AD
 [LLM API] Accumulated thinking: 794 chars
 [LLM API] Calling onChunk with: {thinkingLength: 794, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(181), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 794, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Badge"
 [LLM API] tagBuffer:  Badge
 [LLM API] Accumulated thinking: 800 chars
 [LLM API] Calling onChunk with: {thinkingLength: 800, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(182), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 800, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " process"
 [LLM API] tagBuffer:  process
 [LLM API] Accumulated thinking: 808 chars
 [LLM API] Calling onChunk with: {thinkingLength: 808, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(183), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 808, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 812 chars
 [LLM API] Calling onChunk with: {thinkingLength: 812, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(184), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 812, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 815 chars
 [LLM API] Calling onChunk with: {thinkingLength: 815, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(185), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 815, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 818 chars
 [LLM API] Calling onChunk with: {thinkingLength: 818, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(186), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 818, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 823 chars
 [LLM API] Calling onChunk with: {thinkingLength: 823, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(187), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 823, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 830 chars
 [LLM API] Calling onChunk with: {thinkingLength: 830, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(188), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 830, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 831 chars
 [LLM API] Calling onChunk with: {thinkingLength: 831, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(189), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 831, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "3"
 [LLM API] tagBuffer: 3
 [LLM API] Accumulated thinking: 832 chars
 [LLM API] Calling onChunk with: {thinkingLength: 832, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(190), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 832, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 833 chars
 [LLM API] Calling onChunk with: {thinkingLength: 833, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(191), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 833, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 834 chars
 [LLM API] Calling onChunk with: {thinkingLength: 834, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(192), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 834, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 836 chars
 [LLM API] Calling onChunk with: {thinkingLength: 836, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(193), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 836, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 838 chars
 [LLM API] Calling onChunk with: {thinkingLength: 838, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(194), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 838, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 839 chars
 [LLM API] Calling onChunk with: {thinkingLength: 839, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(195), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 839, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 840 chars
 [LLM API] Calling onChunk with: {thinkingLength: 840, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(196), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 840, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 842 chars
 [LLM API] Calling onChunk with: {thinkingLength: 842, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(197), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 842, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "40"
 [LLM API] tagBuffer: 40
 [LLM API] Accumulated thinking: 844 chars
 [LLM API] Calling onChunk with: {thinkingLength: 844, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(198), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 844, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 846 chars
 [LLM API] Calling onChunk with: {thinkingLength: 846, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(199), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 846, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 848 chars
 [LLM API] Calling onChunk with: {thinkingLength: 848, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(200), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 848, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "PV"
 [LLM API] tagBuffer: PV
 [LLM API] Accumulated thinking: 850 chars
 [LLM API] Calling onChunk with: {thinkingLength: 850, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(201), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 850, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "NG"
 [LLM API] tagBuffer: NG
 [LLM API] Accumulated thinking: 852 chars
 [LLM API] Calling onChunk with: {thinkingLength: 852, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(202), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 852, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "S"
 [LLM API] tagBuffer: S
 [LLM API] Accumulated thinking: 853 chars
 [LLM API] Calling onChunk with: {thinkingLength: 853, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(203), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 853, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " UN"
 [LLM API] tagBuffer:  UN
 [LLM API] Accumulated thinking: 856 chars
 [LLM API] Calling onChunk with: {thinkingLength: 856, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(204), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 856, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ESC"
 [LLM API] tagBuffer: ESC
 [LLM API] Accumulated thinking: 859 chars
 [LLM API] Calling onChunk with: {thinkingLength: 859, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(205), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 859, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ORT"
 [LLM API] tagBuffer: ORT
 [LLM API] Accumulated thinking: 862 chars
 [LLM API] Calling onChunk with: {thinkingLength: 862, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(206), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 862, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ED"
 [LLM API] tagBuffer: ED
 [LLM API] Accumulated thinking: 864 chars
 [LLM API] Calling onChunk with: {thinkingLength: 864, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(207), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 864, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ACCESS"
 [LLM API] tagBuffer:  ACCESS
 [LLM API] Accumulated thinking: 871 chars
 [LLM API] Calling onChunk with: {thinkingLength: 871, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(208), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 871, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " SCREEN"
 [LLM API] tagBuffer:  SCREEN
 [LLM API] Accumulated thinking: 878 chars
 [LLM API] Calling onChunk with: {thinkingLength: 878, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(209), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 878, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ING"
 [LLM API] tagBuffer: ING
 [LLM API] Accumulated thinking: 881 chars
 [LLM API] Calling onChunk with: {thinkingLength: 881, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(210), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 881, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 882 chars
 [LLM API] Calling onChunk with: {thinkingLength: 882, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(211), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 882, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 884 chars
 [LLM API] Calling onChunk with: {thinkingLength: 884, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(212), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 884, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Rev"
 [LLM API] tagBuffer: Rev
 [LLM API] Accumulated thinking: 887 chars
 [LLM API] Calling onChunk with: {thinkingLength: 887, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(213), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 887, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 888 chars
 [LLM API] Calling onChunk with: {thinkingLength: 888, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(214), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 888, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 889 chars
 [LLM API] Calling onChunk with: {thinkingLength: 889, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(215), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 889, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 890 chars
 [LLM API] Calling onChunk with: {thinkingLength: 890, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(216), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 890, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "30"
 [LLM API] tagBuffer: 30
 [LLM API] Accumulated thinking: 892 chars
 [LLM API] Calling onChunk with: {thinkingLength: 892, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(217), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 892, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")"
 [LLM API] tagBuffer: )
 [LLM API] Accumulated thinking: 893 chars
 [LLM API] Calling onChunk with: {thinkingLength: 893, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(218), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 893, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 895 chars
 [LLM API] Calling onChunk with: {thinkingLength: 895, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(219), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 895, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " This"
 [LLM API] tagBuffer:  This
 [LLM API] Accumulated thinking: 900 chars
 [LLM API] Calling onChunk with: {thinkingLength: 900, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(220), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 900, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " appears"
 [LLM API] tagBuffer:  appears
 [LLM API] Accumulated thinking: 908 chars
 [LLM API] Calling onChunk with: {thinkingLength: 908, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(221), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 908, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 911 chars
 [LLM API] Calling onChunk with: {thinkingLength: 911, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(222), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 911, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated thinking: 914 chars
 [LLM API] Calling onChunk with: {thinkingLength: 914, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(223), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 914, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 918 chars
 [LLM API] Calling onChunk with: {thinkingLength: 918, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(224), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 918, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " main"
 [LLM API] tagBuffer:  main
 [LLM API] Accumulated thinking: 923 chars
 [LLM API] Calling onChunk with: {thinkingLength: 923, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(225), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 923, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 933 chars
 [LLM API] Calling onChunk with: {thinkingLength: 933, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(226), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 933, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 937 chars
 [LLM API] Calling onChunk with: {thinkingLength: 937, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(227), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 937, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 940 chars
 [LLM API] Calling onChunk with: {thinkingLength: 940, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(228), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 940, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 943 chars
 [LLM API] Calling onChunk with: {thinkingLength: 943, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(229), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 943, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 948 chars
 [LLM API] Calling onChunk with: {thinkingLength: 948, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(230), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 948, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 955 chars
 [LLM API] Calling onChunk with: {thinkingLength: 955, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(231), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 955, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " screening"
 [LLM API] tagBuffer:  screening
 [LLM API] Accumulated thinking: 965 chars
 [LLM API] Calling onChunk with: {thinkingLength: 965, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(232), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 965, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n\n"
 [LLM API] tagBuffer: 


 [LLM API] Accumulated thinking: 967 chars
 [LLM API] Calling onChunk with: {thinkingLength: 967, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(233), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 967, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Let"
 [LLM API] tagBuffer: Let
 [LLM API] Accumulated thinking: 970 chars
 [LLM API] Calling onChunk with: {thinkingLength: 970, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(234), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 970, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " me"
 [LLM API] tagBuffer:  me
 [LLM API] Accumulated thinking: 973 chars
 [LLM API] Calling onChunk with: {thinkingLength: 973, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(235), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 973, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " search"
 [LLM API] tagBuffer:  search
 [LLM API] Accumulated thinking: 980 chars
 [LLM API] Calling onChunk with: {thinkingLength: 980, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(236), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 980, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 984 chars
 [LLM API] Calling onChunk with: {thinkingLength: 984, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(237), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 984, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " more"
 [LLM API] tagBuffer:  more
 [LLM API] Accumulated thinking: 989 chars
 [LLM API] Calling onChunk with: {thinkingLength: 989, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(238), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 989, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " specific"
 [LLM API] tagBuffer:  specific
 [LLM API] Accumulated thinking: 998 chars
 [LLM API] Calling onChunk with: {thinkingLength: 998, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(239), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 998, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " information"
 [LLM API] tagBuffer:  information
 [LLM API] Accumulated thinking: 1010 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1010, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(240), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1010, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " about"
 [LLM API] tagBuffer:  about
 [LLM API] Accumulated thinking: 1016 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1016, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(241), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1016, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated thinking: 1022 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1022, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(242), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1022, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated thinking: 1025 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1025, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(243), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1025, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 1029 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1029, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(244), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1029, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " primary"
 [LLM API] tagBuffer:  primary
 [LLM API] Accumulated thinking: 1037 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1037, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(245), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1037, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 1047 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1047, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(246), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1047, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 1052 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1052, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(247), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1052, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 1054 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1054, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(248), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1054, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "man"
 [LLM API] tagBuffer: man
 [LLM API] Accumulated thinking: 1057 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1057, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(249), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1057, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ages"
 [LLM API] tagBuffer: ages
 [LLM API] Accumulated thinking: 1061 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1061, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(250), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1061, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 1062 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1062, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(251), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1062, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 1065 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1065, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(252), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1065, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 1068 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1068, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(253), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1068, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 1073 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1073, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(254), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1073, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 1080 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1080, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(255), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1080, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1081 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1081, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(256), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1081, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " I"
 [LLM API] tagBuffer:  I
 [LLM API] Accumulated thinking: 1083 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1083, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(257), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1083, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " should"
 [LLM API] tagBuffer:  should
 [LLM API] Accumulated thinking: 1090 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1090, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(258), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1090, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " look"
 [LLM API] tagBuffer:  look
 [LLM API] Accumulated thinking: 1095 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1095, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(259), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1095, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 1099 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1099, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(260), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1099, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 1103 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1103, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(261), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1103, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " main"
 [LLM API] tagBuffer:  main
 [LLM API] Accumulated thinking: 1108 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1108, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(262), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1108, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 1115 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1115, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(263), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1115, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " control"
 [LLM API] tagBuffer:  control
 [LLM API] Accumulated thinking: 1123 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1123, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(264), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1123, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 1133 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1133, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(265), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1133, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1134 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1134, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(266), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1134, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "</think>"
 [LLM API] tagBuffer: </think>
 [LLM API] Exited </think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 1134, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(267), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1134, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Keeping potential partial tag in buffer: 

 [LLM API] Calling onChunk with: {thinkingLength: 1134, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(268), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1134, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: tool: "{\"action\": \"searching for specific infor
 [LLM API] FOUND TOOL EVENT, line: tool: "{\"action\": \"searching for specific information about the main access control procedure\"}"
 [LLM API] Tool string to parse: "{\"action\": \"searching for specific information about the main access control procedure\"}"
 [LLM API] Parsed tool JSON: {"action": "searching for specific information about the main access control procedure"}
 [LLM API] Processing line: data: "\n<think>"
 [LLM API] tagBuffer: 

<think>
 [LLM API] Entered <think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 1134, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(269), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1134, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Now"
 [LLM API] tagBuffer: Now
 [LLM API] Accumulated thinking: 1137 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1137, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(270), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1137, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " I"
 [LLM API] tagBuffer:  I
 [LLM API] Accumulated thinking: 1139 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1139, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(271), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1139, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " can"
 [LLM API] tagBuffer:  can
 [LLM API] Accumulated thinking: 1143 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1143, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(272), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1143, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " see"
 [LLM API] tagBuffer:  see
 [LLM API] Accumulated thinking: 1147 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1147, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(273), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1147, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 1151 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1151, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(274), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1151, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " pattern"
 [LLM API] tagBuffer:  pattern
 [LLM API] Accumulated thinking: 1159 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1159, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(275), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1159, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " more"
 [LLM API] tagBuffer:  more
 [LLM API] Accumulated thinking: 1164 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1164, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(276), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1164, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " clearly"
 [LLM API] tagBuffer:  clearly
 [LLM API] Accumulated thinking: 1172 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1172, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(277), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1172, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1173 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1173, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(278), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1173, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " The"
 [LLM API] tagBuffer:  The
 [LLM API] Accumulated thinking: 1177 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1177, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(279), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1177, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 1179 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1179, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(280), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1179, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "P"
 [LLM API] tagBuffer: P
 [LLM API] Accumulated thinking: 1180 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1180, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(281), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1180, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "alo"
 [LLM API] tagBuffer: alo
 [LLM API] Accumulated thinking: 1183 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1183, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(282), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1183, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated thinking: 1189 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1189, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(283), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1189, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Bad"
 [LLM API] tagBuffer:  Bad
 [LLM API] Accumulated thinking: 1193 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1193, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(284), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1193, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ging"
 [LLM API] tagBuffer: ging
 [LLM API] Accumulated thinking: 1197 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1197, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(285), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1197, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Procedure"
 [LLM API] tagBuffer:  Procedure
 [LLM API] Accumulated thinking: 1207 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1207, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(286), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1207, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 1208 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1208, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(287), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1208, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 1210 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1210, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(288), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1210, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 1212 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1212, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(289), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1212, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 1214 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1214, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(290), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1214, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 1215 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1215, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(291), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1215, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 1216 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1216, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(292), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1216, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 1218 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1218, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(293), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1218, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated thinking: 1220 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1220, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(294), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1220, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")"
 [LLM API] tagBuffer: )
 [LLM API] Accumulated thinking: 1221 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1221, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(295), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1221, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " appears"
 [LLM API] tagBuffer:  appears
 [LLM API] Accumulated thinking: 1229 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1229, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(296), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1229, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 1232 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1232, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(297), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1232, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated thinking: 1235 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1235, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(298), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1235, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 1239 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1239, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(299), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1239, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " main"
 [LLM API] tagBuffer:  main
 [LLM API] Accumulated thinking: 1244 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1244, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(300), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1244, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 1254 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1254, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(301), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1254, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 1259 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1259, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(302), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1259, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " manages"
 [LLM API] tagBuffer:  manages
 [LLM API] Accumulated thinking: 1267 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1267, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(303), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1267, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 1270 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1270, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(304), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1270, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 1273 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1273, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(305), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1273, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 1278 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1278, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(306), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1278, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 1285 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1285, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(307), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1285, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1286 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1286, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(308), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1286, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " From"
 [LLM API] tagBuffer:  From
 [LLM API] Accumulated thinking: 1291 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1291, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(309), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1291, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 1295 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1295, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(310), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1295, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " search"
 [LLM API] tagBuffer:  search
 [LLM API] Accumulated thinking: 1302 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1302, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(311), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1302, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " results"
 [LLM API] tagBuffer:  results
 [LLM API] Accumulated thinking: 1310 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1310, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(312), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1310, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 1311 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1311, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(313), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1311, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " I"
 [LLM API] tagBuffer:  I
 [LLM API] Accumulated thinking: 1313 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1313, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(314), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1313, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " can"
 [LLM API] tagBuffer:  can
 [LLM API] Accumulated thinking: 1317 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1317, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(315), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1317, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " see"
 [LLM API] tagBuffer:  see
 [LLM API] Accumulated thinking: 1321 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1321, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(316), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1321, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 1326 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1326, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(317), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1326, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " this"
 [LLM API] tagBuffer:  this
 [LLM API] Accumulated thinking: 1331 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1331, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(318), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1331, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 1341 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1341, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(319), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1341, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " covers"
 [LLM API] tagBuffer:  covers
 [LLM API] Accumulated thinking: 1348 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1348, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(320), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1348, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ":\n\n"
 [LLM API] tagBuffer: :


 [LLM API] Accumulated thinking: 1351 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1351, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(321), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1351, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "1"
 [LLM API] tagBuffer: 1
 [LLM API] Accumulated thinking: 1352 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1352, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(322), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1352, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1353 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1353, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(323), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1353, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " AC"
 [LLM API] tagBuffer:  AC
 [LLM API] Accumulated thinking: 1356 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1356, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(324), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1356, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "AD"
 [LLM API] tagBuffer: AD
 [LLM API] Accumulated thinking: 1358 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1358, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(325), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1358, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Badge"
 [LLM API] tagBuffer:  Badge
 [LLM API] Accumulated thinking: 1364 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1364, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(326), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1364, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " issuance"
 [LLM API] tagBuffer:  issuance
 [LLM API] Accumulated thinking: 1373 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1373, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(327), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1373, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated thinking: 1377 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1377, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(328), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1377, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated thinking: 1390 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1390, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(329), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1390, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 1391 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1391, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(330), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1391, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "2"
 [LLM API] tagBuffer: 2
 [LLM API] Accumulated thinking: 1392 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1392, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(331), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1392, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1393 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1393, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(332), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1393, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Un"
 [LLM API] tagBuffer:  Un
 [LLM API] Accumulated thinking: 1396 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1396, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(333), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1396, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 1399 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1399, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(334), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1399, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 1404 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1404, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(335), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1404, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 1411 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1411, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(336), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1411, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " request"
 [LLM API] tagBuffer:  request
 [LLM API] Accumulated thinking: 1419 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1419, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(337), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1419, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " processing"
 [LLM API] tagBuffer:  processing
 [LLM API] Accumulated thinking: 1430 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1430, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(338), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1430, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " through"
 [LLM API] tagBuffer:  through
 [LLM API] Accumulated thinking: 1438 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1438, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(339), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1438, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " SS"
 [LLM API] tagBuffer:  SS
 [LLM API] Accumulated thinking: 1441 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1441, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(340), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1441, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "IS"
 [LLM API] tagBuffer: IS
 [LLM API] Accumulated thinking: 1443 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1443, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(341), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1443, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 1444 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1444, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(342), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1444, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "3"
 [LLM API] tagBuffer: 3
 [LLM API] Accumulated thinking: 1445 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1445, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(343), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1445, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1446 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1446, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(344), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1446, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verification"
 [LLM API] tagBuffer:  Verification
 [LLM API] Accumulated thinking: 1459 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1459, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(345), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1459, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 1464 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1464, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(346), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1464, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " individuals"
 [LLM API] tagBuffer:  individuals
 [LLM API] Accumulated thinking: 1476 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1476, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(347), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1476, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " meet"
 [LLM API] tagBuffer:  meet
 [LLM API] Accumulated thinking: 1481 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1481, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(348), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1481, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " all"
 [LLM API] tagBuffer:  all
 [LLM API] Accumulated thinking: 1485 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1485, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(349), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1485, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated thinking: 1498 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1498, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(350), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1498, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 1502 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1502, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(351), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1502, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 1505 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1505, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(352), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1505, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 1508 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1508, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(353), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1508, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 1513 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1513, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(354), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1513, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 1520 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1520, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(355), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1520, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 1521 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1521, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(356), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1521, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated thinking: 1522 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1522, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(357), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1522, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1523 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1523, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(358), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1523, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Integration"
 [LLM API] tagBuffer:  Integration
 [LLM API] Accumulated thinking: 1535 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1535, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(359), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1535, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " with"
 [LLM API] tagBuffer:  with
 [LLM API] Accumulated thinking: 1540 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1540, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(360), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1540, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " other"
 [LLM API] tagBuffer:  other
 [LLM API] Accumulated thinking: 1546 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1546, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(361), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1546, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated thinking: 1557 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1557, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(362), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1557, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " like"
 [LLM API] tagBuffer:  like
 [LLM API] Accumulated thinking: 1562 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1562, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(363), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1562, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 1563 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1563, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(364), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1563, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 1565 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1565, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(365), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1565, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 1567 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1567, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(366), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1567, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 1568 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1568, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(367), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1568, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 1569 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1569, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(368), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1569, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 1571 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1571, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(369), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1571, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "40"
 [LLM API] tagBuffer: 40
 [LLM API] Accumulated thinking: 1573 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1573, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(370), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1573, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 1575 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1575, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(371), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1575, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Access"
 [LLM API] tagBuffer: Access
 [LLM API] Accumulated thinking: 1581 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1581, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(372), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1581, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Authorization"
 [LLM API] tagBuffer:  Authorization
 [LLM API] Accumulated thinking: 1595 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1595, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(373), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1595, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")\n\n"
 [LLM API] tagBuffer: )


 [LLM API] Accumulated thinking: 1598 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1598, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(374), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1598, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "However"
 [LLM API] tagBuffer: However
 [LLM API] Accumulated thinking: 1605 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1605, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(375), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1605, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 1606 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1606, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(376), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1606, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " I"
 [LLM API] tagBuffer:  I
 [LLM API] Accumulated thinking: 1608 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1608, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(377), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1608, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " also"
 [LLM API] tagBuffer:  also
 [LLM API] Accumulated thinking: 1613 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1613, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(378), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1613, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " see"
 [LLM API] tagBuffer:  see
 [LLM API] Accumulated thinking: 1617 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1617, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(379), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1617, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 1622 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1622, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(380), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1622, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 1632 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1632, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(381), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1632, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 1633 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1633, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(382), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1633, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 1635 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1635, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(383), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1635, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 1637 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1637, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(384), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1637, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 1638 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1638, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(385), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1638, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 1639 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1639, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(386), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1639, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 1641 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1641, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(387), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1641, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "40"
 [LLM API] tagBuffer: 40
 [LLM API] Accumulated thinking: 1643 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1643, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(388), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1643, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated thinking: 1646 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1646, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(389), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1646, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " referenced"
 [LLM API] tagBuffer:  referenced
 [LLM API] Accumulated thinking: 1657 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1657, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(390), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1657, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " as"
 [LLM API] tagBuffer:  as
 [LLM API] Accumulated thinking: 1660 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1660, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(391), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1660, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 1662 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1662, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(392), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1662, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Access"
 [LLM API] tagBuffer: Access
 [LLM API] Accumulated thinking: 1668 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1668, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(393), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1668, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Authorization"
 [LLM API] tagBuffer:  Authorization
 [LLM API] Accumulated thinking: 1682 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1682, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(394), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1682, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 1683 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1683, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(395), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1683, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated thinking: 1687 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1687, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(396), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1687, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " seems"
 [LLM API] tagBuffer:  seems
 [LLM API] Accumulated thinking: 1693 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1693, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(397), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1693, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 1696 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1696, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(398), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1696, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " contain"
 [LLM API] tagBuffer:  contain
 [LLM API] Accumulated thinking: 1704 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1704, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(399), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1704, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 1708 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1708, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(400), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1708, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " actual"
 [LLM API] tagBuffer:  actual
 [LLM API] Accumulated thinking: 1715 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1715, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(401), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1715, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " screening"
 [LLM API] tagBuffer:  screening
 [LLM API] Accumulated thinking: 1725 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1725, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(402), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1725, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated thinking: 1729 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1729, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(403), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1729, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " authorization"
 [LLM API] tagBuffer:  authorization
 [LLM API] Accumulated thinking: 1743 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1743, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(404), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1743, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated thinking: 1756 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1756, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(405), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1756, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 1760 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1760, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(406), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1760, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 1763 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1763, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(407), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1763, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 1766 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1766, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(408), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1766, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 1771 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1771, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(409), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1771, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 1778 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1778, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(410), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1778, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1779 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1779, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(411), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1779, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Let"
 [LLM API] tagBuffer:  Let
 [LLM API] Accumulated thinking: 1783 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1783, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(412), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1783, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " me"
 [LLM API] tagBuffer:  me
 [LLM API] Accumulated thinking: 1786 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1786, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(413), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1786, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " get"
 [LLM API] tagBuffer:  get
 [LLM API] Accumulated thinking: 1790 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1790, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(414), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1790, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " more"
 [LLM API] tagBuffer:  more
 [LLM API] Accumulated thinking: 1795 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1795, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(415), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1795, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " specific"
 [LLM API] tagBuffer:  specific
 [LLM API] Accumulated thinking: 1804 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1804, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(416), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1804, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " information"
 [LLM API] tagBuffer:  information
 [LLM API] Accumulated thinking: 1816 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1816, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(417), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1816, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " about"
 [LLM API] tagBuffer:  about
 [LLM API] Accumulated thinking: 1822 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1822, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(418), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1822, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " what"
 [LLM API] tagBuffer:  what
 [LLM API] Accumulated thinking: 1827 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1827, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(419), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1827, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " each"
 [LLM API] tagBuffer:  each
 [LLM API] Accumulated thinking: 1832 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1832, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(420), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1832, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 1842 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1842, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(421), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1842, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " does"
 [LLM API] tagBuffer:  does
 [LLM API] Accumulated thinking: 1847 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1847, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(422), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1847, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " by"
 [LLM API] tagBuffer:  by
 [LLM API] Accumulated thinking: 1850 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1850, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(423), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1850, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " getting"
 [LLM API] tagBuffer:  getting
 [LLM API] Accumulated thinking: 1858 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1858, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(424), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1858, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " their"
 [LLM API] tagBuffer:  their
 [LLM API] Accumulated thinking: 1864 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1864, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(425), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1864, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " latest"
 [LLM API] tagBuffer:  latest
 [LLM API] Accumulated thinking: 1871 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1871, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(426), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1871, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " revisions"
 [LLM API] tagBuffer:  revisions
 [LLM API] Accumulated thinking: 1881 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1881, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(427), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1881, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 1882 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1882, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(428), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1882, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "</think>"
 [LLM API] tagBuffer: </think>
 [LLM API] Exited </think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 1882, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(429), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1882, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Keeping potential partial tag in buffer: 

 [LLM API] Calling onChunk with: {thinkingLength: 1882, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(430), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1882, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: tool: "{\n    \"action\": \"getting the latest rev
 [LLM API] FOUND TOOL EVENT, line: tool: "{\n    \"action\": \"getting the latest revisions of procedures 20DP-0SK39 and 20DP-0SK40\"\n}"
 [LLM API] Tool string to parse: "{\n    \"action\": \"getting the latest revisions of procedures 20DP-0SK39 and 20DP-0SK40\"\n}"
 [LLM API] Parsed tool JSON: {
    "action": "getting the latest revisions of procedures 20DP-0SK39 and 20DP-0SK40"
}
 [LLM API] Processing line: data: "\n<think>"
 [LLM API] tagBuffer: 

<think>
 [LLM API] Entered <think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 1882, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(431), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1882, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Now"
 [LLM API] tagBuffer: Now
 [LLM API] Accumulated thinking: 1885 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1885, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(432), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1885, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " I"
 [LLM API] tagBuffer:  I
 [LLM API] Accumulated thinking: 1887 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1887, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(433), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1887, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " have"
 [LLM API] tagBuffer:  have
 [LLM API] Accumulated thinking: 1892 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1892, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(434), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1892, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 1896 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1896, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(435), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1896, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " latest"
 [LLM API] tagBuffer:  latest
 [LLM API] Accumulated thinking: 1903 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1903, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(436), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1903, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " revisions"
 [LLM API] tagBuffer:  revisions
 [LLM API] Accumulated thinking: 1913 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1913, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(437), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1913, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ":\n"
 [LLM API] tagBuffer: :

 [LLM API] Accumulated thinking: 1915 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1915, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(438), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1915, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 1916 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1916, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(439), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1916, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 1917 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1917, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(440), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1917, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 1919 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1919, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(441), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1919, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 1921 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1921, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(442), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1921, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 1922 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1922, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(443), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1922, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 1923 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1923, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(444), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1923, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 1925 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1925, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(445), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1925, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated thinking: 1927 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1927, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(446), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1927, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 1929 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1929, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(447), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1929, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "P"
 [LLM API] tagBuffer: P
 [LLM API] Accumulated thinking: 1930 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1930, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(448), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1930, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "alo"
 [LLM API] tagBuffer: alo
 [LLM API] Accumulated thinking: 1933 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1933, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(449), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1933, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated thinking: 1939 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1939, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(450), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1939, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Bad"
 [LLM API] tagBuffer:  Bad
 [LLM API] Accumulated thinking: 1943 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1943, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(451), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1943, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ging"
 [LLM API] tagBuffer: ging
 [LLM API] Accumulated thinking: 1947 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1947, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(452), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1947, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Procedure"
 [LLM API] tagBuffer:  Procedure
 [LLM API] Accumulated thinking: 1957 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1957, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(453), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1957, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")"
 [LLM API] tagBuffer: )
 [LLM API] Accumulated thinking: 1958 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1958, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(454), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1958, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 1960 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1960, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(455), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1960, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Rev"
 [LLM API] tagBuffer:  Rev
 [LLM API] Accumulated thinking: 1964 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1964, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(456), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1964, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 1965 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1965, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(457), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1965, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "04"
 [LLM API] tagBuffer: 04
 [LLM API] Accumulated thinking: 1967 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1967, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(458), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1967, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "2"
 [LLM API] tagBuffer: 2
 [LLM API] Accumulated thinking: 1968 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1968, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(459), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1968, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 1969 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1969, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(460), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1969, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 1970 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1970, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(461), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1970, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 1971 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1971, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(462), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1971, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 1973 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1973, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(463), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1973, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 1975 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1975, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(464), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1975, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 1976 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1976, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(465), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1976, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 1977 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1977, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(466), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1977, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 1979 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1979, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(467), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1979, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "40"
 [LLM API] tagBuffer: 40
 [LLM API] Accumulated thinking: 1981 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1981, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(468), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1981, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 1983 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1983, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(469), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1983, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "PV"
 [LLM API] tagBuffer: PV
 [LLM API] Accumulated thinking: 1985 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1985, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(470), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1985, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "NG"
 [LLM API] tagBuffer: NG
 [LLM API] Accumulated thinking: 1987 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1987, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(471), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1987, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "S"
 [LLM API] tagBuffer: S
 [LLM API] Accumulated thinking: 1988 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1988, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(472), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1988, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Un"
 [LLM API] tagBuffer:  Un
 [LLM API] Accumulated thinking: 1991 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1991, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(473), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1991, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 1994 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1994, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(474), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1994, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 1999 chars
 [LLM API] Calling onChunk with: {thinkingLength: 1999, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(475), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 1999, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Access"
 [LLM API] tagBuffer:  Access
 [LLM API] Accumulated thinking: 2006 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2006, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(476), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2006, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Screening"
 [LLM API] tagBuffer:  Screening
 [LLM API] Accumulated thinking: 2016 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2016, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(477), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2016, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")"
 [LLM API] tagBuffer: )
 [LLM API] Accumulated thinking: 2017 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2017, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(478), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2017, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 2019 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2019, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(479), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2019, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Rev"
 [LLM API] tagBuffer:  Rev
 [LLM API] Accumulated thinking: 2023 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2023, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(480), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2023, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 2024 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2024, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(481), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2024, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 2025 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2025, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(482), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2025, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "50"
 [LLM API] tagBuffer: 50
 [LLM API] Accumulated thinking: 2027 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2027, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(483), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2027, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n\n"
 [LLM API] tagBuffer: 


 [LLM API] Accumulated thinking: 2029 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2029, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(484), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2029, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Let"
 [LLM API] tagBuffer: Let
 [LLM API] Accumulated thinking: 2032 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2032, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(485), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2032, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " me"
 [LLM API] tagBuffer:  me
 [LLM API] Accumulated thinking: 2035 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2035, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(486), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2035, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " search"
 [LLM API] tagBuffer:  search
 [LLM API] Accumulated thinking: 2042 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2042, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(487), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2042, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 2046 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2046, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(488), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2046, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " one"
 [LLM API] tagBuffer:  one
 [LLM API] Accumulated thinking: 2050 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2050, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(489), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2050, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " more"
 [LLM API] tagBuffer:  more
 [LLM API] Accumulated thinking: 2055 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2055, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(490), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2055, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " specific"
 [LLM API] tagBuffer:  specific
 [LLM API] Accumulated thinking: 2064 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2064, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(491), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2064, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " term"
 [LLM API] tagBuffer:  term
 [LLM API] Accumulated thinking: 2069 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2069, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(492), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2069, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 2072 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2072, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(493), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2072, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " understand"
 [LLM API] tagBuffer:  understand
 [LLM API] Accumulated thinking: 2083 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2083, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(494), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2083, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated thinking: 2089 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2089, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(495), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2089, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 2099 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2099, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(496), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2099, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated thinking: 2102 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2102, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(497), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2102, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 2106 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2106, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(498), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2106, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " primary"
 [LLM API] tagBuffer:  primary
 [LLM API] Accumulated thinking: 2114 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2114, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(499), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2114, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 2116 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2116, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(500), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2116, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "manager"
 [LLM API] tagBuffer: manager
 [LLM API] Accumulated thinking: 2123 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2123, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(501), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2123, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 2124 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2124, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(502), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2124, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " of"
 [LLM API] tagBuffer:  of
 [LLM API] Accumulated thinking: 2127 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2127, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(503), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2127, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 2130 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2130, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(504), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2130, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 2133 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2133, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(505), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2133, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 2138 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2138, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(506), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2138, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 2145 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2145, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(507), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2145, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " vs"
 [LLM API] tagBuffer:  vs
 [LLM API] Accumulated thinking: 2148 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2148, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(508), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2148, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated thinking: 2154 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2154, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(509), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2154, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " one"
 [LLM API] tagBuffer:  one
 [LLM API] Accumulated thinking: 2158 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2158, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(510), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2158, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " handles"
 [LLM API] tagBuffer:  handles
 [LLM API] Accumulated thinking: 2166 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2166, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(511), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2166, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 2170 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2170, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(512), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2170, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " specific"
 [LLM API] tagBuffer:  specific
 [LLM API] Accumulated thinking: 2179 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2179, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(513), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2179, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " screening"
 [LLM API] tagBuffer:  screening
 [LLM API] Accumulated thinking: 2189 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2189, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(514), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2189, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated thinking: 2202 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2202, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(515), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2202, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 2203 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2203, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(516), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2203, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "</think>"
 [LLM API] tagBuffer: </think>
 [LLM API] Exited </think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 2203, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(517), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2203, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Keeping potential partial tag in buffer: 

 [LLM API] Calling onChunk with: {thinkingLength: 2203, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(518), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2203, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: tool: "{\"action\": \"searching for specific term\
 [LLM API] FOUND TOOL EVENT, line: tool: "{\"action\": \"searching for specific term\"}"
 [LLM API] Tool string to parse: "{\"action\": \"searching for specific term\"}"
 [LLM API] Parsed tool JSON: {"action": "searching for specific term"}
 [LLM API] Processing line: data: "\n<think>"
 [LLM API] tagBuffer: 

<think>
 [LLM API] Entered <think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 2203, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(519), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2203, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Based"
 [LLM API] tagBuffer: Based
 [LLM API] Accumulated thinking: 2208 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2208, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(520), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2208, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " on"
 [LLM API] tagBuffer:  on
 [LLM API] Accumulated thinking: 2211 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2211, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(521), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2211, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 2215 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2215, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(522), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2215, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " search"
 [LLM API] tagBuffer:  search
 [LLM API] Accumulated thinking: 2222 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2222, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(523), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2222, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " results"
 [LLM API] tagBuffer:  results
 [LLM API] Accumulated thinking: 2230 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2230, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(524), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2230, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 2231 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2231, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(525), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2231, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " I"
 [LLM API] tagBuffer:  I
 [LLM API] Accumulated thinking: 2233 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2233, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(526), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2233, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " can"
 [LLM API] tagBuffer:  can
 [LLM API] Accumulated thinking: 2237 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2237, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(527), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2237, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " now"
 [LLM API] tagBuffer:  now
 [LLM API] Accumulated thinking: 2241 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2241, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(528), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2241, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " provide"
 [LLM API] tagBuffer:  provide
 [LLM API] Accumulated thinking: 2249 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2249, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(529), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2249, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " a"
 [LLM API] tagBuffer:  a
 [LLM API] Accumulated thinking: 2251 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2251, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(530), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2251, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " comprehensive"
 [LLM API] tagBuffer:  comprehensive
 [LLM API] Accumulated thinking: 2265 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2265, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(531), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2265, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " answer"
 [LLM API] tagBuffer:  answer
 [LLM API] Accumulated thinking: 2272 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2272, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(532), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2272, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " about"
 [LLM API] tagBuffer:  about
 [LLM API] Accumulated thinking: 2278 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2278, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(533), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2278, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated thinking: 2284 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2284, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(534), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2284, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 2294 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2294, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(535), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2294, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " manages"
 [LLM API] tagBuffer:  manages
 [LLM API] Accumulated thinking: 2302 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2302, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(536), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2302, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 2305 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2305, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(537), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2305, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 2308 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2308, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(538), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2308, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 2313 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2313, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(539), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2313, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 2320 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2320, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(540), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2320, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " at"
 [LLM API] tagBuffer:  at
 [LLM API] Accumulated thinking: 2323 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2323, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(541), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2323, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Palo"
 [LLM API] tagBuffer:  Palo
 [LLM API] Accumulated thinking: 2328 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2328, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(542), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2328, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated thinking: 2334 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2334, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(543), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2334, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 2335 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2335, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(544), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2335, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " From"
 [LLM API] tagBuffer:  From
 [LLM API] Accumulated thinking: 2340 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2340, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(545), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2340, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 2344 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2344, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(546), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2344, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " documents"
 [LLM API] tagBuffer:  documents
 [LLM API] Accumulated thinking: 2354 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2354, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(547), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2354, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 2355 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2355, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(548), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2355, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " I"
 [LLM API] tagBuffer:  I
 [LLM API] Accumulated thinking: 2357 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2357, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(549), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2357, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " can"
 [LLM API] tagBuffer:  can
 [LLM API] Accumulated thinking: 2361 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2361, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(550), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2361, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " see"
 [LLM API] tagBuffer:  see
 [LLM API] Accumulated thinking: 2365 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2365, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(551), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2365, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " two"
 [LLM API] tagBuffer:  two
 [LLM API] Accumulated thinking: 2369 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2369, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(552), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2369, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " main"
 [LLM API] tagBuffer:  main
 [LLM API] Accumulated thinking: 2374 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2374, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(553), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2374, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated thinking: 2385 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2385, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(554), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2385, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " involved"
 [LLM API] tagBuffer:  involved
 [LLM API] Accumulated thinking: 2394 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2394, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(555), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2394, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ":\n\n"
 [LLM API] tagBuffer: :


 [LLM API] Accumulated thinking: 2397 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2397, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(556), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2397, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "1"
 [LLM API] tagBuffer: 1
 [LLM API] Accumulated thinking: 2398 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2398, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(557), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2398, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 2399 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2399, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(558), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2399, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " **"
 [LLM API] tagBuffer:  **
 [LLM API] Accumulated thinking: 2402 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2402, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(559), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2402, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 2404 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2404, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(560), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2404, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 2406 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2406, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(561), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2406, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 2407 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2407, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(562), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2407, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 2408 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2408, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(563), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2408, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 2410 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2410, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(564), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2410, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated thinking: 2412 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2412, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(565), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2412, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 2414 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2414, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(566), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2414, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "P"
 [LLM API] tagBuffer: P
 [LLM API] Accumulated thinking: 2415 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2415, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(567), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2415, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "alo"
 [LLM API] tagBuffer: alo
 [LLM API] Accumulated thinking: 2418 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2418, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(568), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2418, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated thinking: 2424 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2424, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(569), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2424, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Bad"
 [LLM API] tagBuffer:  Bad
 [LLM API] Accumulated thinking: 2428 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2428, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(570), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2428, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ging"
 [LLM API] tagBuffer: ging
 [LLM API] Accumulated thinking: 2432 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2432, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(571), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2432, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Procedure"
 [LLM API] tagBuffer:  Procedure
 [LLM API] Accumulated thinking: 2442 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2442, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(572), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2442, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")**"
 [LLM API] tagBuffer: )**
 [LLM API] Accumulated thinking: 2445 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2445, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(573), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2445, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 2447 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2447, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(574), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2447, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " This"
 [LLM API] tagBuffer:  This
 [LLM API] Accumulated thinking: 2452 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2452, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(575), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2452, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " appears"
 [LLM API] tagBuffer:  appears
 [LLM API] Accumulated thinking: 2460 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2460, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(576), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2460, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 2463 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2463, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(577), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2463, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated thinking: 2466 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2466, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(578), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2466, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 2470 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2470, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(579), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2470, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " primary"
 [LLM API] tagBuffer:  primary
 [LLM API] Accumulated thinking: 2478 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2478, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(580), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2478, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 2488 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2488, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(581), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2488, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 2493 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2493, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(582), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2493, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 2495 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2495, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(583), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2495, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "man"
 [LLM API] tagBuffer: man
 [LLM API] Accumulated thinking: 2498 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2498, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(584), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2498, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ages"
 [LLM API] tagBuffer: ages
 [LLM API] Accumulated thinking: 2502 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2502, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(585), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2502, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 2503 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2503, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(586), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2503, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 2506 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2506, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(587), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2506, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 2509 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2509, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(588), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2509, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 2514 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2514, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(589), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2514, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 2521 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2521, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(590), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2521, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 2522 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2522, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(591), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2522, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " It"
 [LLM API] tagBuffer:  It
 [LLM API] Accumulated thinking: 2525 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2525, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(592), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2525, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " handles"
 [LLM API] tagBuffer:  handles
 [LLM API] Accumulated thinking: 2533 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2533, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(593), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2533, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ":\n"
 [LLM API] tagBuffer: :

 [LLM API] Accumulated thinking: 2535 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2535, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(594), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2535, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated thinking: 2537 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2537, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(595), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2537, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 2539 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2539, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(596), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2539, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " AC"
 [LLM API] tagBuffer:  AC
 [LLM API] Accumulated thinking: 2542 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2542, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(597), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2542, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "AD"
 [LLM API] tagBuffer: AD
 [LLM API] Accumulated thinking: 2544 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2544, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(598), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2544, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Badge"
 [LLM API] tagBuffer:  Badge
 [LLM API] Accumulated thinking: 2550 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2550, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(599), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2550, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " issuance"
 [LLM API] tagBuffer:  issuance
 [LLM API] Accumulated thinking: 2559 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2559, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(600), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2559, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated thinking: 2563 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2563, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(601), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2563, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " management"
 [LLM API] tagBuffer:  management
 [LLM API] Accumulated thinking: 2574 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2574, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(602), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2574, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 2575 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2575, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(603), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2575, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated thinking: 2577 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2577, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(604), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2577, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 2579 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2579, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(605), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2579, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Un"
 [LLM API] tagBuffer:  Un
 [LLM API] Accumulated thinking: 2582 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2582, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(606), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2582, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 2585 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2585, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(607), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2585, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 2590 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2590, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(608), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2590, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 2597 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2597, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(609), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2597, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " request"
 [LLM API] tagBuffer:  request
 [LLM API] Accumulated thinking: 2605 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2605, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(610), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2605, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " processing"
 [LLM API] tagBuffer:  processing
 [LLM API] Accumulated thinking: 2616 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2616, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(611), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2616, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " through"
 [LLM API] tagBuffer:  through
 [LLM API] Accumulated thinking: 2624 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2624, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(612), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2624, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " SS"
 [LLM API] tagBuffer:  SS
 [LLM API] Accumulated thinking: 2627 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2627, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(613), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2627, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "IS"
 [LLM API] tagBuffer: IS
 [LLM API] Accumulated thinking: 2629 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2629, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(614), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2629, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 2630 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2630, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(615), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2630, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated thinking: 2632 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2632, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(616), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2632, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 2634 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2634, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(617), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2634, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verification"
 [LLM API] tagBuffer:  Verification
 [LLM API] Accumulated thinking: 2647 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2647, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(618), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2647, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 2652 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2652, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(619), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2652, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " all"
 [LLM API] tagBuffer:  all
 [LLM API] Accumulated thinking: 2656 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2656, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(620), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2656, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated thinking: 2669 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2669, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(621), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2669, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " are"
 [LLM API] tagBuffer:  are
 [LLM API] Accumulated thinking: 2673 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2673, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(622), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2673, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " met"
 [LLM API] tagBuffer:  met
 [LLM API] Accumulated thinking: 2677 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2677, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(623), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2677, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 2678 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2678, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(624), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2678, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated thinking: 2680 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2680, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(625), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2680, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 2682 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2682, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(626), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2682, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " AC"
 [LLM API] tagBuffer:  AC
 [LLM API] Accumulated thinking: 2685 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2685, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(627), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2685, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "AD"
 [LLM API] tagBuffer: AD
 [LLM API] Accumulated thinking: 2687 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2687, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(628), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2687, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Badge"
 [LLM API] tagBuffer:  Badge
 [LLM API] Accumulated thinking: 2693 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2693, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(629), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2693, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " termination"
 [LLM API] tagBuffer:  termination
 [LLM API] Accumulated thinking: 2705 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2705, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(630), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2705, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "/react"
 [LLM API] tagBuffer: /react
 [LLM API] Accumulated thinking: 2711 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2711, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(631), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2711, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ivation"
 [LLM API] tagBuffer: ivation
 [LLM API] Accumulated thinking: 2718 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2718, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(632), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2718, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 2719 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2719, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(633), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2719, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated thinking: 2721 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2721, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(634), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2721, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 2723 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2723, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(635), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2723, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Administrative"
 [LLM API] tagBuffer:  Administrative
 [LLM API] Accumulated thinking: 2738 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2738, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(636), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2738, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " aspects"
 [LLM API] tagBuffer:  aspects
 [LLM API] Accumulated thinking: 2746 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2746, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(637), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2746, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " of"
 [LLM API] tagBuffer:  of
 [LLM API] Accumulated thinking: 2749 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2749, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(638), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2749, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 2752 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2752, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(639), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2752, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 2755 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2755, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(640), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2755, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 2760 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2760, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(641), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2760, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 2767 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2767, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(642), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2767, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n\n"
 [LLM API] tagBuffer: 


 [LLM API] Accumulated thinking: 2769 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2769, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(643), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2769, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "2"
 [LLM API] tagBuffer: 2
 [LLM API] Accumulated thinking: 2770 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2770, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(644), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2770, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 2771 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2771, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(645), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2771, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " **"
 [LLM API] tagBuffer:  **
 [LLM API] Accumulated thinking: 2774 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2774, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(646), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2774, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 2776 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2776, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(647), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2776, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 2778 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2778, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(648), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2778, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 2779 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2779, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(649), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2779, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 2780 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2780, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(650), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2780, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 2782 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2782, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(651), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2782, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "40"
 [LLM API] tagBuffer: 40
 [LLM API] Accumulated thinking: 2784 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2784, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(652), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2784, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 2786 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2786, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(653), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2786, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "PV"
 [LLM API] tagBuffer: PV
 [LLM API] Accumulated thinking: 2788 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2788, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(654), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2788, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "NG"
 [LLM API] tagBuffer: NG
 [LLM API] Accumulated thinking: 2790 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2790, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(655), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2790, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "S"
 [LLM API] tagBuffer: S
 [LLM API] Accumulated thinking: 2791 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2791, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(656), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2791, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Un"
 [LLM API] tagBuffer:  Un
 [LLM API] Accumulated thinking: 2794 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2794, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(657), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2794, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 2797 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2797, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(658), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2797, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 2802 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2802, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(659), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2802, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Access"
 [LLM API] tagBuffer:  Access
 [LLM API] Accumulated thinking: 2809 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2809, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(660), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2809, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Screening"
 [LLM API] tagBuffer:  Screening
 [LLM API] Accumulated thinking: 2819 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2819, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(661), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2819, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")**"
 [LLM API] tagBuffer: )**
 [LLM API] Accumulated thinking: 2822 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2822, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(662), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2822, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 2824 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2824, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(663), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2824, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " This"
 [LLM API] tagBuffer:  This
 [LLM API] Accumulated thinking: 2829 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2829, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(664), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2829, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " appears"
 [LLM API] tagBuffer:  appears
 [LLM API] Accumulated thinking: 2837 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2837, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(665), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2837, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 2840 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2840, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(666), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2840, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " handle"
 [LLM API] tagBuffer:  handle
 [LLM API] Accumulated thinking: 2847 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2847, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(667), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2847, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 2851 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2851, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(668), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2851, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " actual"
 [LLM API] tagBuffer:  actual
 [LLM API] Accumulated thinking: 2858 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2858, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(669), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2858, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " screening"
 [LLM API] tagBuffer:  screening
 [LLM API] Accumulated thinking: 2868 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2868, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(670), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2868, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 2869 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2869, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(671), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2869, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " investigation"
 [LLM API] tagBuffer:  investigation
 [LLM API] Accumulated thinking: 2883 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2883, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(672), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2883, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 2884 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2884, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(673), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2884, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated thinking: 2888 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2888, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(674), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2888, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " security"
 [LLM API] tagBuffer:  security
 [LLM API] Accumulated thinking: 2897 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2897, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(675), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2897, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " assessment"
 [LLM API] tagBuffer:  assessment
 [LLM API] Accumulated thinking: 2908 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2908, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(676), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2908, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated thinking: 2921 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2921, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(677), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2921, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 2925 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2925, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(678), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2925, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 2928 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2928, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(679), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2928, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 2931 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2931, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(680), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2931, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 2936 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2936, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(681), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2936, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 2943 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2943, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(682), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2943, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " authorization"
 [LLM API] tagBuffer:  authorization
 [LLM API] Accumulated thinking: 2957 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2957, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(683), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2957, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated thinking: 2958 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2958, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(684), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2958, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " It"
 [LLM API] tagBuffer:  It
 [LLM API] Accumulated thinking: 2961 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2961, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(685), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2961, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " focuses"
 [LLM API] tagBuffer:  focuses
 [LLM API] Accumulated thinking: 2969 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2969, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(686), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2969, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " on"
 [LLM API] tagBuffer:  on
 [LLM API] Accumulated thinking: 2972 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2972, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(687), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2972, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ":\n"
 [LLM API] tagBuffer: :

 [LLM API] Accumulated thinking: 2974 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2974, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(688), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2974, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated thinking: 2976 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2976, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(689), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2976, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 2978 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2978, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(690), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2978, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Background"
 [LLM API] tagBuffer:  Background
 [LLM API] Accumulated thinking: 2989 chars
 [LLM API] Calling onChunk with: {thinkingLength: 2989, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(691), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 2989, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " investigations"
 [LLM API] tagBuffer:  investigations
 [LLM API] Accumulated thinking: 3004 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3004, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(692), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3004, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 3005 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3005, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(693), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3005, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated thinking: 3007 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3007, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(694), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3007, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 3009 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3009, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(695), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3009, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Security"
 [LLM API] tagBuffer:  Security
 [LLM API] Accumulated thinking: 3018 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3018, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(696), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3018, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " screening"
 [LLM API] tagBuffer:  screening
 [LLM API] Accumulated thinking: 3028 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3028, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(697), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3028, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated thinking: 3041 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3041, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(698), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3041, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 3042 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3042, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(699), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3042, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated thinking: 3044 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3044, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(700), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3044, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 3046 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3046, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(701), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3046, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Fitness"
 [LLM API] tagBuffer:  Fitness
 [LLM API] Accumulated thinking: 3054 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3054, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(702), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3054, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 3058 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3058, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(703), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3058, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Duty"
 [LLM API] tagBuffer:  Duty
 [LLM API] Accumulated thinking: 3063 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3063, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(704), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3063, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " verification"
 [LLM API] tagBuffer:  verification
 [LLM API] Accumulated thinking: 3076 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3076, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(705), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3076, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated thinking: 3077 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3077, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(706), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3077, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated thinking: 3079 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3079, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(707), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3079, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated thinking: 3081 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3081, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(708), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3081, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " The"
 [LLM API] tagBuffer:  The
 [LLM API] Accumulated thinking: 3085 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3085, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(709), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3085, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " actual"
 [LLM API] tagBuffer:  actual
 [LLM API] Accumulated thinking: 3092 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3092, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(710), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3092, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " criteria"
 [LLM API] tagBuffer:  criteria
 [LLM API] Accumulated thinking: 3101 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3101, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(711), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3101, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 3106 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3106, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(712), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3106, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " must"
 [LLM API] tagBuffer:  must
 [LLM API] Accumulated thinking: 3111 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3111, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(713), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3111, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated thinking: 3114 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3114, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(714), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3114, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " met"
 [LLM API] tagBuffer:  met
 [LLM API] Accumulated thinking: 3118 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3118, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(715), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3118, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 3122 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3122, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(716), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3122, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 3125 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3125, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(717), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3125, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 3128 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3128, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(718), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3128, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 3133 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3133, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(719), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3133, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 3140 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3140, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(720), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3140, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n\n"
 [LLM API] tagBuffer: 


 [LLM API] Accumulated thinking: 3142 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3142, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(721), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3142, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "From"
 [LLM API] tagBuffer: From
 [LLM API] Accumulated thinking: 3146 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3146, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(722), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3146, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 3150 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3150, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(723), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3150, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " search"
 [LLM API] tagBuffer:  search
 [LLM API] Accumulated thinking: 3157 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3157, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(724), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3157, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " results"
 [LLM API] tagBuffer:  results
 [LLM API] Accumulated thinking: 3165 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3165, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(725), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3165, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 3166 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3166, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(726), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3166, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " it"
 [LLM API] tagBuffer:  it
 [LLM API] Accumulated thinking: 3169 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3169, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(727), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3169, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "'s"
 [LLM API] tagBuffer: 's
 [LLM API] Accumulated thinking: 3171 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3171, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(728), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3171, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " clear"
 [LLM API] tagBuffer:  clear
 [LLM API] Accumulated thinking: 3177 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3177, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(729), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3177, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 3182 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3182, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(730), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3182, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ":\n\n"
 [LLM API] tagBuffer: :


 [LLM API] Accumulated thinking: 3185 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3185, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(731), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3185, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3186 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3186, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(732), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3186, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " The"
 [LLM API] tagBuffer:  The
 [LLM API] Accumulated thinking: 3190 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3190, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(733), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3190, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 3191 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3191, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(734), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3191, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 3193 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3193, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(735), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3193, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 3195 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3195, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(736), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3195, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3196 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3196, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(737), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3196, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 3197 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3197, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(738), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3197, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 3199 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3199, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(739), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3199, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated thinking: 3201 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3201, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(740), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3201, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 3211 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3211, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(741), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3211, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated thinking: 3214 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3214, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(742), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3214, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " referenced"
 [LLM API] tagBuffer:  referenced
 [LLM API] Accumulated thinking: 3225 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3225, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(743), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3225, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " as"
 [LLM API] tagBuffer:  as
 [LLM API] Accumulated thinking: 3228 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3228, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(744), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3228, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " containing"
 [LLM API] tagBuffer:  containing
 [LLM API] Accumulated thinking: 3239 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3239, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(745), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3239, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 3243 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3243, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(746), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3243, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 3245 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3245, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(747), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3245, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "g"
 [LLM API] tagBuffer: g
 [LLM API] Accumulated thinking: 3246 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3246, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(748), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3246, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "over"
 [LLM API] tagBuffer: over
 [LLM API] Accumulated thinking: 3250 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3250, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(749), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3250, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ning"
 [LLM API] tagBuffer: ning
 [LLM API] Accumulated thinking: 3254 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3254, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(750), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3254, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated thinking: 3265 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3265, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(751), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3265, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 3266 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3266, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(752), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3266, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 3270 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3270, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(753), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3270, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 3273 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3273, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(754), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3273, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 3276 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3276, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(755), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3276, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 3281 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3281, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(756), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3281, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 3288 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3288, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(757), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3288, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ["
 [LLM API] tagBuffer:  [
 [LLM API] Accumulated thinking: 3290 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3290, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(758), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3290, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Source"
 [LLM API] tagBuffer: Source
 [LLM API] Accumulated thinking: 3296 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3296, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(759), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3296, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ":"
 [LLM API] tagBuffer: :
 [LLM API] Accumulated thinking: 3297 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3297, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(760), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3297, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " {"
 [LLM API] tagBuffer:  {
 [LLM API] Accumulated thinking: 3299 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3299, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(761), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3299, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "74"
 [LLM API] tagBuffer: 74
 [LLM API] Accumulated thinking: 3301 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3301, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(762), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3301, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "CF"
 [LLM API] tagBuffer: CF
 [LLM API] Accumulated thinking: 3303 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3303, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(763), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3303, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "9"
 [LLM API] tagBuffer: 9
 [LLM API] Accumulated thinking: 3304 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3304, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(764), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3304, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "E"
 [LLM API] tagBuffer: E
 [LLM API] Accumulated thinking: 3305 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3305, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(765), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3305, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "8"
 [LLM API] tagBuffer: 8
 [LLM API] Accumulated thinking: 3306 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3306, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(766), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3306, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "E"
 [LLM API] tagBuffer: E
 [LLM API] Accumulated thinking: 3307 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3307, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(767), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3307, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3308 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3308, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(768), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3308, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "95"
 [LLM API] tagBuffer: 95
 [LLM API] Accumulated thinking: 3310 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3310, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(769), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3310, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "2"
 [LLM API] tagBuffer: 2
 [LLM API] Accumulated thinking: 3311 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3311, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(770), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3311, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "F"
 [LLM API] tagBuffer: F
 [LLM API] Accumulated thinking: 3312 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3312, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(771), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3312, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3313 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3313, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(772), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3313, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated thinking: 3314 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3314, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(773), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3314, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "16"
 [LLM API] tagBuffer: 16
 [LLM API] Accumulated thinking: 3316 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3316, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(774), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3316, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated thinking: 3317 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3317, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(775), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3317, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3318 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3318, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(776), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3318, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "8"
 [LLM API] tagBuffer: 8
 [LLM API] Accumulated thinking: 3319 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3319, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(777), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3319, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "76"
 [LLM API] tagBuffer: 76
 [LLM API] Accumulated thinking: 3321 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3321, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(778), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3321, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated thinking: 3322 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3322, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(779), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3322, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-F"
 [LLM API] tagBuffer: -F
 [LLM API] Accumulated thinking: 3324 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3324, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(780), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3324, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "79"
 [LLM API] tagBuffer: 79
 [LLM API] Accumulated thinking: 3326 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3326, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(781), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3326, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "B"
 [LLM API] tagBuffer: B
 [LLM API] Accumulated thinking: 3327 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3327, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(782), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3327, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "61"
 [LLM API] tagBuffer: 61
 [LLM API] Accumulated thinking: 3329 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3329, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(783), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3329, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ADE"
 [LLM API] tagBuffer: ADE
 [LLM API] Accumulated thinking: 3332 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3332, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(784), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3332, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "BC"
 [LLM API] tagBuffer: BC
 [LLM API] Accumulated thinking: 3334 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3334, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(785), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3334, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "5"
 [LLM API] tagBuffer: 5
 [LLM API] Accumulated thinking: 3335 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3335, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(786), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3335, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "}]\n"
 [LLM API] tagBuffer: }]

 [LLM API] Accumulated thinking: 3338 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3338, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(787), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3338, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3339 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3339, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(788), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3339, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 3340 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3340, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(789), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3340, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 3342 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3342, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(790), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3342, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 3344 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3344, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(791), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3344, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3345 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3345, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(792), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3345, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 3346 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3346, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(793), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3346, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 3348 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3348, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(794), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3348, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "40"
 [LLM API] tagBuffer: 40
 [LLM API] Accumulated thinking: 3350 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3350, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(795), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3350, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated thinking: 3353 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3353, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(796), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3353, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " referenced"
 [LLM API] tagBuffer:  referenced
 [LLM API] Accumulated thinking: 3364 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3364, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(797), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3364, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " as"
 [LLM API] tagBuffer:  as
 [LLM API] Accumulated thinking: 3367 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3367, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(798), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3367, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " containing"
 [LLM API] tagBuffer:  containing
 [LLM API] Accumulated thinking: 3378 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3378, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(799), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3378, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 3382 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3382, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(800), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3382, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " specific"
 [LLM API] tagBuffer:  specific
 [LLM API] Accumulated thinking: 3391 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3391, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(801), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3391, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated thinking: 3404 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3404, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(802), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3404, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 3409 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3409, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(803), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3409, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " must"
 [LLM API] tagBuffer:  must
 [LLM API] Accumulated thinking: 3414 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3414, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(804), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3414, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated thinking: 3417 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3417, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(805), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3417, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " met"
 [LLM API] tagBuffer:  met
 [LLM API] Accumulated thinking: 3421 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3421, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(806), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3421, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated thinking: 3425 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3425, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(807), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3425, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 3428 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3428, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(808), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3428, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 3431 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3431, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(809), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3431, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 3436 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3436, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(810), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3436, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 3443 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3443, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(811), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3443, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " authorization"
 [LLM API] tagBuffer:  authorization
 [LLM API] Accumulated thinking: 3457 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3457, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(812), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3457, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ["
 [LLM API] tagBuffer:  [
 [LLM API] Accumulated thinking: 3459 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3459, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(813), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3459, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "Source"
 [LLM API] tagBuffer: Source
 [LLM API] Accumulated thinking: 3465 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3465, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(814), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3465, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ":"
 [LLM API] tagBuffer: :
 [LLM API] Accumulated thinking: 3466 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3466, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(815), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3466, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " {"
 [LLM API] tagBuffer:  {
 [LLM API] Accumulated thinking: 3468 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3468, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(816), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3468, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "CB"
 [LLM API] tagBuffer: CB
 [LLM API] Accumulated thinking: 3470 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3470, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(817), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3470, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "9"
 [LLM API] tagBuffer: 9
 [LLM API] Accumulated thinking: 3471 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3471, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(818), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3471, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "F"
 [LLM API] tagBuffer: F
 [LLM API] Accumulated thinking: 3472 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3472, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(819), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3472, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "6"
 [LLM API] tagBuffer: 6
 [LLM API] Accumulated thinking: 3473 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3473, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(820), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3473, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "C"
 [LLM API] tagBuffer: C
 [LLM API] Accumulated thinking: 3474 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3474, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(821), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3474, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated thinking: 3475 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3475, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(822), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3475, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "E"
 [LLM API] tagBuffer: E
 [LLM API] Accumulated thinking: 3476 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3476, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(823), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3476, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3477 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3477, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(824), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3477, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated thinking: 3478 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3478, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(825), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3478, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "FF"
 [LLM API] tagBuffer: FF
 [LLM API] Accumulated thinking: 3480 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3480, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(826), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3480, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "9"
 [LLM API] tagBuffer: 9
 [LLM API] Accumulated thinking: 3481 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3481, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(827), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3481, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3482 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3482, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(828), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3482, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated thinking: 3483 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3483, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(829), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3483, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "AF"
 [LLM API] tagBuffer: AF
 [LLM API] Accumulated thinking: 3485 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3485, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(830), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3485, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "7"
 [LLM API] tagBuffer: 7
 [LLM API] Accumulated thinking: 3486 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3486, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(831), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3486, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-B"
 [LLM API] tagBuffer: -B
 [LLM API] Accumulated thinking: 3488 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3488, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(832), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3488, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated thinking: 3490 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3490, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(833), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3490, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "8"
 [LLM API] tagBuffer: 8
 [LLM API] Accumulated thinking: 3491 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3491, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(834), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3491, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3492 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3492, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(835), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3492, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "5"
 [LLM API] tagBuffer: 5
 [LLM API] Accumulated thinking: 3493 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3493, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(836), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3493, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "A"
 [LLM API] tagBuffer: A
 [LLM API] Accumulated thinking: 3494 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3494, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(837), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3494, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "66"
 [LLM API] tagBuffer: 66
 [LLM API] Accumulated thinking: 3496 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3496, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(838), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3496, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "F"
 [LLM API] tagBuffer: F
 [LLM API] Accumulated thinking: 3497 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3497, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(839), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3497, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "9"
 [LLM API] tagBuffer: 9
 [LLM API] Accumulated thinking: 3498 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3498, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(840), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3498, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "27"
 [LLM API] tagBuffer: 27
 [LLM API] Accumulated thinking: 3500 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3500, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(841), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3500, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "58"
 [LLM API] tagBuffer: 58
 [LLM API] Accumulated thinking: 3502 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3502, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(842), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3502, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "B"
 [LLM API] tagBuffer: B
 [LLM API] Accumulated thinking: 3503 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3503, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(843), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3503, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "1"
 [LLM API] tagBuffer: 1
 [LLM API] Accumulated thinking: 3504 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3504, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(844), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3504, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "}]\n"
 [LLM API] tagBuffer: }]

 [LLM API] Accumulated thinking: 3507 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3507, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(845), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3507, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3508 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3508, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(846), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3508, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Both"
 [LLM API] tagBuffer:  Both
 [LLM API] Accumulated thinking: 3513 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3513, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(847), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3513, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated thinking: 3524 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3524, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(848), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3524, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " work"
 [LLM API] tagBuffer:  work
 [LLM API] Accumulated thinking: 3529 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3529, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(849), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3529, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " together"
 [LLM API] tagBuffer:  together
 [LLM API] Accumulated thinking: 3538 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3538, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(850), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3538, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 3539 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3539, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(851), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3539, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " with"
 [LLM API] tagBuffer:  with
 [LLM API] Accumulated thinking: 3544 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3544, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(852), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3544, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 3545 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3545, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(853), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3545, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 3547 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3547, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(854), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3547, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 3549 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3549, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(855), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3549, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3550 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3550, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(856), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3550, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 3551 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3551, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(857), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3551, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 3553 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3553, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(858), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3553, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated thinking: 3555 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3555, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(859), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3555, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " managing"
 [LLM API] tagBuffer:  managing
 [LLM API] Accumulated thinking: 3564 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3564, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(860), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3564, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 3568 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3568, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(861), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3568, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " administrative"
 [LLM API] tagBuffer:  administrative
 [LLM API] Accumulated thinking: 3583 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3583, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(862), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3583, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " process"
 [LLM API] tagBuffer:  process
 [LLM API] Accumulated thinking: 3591 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3591, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(863), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3591, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated thinking: 3595 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3595, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(864), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3595, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated thinking: 3596 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3596, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(865), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3596, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 3598 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3598, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(866), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3598, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 3600 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3600, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(867), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3600, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3601 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3601, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(868), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3601, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 3602 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3602, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(869), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3602, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 3604 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3604, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(870), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3604, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "40"
 [LLM API] tagBuffer: 40
 [LLM API] Accumulated thinking: 3606 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3606, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(871), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3606, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " managing"
 [LLM API] tagBuffer:  managing
 [LLM API] Accumulated thinking: 3615 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3615, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(872), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3615, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 3619 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3619, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(873), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3619, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " security"
 [LLM API] tagBuffer:  security
 [LLM API] Accumulated thinking: 3628 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3628, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(874), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3628, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " screening"
 [LLM API] tagBuffer:  screening
 [LLM API] Accumulated thinking: 3638 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3638, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(875), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3638, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated thinking: 3651 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3651, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(876), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3651, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n\n"
 [LLM API] tagBuffer: 


 [LLM API] Accumulated thinking: 3653 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3653, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(877), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3653, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "However"
 [LLM API] tagBuffer: However
 [LLM API] Accumulated thinking: 3660 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3660, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(878), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3660, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 3661 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3661, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(879), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3661, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " when"
 [LLM API] tagBuffer:  when
 [LLM API] Accumulated thinking: 3666 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3666, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(880), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3666, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " someone"
 [LLM API] tagBuffer:  someone
 [LLM API] Accumulated thinking: 3674 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3674, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(881), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3674, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " asks"
 [LLM API] tagBuffer:  asks
 [LLM API] Accumulated thinking: 3679 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3679, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(882), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3679, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated thinking: 3685 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3685, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(883), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3685, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 3695 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3695, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(884), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3695, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated thinking: 3697 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3697, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(885), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3697, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "man"
 [LLM API] tagBuffer: man
 [LLM API] Accumulated thinking: 3700 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3700, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(886), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3700, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ages"
 [LLM API] tagBuffer: ages
 [LLM API] Accumulated thinking: 3704 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3704, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(887), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3704, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated thinking: 3705 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3705, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(888), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3705, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated thinking: 3708 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3708, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(889), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3708, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated thinking: 3711 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3711, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(890), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3711, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated thinking: 3716 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3716, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(891), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3716, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated thinking: 3723 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3723, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(892), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3723, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 3724 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3724, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(893), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3724, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " they"
 [LLM API] tagBuffer:  they
 [LLM API] Accumulated thinking: 3729 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3729, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(894), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3729, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " are"
 [LLM API] tagBuffer:  are
 [LLM API] Accumulated thinking: 3733 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3733, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(895), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3733, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " typically"
 [LLM API] tagBuffer:  typically
 [LLM API] Accumulated thinking: 3743 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3743, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(896), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3743, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " referring"
 [LLM API] tagBuffer:  referring
 [LLM API] Accumulated thinking: 3753 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3753, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(897), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3753, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " to"
 [LLM API] tagBuffer:  to
 [LLM API] Accumulated thinking: 3756 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3756, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(898), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3756, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 3760 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3760, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(899), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3760, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " primary"
 [LLM API] tagBuffer:  primary
 [LLM API] Accumulated thinking: 3768 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3768, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(900), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3768, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " administrative"
 [LLM API] tagBuffer:  administrative
 [LLM API] Accumulated thinking: 3783 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3783, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(901), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3783, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated thinking: 3793 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3793, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(902), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3793, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated thinking: 3798 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3798, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(903), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3798, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " oversees"
 [LLM API] tagBuffer:  oversees
 [LLM API] Accumulated thinking: 3807 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3807, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(904), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3807, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 3811 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3811, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(905), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3811, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " entire"
 [LLM API] tagBuffer:  entire
 [LLM API] Accumulated thinking: 3818 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3818, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(906), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3818, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " process"
 [LLM API] tagBuffer:  process
 [LLM API] Accumulated thinking: 3826 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3826, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(907), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3826, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated thinking: 3827 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3827, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(908), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3827, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated thinking: 3833 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3833, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(909), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3833, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " would"
 [LLM API] tagBuffer:  would
 [LLM API] Accumulated thinking: 3839 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3839, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(910), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3839, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated thinking: 3842 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3842, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(911), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3842, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated thinking: 3846 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3846, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(912), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3846, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Palo"
 [LLM API] tagBuffer:  Palo
 [LLM API] Accumulated thinking: 3851 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3851, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(913), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3851, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated thinking: 3857 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3857, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(914), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3857, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Bad"
 [LLM API] tagBuffer:  Bad
 [LLM API] Accumulated thinking: 3861 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3861, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(915), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3861, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "ging"
 [LLM API] tagBuffer: ging
 [LLM API] Accumulated thinking: 3865 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3865, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(916), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3865, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " Procedure"
 [LLM API] tagBuffer:  Procedure
 [LLM API] Accumulated thinking: 3875 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3875, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(917), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3875, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated thinking: 3877 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3877, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(918), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3877, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated thinking: 3879 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3879, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(919), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3879, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated thinking: 3881 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3881, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(920), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3881, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated thinking: 3882 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3882, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(921), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3882, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated thinking: 3883 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3883, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(922), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3883, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated thinking: 3885 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3885, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(923), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3885, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated thinking: 3887 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3887, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(924), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3887, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ")."
 [LLM API] tagBuffer: ).
 [LLM API] Accumulated thinking: 3889 chars
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(925), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "</think>"
 [LLM API] tagBuffer: </think>
 [LLM API] Exited </think> tag, remaining buffer: 
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(926), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Keeping potential partial tag in buffer: 

 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(927), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "<"
 [LLM API] tagBuffer: 
<
 [LLM API] Keeping potential partial tag in buffer: 
<
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(928), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: "response"
 [LLM API] tagBuffer: 
<response
 [LLM API] Keeping potential partial tag in buffer: 
<response
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [ChatService] Streaming callback invoked {chunks: Array(929), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 0}
 [LLM API] Processing line: data: ">\n"
 [LLM API] tagBuffer: 
<response>

 [LLM API] Entered <response> tag, remaining buffer: 

 [LLM API] Accumulated response: 1 chars content: 

 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1}
 [ChatService] Streaming callback invoked {chunks: Array(930), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1 first 100 chars: 

 [MarkdownContent] Content changed: 

 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [LLM API] Processing line: data: "Based"
 [LLM API] tagBuffer: Based
 [LLM API] Accumulated response: 6 chars content: Based
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 6}
 [ChatService] Streaming callback invoked {chunks: Array(931), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 6}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 6 first 100 chars: 
Based
 [MarkdownContent] Content changed: 
Based
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " on"
 [LLM API] tagBuffer:  on
 [LLM API] Accumulated response: 9 chars content:  on
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 9}
 [ChatService] Streaming callback invoked {chunks: Array(932), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 9}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 9 first 100 chars: 
Based on
 [LLM API] Processing line: data: " my"
 [LLM API] tagBuffer:  my
 [LLM API] Accumulated response: 12 chars content:  my
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 12}
 [ChatService] Streaming callback invoked {chunks: Array(933), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 12}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 12 first 100 chars: 
Based on my
 [LLM API] Processing line: data: " search"
 [LLM API] tagBuffer:  search
 [LLM API] Accumulated response: 19 chars content:  search
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 19}
 [ChatService] Streaming callback invoked {chunks: Array(934), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 19}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 19 first 100 chars: 
Based on my search
 [MarkdownContent] Content changed: 
Based on my search
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " of"
 [LLM API] tagBuffer:  of
 [LLM API] Accumulated response: 22 chars content:  of
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 22}
 [ChatService] Streaming callback invoked {chunks: Array(935), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 22}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 22 first 100 chars: 
Based on my search of
 [LLM API] Processing line: data: " Palo"
 [LLM API] tagBuffer:  Palo
 [LLM API] Accumulated response: 27 chars content:  Palo
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 27}
 [ChatService] Streaming callback invoked {chunks: Array(936), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 27}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 27 first 100 chars: 
Based on my search of Palo
 [MarkdownContent] Content changed: 
Based on my search of Palo
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated response: 33 chars content:  Verde
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 33}
 [ChatService] Streaming callback invoked {chunks: Array(937), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 33}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 33 first 100 chars: 
Based on my search of Palo Verde
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated response: 44 chars content:  procedures
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 44}
 [ChatService] Streaming callback invoked {chunks: Array(938), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 44}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 44 first 100 chars: 
Based on my search of Palo Verde procedures
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated response: 45 chars content: ,
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 45}
 [ChatService] Streaming callback invoked {chunks: Array(939), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 45}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 45 first 100 chars: 
Based on my search of Palo Verde procedures,
 [LLM API] Processing line: data: " there"
 [LLM API] tagBuffer:  there
 [LLM API] Accumulated response: 51 chars content:  there
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 51}
 [ChatService] Streaming callback invoked {chunks: Array(940), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 51}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 51 first 100 chars: 
Based on my search of Palo Verde procedures, there
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " are"
 [LLM API] tagBuffer:  are
 [LLM API] Accumulated response: 55 chars content:  are
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 55}
 [ChatService] Streaming callback invoked {chunks: Array(941), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 55}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 55 first 100 chars: 
Based on my search of Palo Verde procedures, there are
 [LLM API] Processing line: data: " two"
 [LLM API] tagBuffer:  two
 [LLM API] Accumulated response: 59 chars content:  two
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 59}
 [ChatService] Streaming callback invoked {chunks: Array(942), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 59}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 59 first 100 chars: 
Based on my search of Palo Verde procedures, there are two
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " main"
 [LLM API] tagBuffer:  main
 [LLM API] Accumulated response: 64 chars content:  main
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 64}
 [ChatService] Streaming callback invoked {chunks: Array(943), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 64}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 64 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated response: 75 chars content:  procedures
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 75}
 [ChatService] Streaming callback invoked {chunks: Array(944), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 75}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 75 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " involved"
 [LLM API] tagBuffer:  involved
 [LLM API] Accumulated response: 84 chars content:  involved
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 84}
 [ChatService] Streaming callback invoked {chunks: Array(945), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 84}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 84 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved
 [LLM API] Processing line: data: " in"
 [LLM API] tagBuffer:  in
 [LLM API] Accumulated response: 87 chars content:  in
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 87}
 [ChatService] Streaming callback invoked {chunks: Array(946), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 87}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 87 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " managing"
 [LLM API] tagBuffer:  managing
 [LLM API] Accumulated response: 96 chars content:  managing
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 96}
 [ChatService] Streaming callback invoked {chunks: Array(947), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 96}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 96 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 99 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 99}
 [ChatService] Streaming callback invoked {chunks: Array(948), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 99}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 99 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing un
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 102 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 102}
 [ChatService] Streaming callback invoked {chunks: Array(949), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 102}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 102 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 107 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 107}
 [ChatService] Streaming callback invoked {chunks: Array(950), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 107}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 107 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated response: 114 chars content:  access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 114}
 [ChatService] Streaming callback invoked {chunks: Array(951), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 114}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 114 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated response: 115 chars content: ,
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 115}
 [ChatService] Streaming callback invoked {chunks: Array(952), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 115}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 115 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " but"
 [LLM API] tagBuffer:  but
 [LLM API] Accumulated response: 119 chars content:  but
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 119}
 [ChatService] Streaming callback invoked {chunks: Array(953), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 119}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 119 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 123 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 123}
 [ChatService] Streaming callback invoked {chunks: Array(954), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 123}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 123 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " primary"
 [LLM API] tagBuffer:  primary
 [LLM API] Accumulated response: 131 chars content:  primary
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 131}
 [ChatService] Streaming callback invoked {chunks: Array(955), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 131}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 131 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated response: 141 chars content:  procedure
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 141}
 [ChatService] Streaming callback invoked {chunks: Array(956), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 141}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 141 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated response: 146 chars content:  that
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 146}
 [ChatService] Streaming callback invoked {chunks: Array(957), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 146}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 146 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " manages"
 [LLM API] tagBuffer:  manages
 [LLM API] Accumulated response: 154 chars content:  manages
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 154}
 [ChatService] Streaming callback invoked {chunks: Array(958), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 154}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 154 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 158 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 158}
 [ChatService] Streaming callback invoked {chunks: Array(959), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 158}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 158 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " overall"
 [LLM API] tagBuffer:  overall
 [LLM API] Accumulated response: 166 chars content:  overall
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 166}
 [ChatService] Streaming callback invoked {chunks: Array(960), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 166}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 166 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 169 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 169}
 [ChatService] Streaming callback invoked {chunks: Array(961), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 169}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 169 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 172 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 172}
 [ChatService] Streaming callback invoked {chunks: Array(962), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 172}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 172 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 177 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 177}
 [ChatService] Streaming callback invoked {chunks: Array(963), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 177}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 177 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated response: 184 chars content:  access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 184}
 [ChatService] Streaming callback invoked {chunks: Array(964), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 184}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 184 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " process"
 [LLM API] tagBuffer:  process
 [LLM API] Accumulated response: 192 chars content:  process
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 192}
 [ChatService] Streaming callback invoked {chunks: Array(965), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 192}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 192 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated response: 195 chars content:  is
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 195}
 [ChatService] Streaming callback invoked {chunks: Array(966), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 195}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 195 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ":\n\n"
 [LLM API] tagBuffer: :


 [LLM API] Accumulated response: 198 chars content: :


 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 198}
 [ChatService] Streaming callback invoked {chunks: Array(967), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 198}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 198 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "**"
 [LLM API] tagBuffer: **
 [LLM API] Accumulated response: 200 chars content: **
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 200}
 [ChatService] Streaming callback invoked {chunks: Array(968), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 200}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 200 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated response: 202 chars content: 20
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 202}
 [ChatService] Streaming callback invoked {chunks: Array(969), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 202}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 202 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated response: 204 chars content: DP
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 204}
 [ChatService] Streaming callback invoked {chunks: Array(970), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 204}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 204 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 205 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 205}
 [ChatService] Streaming callback invoked {chunks: Array(971), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 205}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 205 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated response: 206 chars content: 0
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 206}
 [ChatService] Streaming callback invoked {chunks: Array(972), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 206}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 206 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated response: 208 chars content: SK
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 208}
 [ChatService] Streaming callback invoked {chunks: Array(973), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 208}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 208 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated response: 210 chars content: 39
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 210}
 [ChatService] Streaming callback invoked {chunks: Array(974), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 210}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 210 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated response: 212 chars content:  (
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 212}
 [ChatService] Streaming callback invoked {chunks: Array(975), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 212}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 212 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "P"
 [LLM API] tagBuffer: P
 [LLM API] Accumulated response: 213 chars content: P
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 213}
 [ChatService] Streaming callback invoked {chunks: Array(976), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 213}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 213 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "alo"
 [LLM API] tagBuffer: alo
 [LLM API] Accumulated response: 216 chars content: alo
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 216}
 [ChatService] Streaming callback invoked {chunks: Array(977), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 216}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 216 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated response: 222 chars content:  Verde
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 222}
 [ChatService] Streaming callback invoked {chunks: Array(978), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 222}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 222 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Bad"
 [LLM API] tagBuffer:  Bad
 [LLM API] Accumulated response: 226 chars content:  Bad
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 226}
 [ChatService] Streaming callback invoked {chunks: Array(979), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 226}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 226 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "ging"
 [LLM API] tagBuffer: ging
 [LLM API] Accumulated response: 230 chars content: ging
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 230}
 [ChatService] Streaming callback invoked {chunks: Array(980), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 230}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 230 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Procedure"
 [LLM API] tagBuffer:  Procedure
 [LLM API] Accumulated response: 240 chars content:  Procedure
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 240}
 [ChatService] Streaming callback invoked {chunks: Array(981), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 240}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 240 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ")**"
 [LLM API] tagBuffer: )**
 [LLM API] Accumulated response: 243 chars content: )**
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 243}
 [ChatService] Streaming callback invoked {chunks: Array(982), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 243}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 243 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated response: 245 chars content:  -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 245}
 [ChatService] Streaming callback invoked {chunks: Array(983), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 245}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 245 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Rev"
 [LLM API] tagBuffer:  Rev
 [LLM API] Accumulated response: 249 chars content:  Rev
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 249}
 [ChatService] Streaming callback invoked {chunks: Array(984), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 249}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 249 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated response: 250 chars content:  
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 250}
 [ChatService] Streaming callback invoked {chunks: Array(985), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 250}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 250 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "04"
 [LLM API] tagBuffer: 04
 [LLM API] Accumulated response: 252 chars content: 04
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 252}
 [ChatService] Streaming callback invoked {chunks: Array(986), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 252}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 252 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "2"
 [LLM API] tagBuffer: 2
 [LLM API] Accumulated response: 253 chars content: 2
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 253}
 [ChatService] Streaming callback invoked {chunks: Array(987), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 253}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 253 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "\n\n"
 [LLM API] tagBuffer: 


 [LLM API] Accumulated response: 255 chars content: 


 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 255}
 [ChatService] Streaming callback invoked {chunks: Array(988), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 255}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 255 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "This"
 [LLM API] tagBuffer: This
 [LLM API] Accumulated response: 259 chars content: This
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 259}
 [ChatService] Streaming callback invoked {chunks: Array(989), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 259}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 259 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated response: 269 chars content:  procedure
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 269}
 [ChatService] Streaming callback invoked {chunks: Array(990), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 269}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 269 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " serves"
 [LLM API] tagBuffer:  serves
 [LLM API] Accumulated response: 276 chars content:  serves
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 276}
 [ChatService] Streaming callback invoked {chunks: Array(991), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 276}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 276 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " as"
 [LLM API] tagBuffer:  as
 [LLM API] Accumulated response: 279 chars content:  as
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 279}
 [ChatService] Streaming callback invoked {chunks: Array(992), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 279}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 279 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 283 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 283}
 [ChatService] Streaming callback invoked {chunks: Array(993), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 283}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 283 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " primary"
 [LLM API] tagBuffer:  primary
 [LLM API] Accumulated response: 291 chars content:  primary
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 291}
 [ChatService] Streaming callback invoked {chunks: Array(994), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 291}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 291 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " manager"
 [LLM API] tagBuffer:  manager
 [LLM API] Accumulated response: 299 chars content:  manager
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 299}
 [ChatService] Streaming callback invoked {chunks: Array(995), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 299}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 299 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " of"
 [LLM API] tagBuffer:  of
 [LLM API] Accumulated response: 302 chars content:  of
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 302}
 [ChatService] Streaming callback invoked {chunks: Array(996), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 302}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 302 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 305 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 305}
 [ChatService] Streaming callback invoked {chunks: Array(997), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 305}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 305 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 308 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 308}
 [ChatService] Streaming callback invoked {chunks: Array(998), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 308}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 308 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 313 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 313}
 [ChatService] Streaming callback invoked {chunks: Array(999), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 313}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 313 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated response: 320 chars content:  access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 320}
 [ChatService] Streaming callback invoked {chunks: Array(1000), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 320}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 320 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " because"
 [LLM API] tagBuffer:  because
 [LLM API] Accumulated response: 328 chars content:  because
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 328}
 [ChatService] Streaming callback invoked {chunks: Array(1001), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 328}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 328 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " it"
 [LLM API] tagBuffer:  it
 [LLM API] Accumulated response: 331 chars content:  it
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 331}
 [ChatService] Streaming callback invoked {chunks: Array(1002), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 331}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 331 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ":\n\n"
 [LLM API] tagBuffer: :


 [LLM API] Accumulated response: 334 chars content: :


 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 334}
 [ChatService] Streaming callback invoked {chunks: Array(1003), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 334}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 334 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "1"
 [LLM API] tagBuffer: 1
 [LLM API] Accumulated response: 335 chars content: 1
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 335}
 [ChatService] Streaming callback invoked {chunks: Array(1004), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 335}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 335 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated response: 336 chars content: .
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 336}
 [ChatService] Streaming callback invoked {chunks: Array(1005), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 336}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 336 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " **"
 [LLM API] tagBuffer:  **
 [LLM API] Accumulated response: 339 chars content:  **
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 339}
 [ChatService] Streaming callback invoked {chunks: Array(1006), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 339}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 339 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "Man"
 [LLM API] tagBuffer: Man
 [LLM API] Accumulated response: 342 chars content: Man
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 342}
 [ChatService] Streaming callback invoked {chunks: Array(1007), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 342}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 342 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "ages"
 [LLM API] tagBuffer: ages
 [LLM API] Accumulated response: 346 chars content: ages
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 346}
 [ChatService] Streaming callback invoked {chunks: Array(1008), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 346}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 346 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 350 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 350}
 [ChatService] Streaming callback invoked {chunks: Array(1009), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 350}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 350 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " complete"
 [LLM API] tagBuffer:  complete
 [LLM API] Accumulated response: 359 chars content:  complete
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 359}
 [ChatService] Streaming callback invoked {chunks: Array(1010), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 359}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 359 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 362 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 362}
 [ChatService] Streaming callback invoked {chunks: Array(1011), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 362}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 362 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 365 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 365}
 [ChatService] Streaming callback invoked {chunks: Array(1012), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 365}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 365 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 370 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 370}
 [ChatService] Streaming callback invoked {chunks: Array(1013), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 370}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 370 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated response: 377 chars content:  access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 377}
 [ChatService] Streaming callback invoked {chunks: Array(1014), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 377}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 377 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " workflow"
 [LLM API] tagBuffer:  workflow
 [LLM API] Accumulated response: 386 chars content:  workflow
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 386}
 [ChatService] Streaming callback invoked {chunks: Array(1015), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 386}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 386 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "**"
 [LLM API] tagBuffer: **
 [LLM API] Accumulated response: 388 chars content: **
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 388}
 [ChatService] Streaming callback invoked {chunks: Array(1016), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 388}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 388 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated response: 390 chars content:  -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 390}
 [ChatService] Streaming callback invoked {chunks: Array(1017), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 390}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 390 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " From"
 [LLM API] tagBuffer:  From
 [LLM API] Accumulated response: 395 chars content:  From
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 395}
 [ChatService] Streaming callback invoked {chunks: Array(1018), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 395}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 395 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " initial"
 [LLM API] tagBuffer:  initial
 [LLM API] Accumulated response: 403 chars content:  initial
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 403}
 [ChatService] Streaming callback invoked {chunks: Array(1019), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 403}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 403 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " request"
 [LLM API] tagBuffer:  request
 [LLM API] Accumulated response: 411 chars content:  request
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 411}
 [ChatService] Streaming callback invoked {chunks: Array(1020), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 411}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 411 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " through"
 [LLM API] tagBuffer:  through
 [LLM API] Accumulated response: 419 chars content:  through
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 419}
 [ChatService] Streaming callback invoked {chunks: Array(1021), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 419}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 419 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " AC"
 [LLM API] tagBuffer:  AC
 [LLM API] Accumulated response: 422 chars content:  AC
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 422}
 [ChatService] Streaming callback invoked {chunks: Array(1022), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 422}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 422 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "AD"
 [LLM API] tagBuffer: AD
 [LLM API] Accumulated response: 424 chars content: AD
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 424}
 [ChatService] Streaming callback invoked {chunks: Array(1023), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 424}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 424 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Badge"
 [LLM API] tagBuffer:  Badge
 [LLM API] Accumulated response: 430 chars content:  Badge
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 430}
 [ChatService] Streaming callback invoked {chunks: Array(1024), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 430}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 430 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " issuance"
 [LLM API] tagBuffer:  issuance
 [LLM API] Accumulated response: 439 chars content:  issuance
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 439}
 [ChatService] Streaming callback invoked {chunks: Array(1025), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 439}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 439 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated response: 443 chars content:  and
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 443}
 [ChatService] Streaming callback invoked {chunks: Array(1026), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 443}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 443 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " termination"
 [LLM API] tagBuffer:  termination
 [LLM API] Accumulated response: 455 chars content:  termination
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 455}
 [ChatService] Streaming callback invoked {chunks: Array(1027), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 455}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 455 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " ["
 [LLM API] tagBuffer:  [
 [LLM API] Accumulated response: 457 chars content:  [
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 457}
 [ChatService] Streaming callback invoked {chunks: Array(1028), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 457}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 457 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "Source"
 [LLM API] tagBuffer: Source
 [LLM API] Accumulated response: 463 chars content: Source
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 463}
 [ChatService] Streaming callback invoked {chunks: Array(1029), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 463}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 463 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ":"
 [LLM API] tagBuffer: :
 [LLM API] Accumulated response: 464 chars content: :
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 464}
 [ChatService] Streaming callback invoked {chunks: Array(1030), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 464}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 464 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " {"
 [LLM API] tagBuffer:  {
 [LLM API] Accumulated response: 466 chars content:  {
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 466}
 [ChatService] Streaming callback invoked {chunks: Array(1031), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 466}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 466 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "CB"
 [LLM API] tagBuffer: CB
 [LLM API] Accumulated response: 468 chars content: CB
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 468}
 [ChatService] Streaming callback invoked {chunks: Array(1032), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 468}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 468 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "9"
 [LLM API] tagBuffer: 9
 [LLM API] Accumulated response: 469 chars content: 9
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 469}
 [ChatService] Streaming callback invoked {chunks: Array(1033), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 469}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 469 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "F"
 [LLM API] tagBuffer: F
 [LLM API] Accumulated response: 470 chars content: F
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 470}
 [ChatService] Streaming callback invoked {chunks: Array(1034), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 470}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 470 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "6"
 [LLM API] tagBuffer: 6
 [LLM API] Accumulated response: 471 chars content: 6
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 471}
 [ChatService] Streaming callback invoked {chunks: Array(1035), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 471}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 471 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "C"
 [LLM API] tagBuffer: C
 [LLM API] Accumulated response: 472 chars content: C
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 472}
 [ChatService] Streaming callback invoked {chunks: Array(1036), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 472}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 472 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated response: 473 chars content: 4
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 473}
 [ChatService] Streaming callback invoked {chunks: Array(1037), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 473}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 473 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "E"
 [LLM API] tagBuffer: E
 [LLM API] Accumulated response: 474 chars content: E
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 474}
 [ChatService] Streaming callback invoked {chunks: Array(1038), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 474}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 474 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 475 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 475}
 [ChatService] Streaming callback invoked {chunks: Array(1039), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 475}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 475 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated response: 476 chars content: 4
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 476}
 [ChatService] Streaming callback invoked {chunks: Array(1040), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 476}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 476 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "FF"
 [LLM API] tagBuffer: FF
 [LLM API] Accumulated response: 478 chars content: FF
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 478}
 [ChatService] Streaming callback invoked {chunks: Array(1041), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 478}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 478 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "9"
 [LLM API] tagBuffer: 9
 [LLM API] Accumulated response: 479 chars content: 9
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 479}
 [ChatService] Streaming callback invoked {chunks: Array(1042), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 479}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 479 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 480 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 480}
 [ChatService] Streaming callback invoked {chunks: Array(1043), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 480}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 480 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated response: 481 chars content: 4
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 481}
 [ChatService] Streaming callback invoked {chunks: Array(1044), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 481}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 481 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "AF"
 [LLM API] tagBuffer: AF
 [LLM API] Accumulated response: 483 chars content: AF
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 483}
 [ChatService] Streaming callback invoked {chunks: Array(1045), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 483}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 483 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "7"
 [LLM API] tagBuffer: 7
 [LLM API] Accumulated response: 484 chars content: 7
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 484}
 [ChatService] Streaming callback invoked {chunks: Array(1046), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 484}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 484 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "-B"
 [LLM API] tagBuffer: -B
 [LLM API] Accumulated response: 486 chars content: -B
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 486}
 [ChatService] Streaming callback invoked {chunks: Array(1047), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 486}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 486 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated response: 488 chars content: 39
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 488}
 [ChatService] Streaming callback invoked {chunks: Array(1048), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 488}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 488 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "8"
 [LLM API] tagBuffer: 8
 [LLM API] Accumulated response: 489 chars content: 8
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 489}
 [ChatService] Streaming callback invoked {chunks: Array(1049), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 489}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 489 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 490 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 490}
 [ChatService] Streaming callback invoked {chunks: Array(1050), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 490}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 490 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "5"
 [LLM API] tagBuffer: 5
 [LLM API] Accumulated response: 491 chars content: 5
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 491}
 [ChatService] Streaming callback invoked {chunks: Array(1051), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 491}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 491 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "A"
 [LLM API] tagBuffer: A
 [LLM API] Accumulated response: 492 chars content: A
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 492}
 [ChatService] Streaming callback invoked {chunks: Array(1052), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 492}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 492 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "66"
 [LLM API] tagBuffer: 66
 [LLM API] Accumulated response: 494 chars content: 66
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 494}
 [ChatService] Streaming callback invoked {chunks: Array(1053), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 494}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 494 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "F"
 [LLM API] tagBuffer: F
 [LLM API] Accumulated response: 495 chars content: F
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 495}
 [ChatService] Streaming callback invoked {chunks: Array(1054), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 495}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 495 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "9"
 [LLM API] tagBuffer: 9
 [LLM API] Accumulated response: 496 chars content: 9
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 496}
 [ChatService] Streaming callback invoked {chunks: Array(1055), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 496}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 496 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "27"
 [LLM API] tagBuffer: 27
 [LLM API] Accumulated response: 498 chars content: 27
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 498}
 [ChatService] Streaming callback invoked {chunks: Array(1056), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 498}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 498 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "58"
 [LLM API] tagBuffer: 58
 [LLM API] Accumulated response: 500 chars content: 58
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 500}
 [ChatService] Streaming callback invoked {chunks: Array(1057), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 500}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 500 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "B"
 [LLM API] tagBuffer: B
 [LLM API] Accumulated response: 501 chars content: B
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 501}
 [ChatService] Streaming callback invoked {chunks: Array(1058), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 501}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 501 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "1"
 [LLM API] tagBuffer: 1
 [LLM API] Accumulated response: 502 chars content: 1
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 502}
 [ChatService] Streaming callback invoked {chunks: Array(1059), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 502}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 502 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "}"
 [LLM API] tagBuffer: }
 [LLM API] Accumulated response: 503 chars content: }
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 503}
 [ChatService] Streaming callback invoked {chunks: Array(1060), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 503}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 503 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "]\n\n"
 [LLM API] tagBuffer: ]


 [LLM API] Accumulated response: 506 chars content: ]


 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 506}
 [ChatService] Streaming callback invoked {chunks: Array(1061), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 506}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 506 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "2"
 [LLM API] tagBuffer: 2
 [LLM API] Accumulated response: 507 chars content: 2
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 507}
 [ChatService] Streaming callback invoked {chunks: Array(1062), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 507}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 507 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated response: 508 chars content: .
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 508}
 [ChatService] Streaming callback invoked {chunks: Array(1063), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 508}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 508 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " **"
 [LLM API] tagBuffer:  **
 [LLM API] Accumulated response: 511 chars content:  **
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 511}
 [ChatService] Streaming callback invoked {chunks: Array(1064), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 511}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 511 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "Handles"
 [LLM API] tagBuffer: Handles
 [LLM API] Accumulated response: 518 chars content: Handles
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 518}
 [ChatService] Streaming callback invoked {chunks: Array(1065), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 518}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 518 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " all"
 [LLM API] tagBuffer:  all
 [LLM API] Accumulated response: 522 chars content:  all
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 522}
 [ChatService] Streaming callback invoked {chunks: Array(1066), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 522}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 522 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " administrative"
 [LLM API] tagBuffer:  administrative
 [LLM API] Accumulated response: 537 chars content:  administrative
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 537}
 [ChatService] Streaming callback invoked {chunks: Array(1067), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 537}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 537 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " aspects"
 [LLM API] tagBuffer:  aspects
 [LLM API] Accumulated response: 545 chars content:  aspects
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 545}
 [ChatService] Streaming callback invoked {chunks: Array(1068), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 545}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 545 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "**"
 [LLM API] tagBuffer: **
 [LLM API] Accumulated response: 547 chars content: **
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 547}
 [ChatService] Streaming callback invoked {chunks: Array(1069), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 547}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 547 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " including"
 [LLM API] tagBuffer:  including
 [LLM API] Accumulated response: 557 chars content:  including
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 557}
 [ChatService] Streaming callback invoked {chunks: Array(1070), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 557}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 557 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ":\n"
 [LLM API] tagBuffer: :

 [LLM API] Accumulated response: 559 chars content: :

 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 559}
 [ChatService] Streaming callback invoked {chunks: Array(1071), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 559}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 559 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated response: 561 chars content:   
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 561}
 [ChatService] Streaming callback invoked {chunks: Array(1072), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 561}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 561 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated response: 563 chars content:  -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 563}
 [ChatService] Streaming callback invoked {chunks: Array(1073), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 563}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 563 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Processing"
 [LLM API] tagBuffer:  Processing
 [LLM API] Accumulated response: 574 chars content:  Processing
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 574}
 [ChatService] Streaming callback invoked {chunks: Array(1074), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 574}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 574 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " Un"
 [LLM API] tagBuffer:  Un
 [LLM API] Accumulated response: 577 chars content:  Un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 577}
 [ChatService] Streaming callback invoked {chunks: Array(1075), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 577}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 577 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 580 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 580}
 [ChatService] Streaming callback invoked {chunks: Array(1076), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 580}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 580 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 585 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 585}
 [ChatService] Streaming callback invoked {chunks: Array(1077), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 585}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 585 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Access"
 [LLM API] tagBuffer:  Access
 [LLM API] Accumulated response: 592 chars content:  Access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 592}
 [ChatService] Streaming callback invoked {chunks: Array(1078), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 592}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 592 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " Requests"
 [LLM API] tagBuffer:  Requests
 [LLM API] Accumulated response: 601 chars content:  Requests
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 601}
 [ChatService] Streaming callback invoked {chunks: Array(1079), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 601}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 601 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated response: 603 chars content:  (
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 603}
 [ChatService] Streaming callback invoked {chunks: Array(1080), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 603}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 603 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "U"
 [LLM API] tagBuffer: U
 [LLM API] Accumulated response: 604 chars content: U
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 604}
 [ChatService] Streaming callback invoked {chunks: Array(1081), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 604}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 604 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "AR"
 [LLM API] tagBuffer: AR
 [LLM API] Accumulated response: 606 chars content: AR
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 606}
 [ChatService] Streaming callback invoked {chunks: Array(1082), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 606}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 606 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ")"
 [LLM API] tagBuffer: )
 [LLM API] Accumulated response: 607 chars content: )
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 607}
 [ChatService] Streaming callback invoked {chunks: Array(1083), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 607}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 607 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " through"
 [LLM API] tagBuffer:  through
 [LLM API] Accumulated response: 615 chars content:  through
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 615}
 [ChatService] Streaming callback invoked {chunks: Array(1084), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 615}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 615 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 619 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 619}
 [ChatService] Streaming callback invoked {chunks: Array(1085), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 619}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 619 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " Security"
 [LLM API] tagBuffer:  Security
 [LLM API] Accumulated response: 628 chars content:  Security
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 628}
 [ChatService] Streaming callback invoked {chunks: Array(1086), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 628}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 628 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Screening"
 [LLM API] tagBuffer:  Screening
 [LLM API] Accumulated response: 638 chars content:  Screening
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 638}
 [ChatService] Streaming callback invoked {chunks: Array(1087), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 638}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 638 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Information"
 [LLM API] tagBuffer:  Information
 [LLM API] Accumulated response: 650 chars content:  Information
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 650}
 [ChatService] Streaming callback invoked {chunks: Array(1088), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 650}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 650 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " System"
 [LLM API] tagBuffer:  System
 [LLM API] Accumulated response: 657 chars content:  System
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 657}
 [ChatService] Streaming callback invoked {chunks: Array(1089), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 657}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 657 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated response: 659 chars content:  (
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 659}
 [ChatService] Streaming callback invoked {chunks: Array(1090), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 659}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 659 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "SS"
 [LLM API] tagBuffer: SS
 [LLM API] Accumulated response: 661 chars content: SS
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 661}
 [ChatService] Streaming callback invoked {chunks: Array(1091), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 661}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 661 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "IS"
 [LLM API] tagBuffer: IS
 [LLM API] Accumulated response: 663 chars content: IS
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 663}
 [ChatService] Streaming callback invoked {chunks: Array(1092), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 663}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 663 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ")\n"
 [LLM API] tagBuffer: )

 [LLM API] Accumulated response: 665 chars content: )

 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 665}
 [ChatService] Streaming callback invoked {chunks: Array(1093), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 665}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 665 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated response: 667 chars content:   
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 667}
 [ChatService] Streaming callback invoked {chunks: Array(1094), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 667}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 667 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated response: 669 chars content:  -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 669}
 [ChatService] Streaming callback invoked {chunks: Array(1095), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 669}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 669 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Ver"
 [LLM API] tagBuffer:  Ver
 [LLM API] Accumulated response: 673 chars content:  Ver
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 673}
 [ChatService] Streaming callback invoked {chunks: Array(1096), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 673}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 673 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "ifying"
 [LLM API] tagBuffer: ifying
 [LLM API] Accumulated response: 679 chars content: ifying
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 679}
 [ChatService] Streaming callback invoked {chunks: Array(1097), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 679}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 679 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated response: 684 chars content:  that
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 684}
 [ChatService] Streaming callback invoked {chunks: Array(1098), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 684}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 684 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " individuals"
 [LLM API] tagBuffer:  individuals
 [LLM API] Accumulated response: 696 chars content:  individuals
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 696}
 [ChatService] Streaming callback invoked {chunks: Array(1099), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 696}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 696 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " meet"
 [LLM API] tagBuffer:  meet
 [LLM API] Accumulated response: 701 chars content:  meet
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 701}
 [ChatService] Streaming callback invoked {chunks: Array(1100), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 701}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 701 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " all"
 [LLM API] tagBuffer:  all
 [LLM API] Accumulated response: 705 chars content:  all
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 705}
 [ChatService] Streaming callback invoked {chunks: Array(1101), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 705}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 705 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated response: 718 chars content:  requirements
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 718}
 [ChatService] Streaming callback invoked {chunks: Array(1102), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 718}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 718 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated response: 722 chars content:  for
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 722}
 [ChatService] Streaming callback invoked {chunks: Array(1103), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 722}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 722 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 725 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 725}
 [ChatService] Streaming callback invoked {chunks: Array(1104), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 725}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 725 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 728 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 728}
 [ChatService] Streaming callback invoked {chunks: Array(1105), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 728}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 728 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 733 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 733}
 [ChatService] Streaming callback invoked {chunks: Array(1106), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 733}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 733 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated response: 740 chars content:  access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 740}
 [ChatService] Streaming callback invoked {chunks: Array(1107), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 740}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 740 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated response: 741 chars content: 

 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 741}
 [ChatService] Streaming callback invoked {chunks: Array(1108), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 741}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 741 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated response: 743 chars content:   
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 743}
 [ChatService] Streaming callback invoked {chunks: Array(1109), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 743}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 743 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated response: 745 chars content:  -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 745}
 [ChatService] Streaming callback invoked {chunks: Array(1110), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 745}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 745 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Iss"
 [LLM API] tagBuffer:  Iss
 [LLM API] Accumulated response: 749 chars content:  Iss
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 749}
 [ChatService] Streaming callback invoked {chunks: Array(1111), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 749}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 749 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "uing"
 [LLM API] tagBuffer: uing
 [LLM API] Accumulated response: 753 chars content: uing
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 753}
 [ChatService] Streaming callback invoked {chunks: Array(1112), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 753}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 753 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated response: 757 chars content:  and
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 757}
 [ChatService] Streaming callback invoked {chunks: Array(1113), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 757}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 757 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " managing"
 [LLM API] tagBuffer:  managing
 [LLM API] Accumulated response: 766 chars content:  managing
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 766}
 [ChatService] Streaming callback invoked {chunks: Array(1114), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 766}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 766 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " AC"
 [LLM API] tagBuffer:  AC
 [LLM API] Accumulated response: 769 chars content:  AC
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 769}
 [ChatService] Streaming callback invoked {chunks: Array(1115), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 769}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 769 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "AD"
 [LLM API] tagBuffer: AD
 [LLM API] Accumulated response: 771 chars content: AD
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 771}
 [ChatService] Streaming callback invoked {chunks: Array(1116), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 771}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 771 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Bad"
 [LLM API] tagBuffer:  Bad
 [LLM API] Accumulated response: 775 chars content:  Bad
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 775}
 [ChatService] Streaming callback invoked {chunks: Array(1117), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 775}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 775 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "ges"
 [LLM API] tagBuffer: ges
 [LLM API] Accumulated response: 778 chars content: ges
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 778}
 [ChatService] Streaming callback invoked {chunks: Array(1118), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 778}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 778 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated response: 780 chars content:  (
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 780}
 [ChatService] Streaming callback invoked {chunks: Array(1119), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 780}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 780 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "the"
 [LLM API] tagBuffer: the
 [LLM API] Accumulated response: 783 chars content: the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 783}
 [ChatService] Streaming callback invoked {chunks: Array(1120), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 783}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 783 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " physical"
 [LLM API] tagBuffer:  physical
 [LLM API] Accumulated response: 792 chars content:  physical
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 792}
 [ChatService] Streaming callback invoked {chunks: Array(1121), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 792}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 792 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " means"
 [LLM API] tagBuffer:  means
 [LLM API] Accumulated response: 798 chars content:  means
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 798}
 [ChatService] Streaming callback invoked {chunks: Array(1122), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 798}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 798 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " for"
 [LLM API] tagBuffer:  for
 [LLM API] Accumulated response: 802 chars content:  for
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 802}
 [ChatService] Streaming callback invoked {chunks: Array(1123), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 802}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 802 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 805 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 805}
 [ChatService] Streaming callback invoked {chunks: Array(1124), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 805}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 805 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 808 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 808}
 [ChatService] Streaming callback invoked {chunks: Array(1125), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 808}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 808 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 813 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 813}
 [ChatService] Streaming callback invoked {chunks: Array(1126), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 813}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 813 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated response: 820 chars content:  access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 820}
 [ChatService] Streaming callback invoked {chunks: Array(1127), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 820}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 820 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ")\n"
 [LLM API] tagBuffer: )

 [LLM API] Accumulated response: 822 chars content: )

 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 822}
 [ChatService] Streaming callback invoked {chunks: Array(1128), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 822}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 822 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated response: 824 chars content:   
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 824}
 [ChatService] Streaming callback invoked {chunks: Array(1129), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 824}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 824 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated response: 826 chars content:  -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 826}
 [ChatService] Streaming callback invoked {chunks: Array(1130), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 826}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 826 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " En"
 [LLM API] tagBuffer:  En
 [LLM API] Accumulated response: 829 chars content:  En
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 829}
 [ChatService] Streaming callback invoked {chunks: Array(1131), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 829}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 829 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "rolling"
 [LLM API] tagBuffer: rolling
 [LLM API] Accumulated response: 836 chars content: rolling
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 836}
 [ChatService] Streaming callback invoked {chunks: Array(1132), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 836}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 836 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " personnel"
 [LLM API] tagBuffer:  personnel
 [LLM API] Accumulated response: 846 chars content:  personnel
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 846}
 [ChatService] Streaming callback invoked {chunks: Array(1133), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 846}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 846 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " into"
 [LLM API] tagBuffer:  into
 [LLM API] Accumulated response: 851 chars content:  into
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 851}
 [ChatService] Streaming callback invoked {chunks: Array(1134), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 851}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 851 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 855 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 855}
 [ChatService] Streaming callback invoked {chunks: Array(1135), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 855}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 855 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Bi"
 [LLM API] tagBuffer:  Bi
 [LLM API] Accumulated response: 858 chars content:  Bi
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 858}
 [ChatService] Streaming callback invoked {chunks: Array(1136), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 858}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 858 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "ometrics"
 [LLM API] tagBuffer: ometrics
 [LLM API] Accumulated response: 866 chars content: ometrics
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 866}
 [ChatService] Streaming callback invoked {chunks: Array(1137), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 866}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 866 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " System"
 [LLM API] tagBuffer:  System
 [LLM API] Accumulated response: 873 chars content:  System
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 873}
 [ChatService] Streaming callback invoked {chunks: Array(1138), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 873}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 873 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "\n"
 [LLM API] tagBuffer: 

 [LLM API] Accumulated response: 874 chars content: 

 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 874}
 [ChatService] Streaming callback invoked {chunks: Array(1139), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 874}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 874 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "  "
 [LLM API] tagBuffer:   
 [LLM API] Accumulated response: 876 chars content:   
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 876}
 [ChatService] Streaming callback invoked {chunks: Array(1140), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 876}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 876 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated response: 878 chars content:  -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 878}
 [ChatService] Streaming callback invoked {chunks: Array(1141), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 878}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 878 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Termin"
 [LLM API] tagBuffer:  Termin
 [LLM API] Accumulated response: 885 chars content:  Termin
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 885}
 [ChatService] Streaming callback invoked {chunks: Array(1142), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 885}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 885 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "ating"
 [LLM API] tagBuffer: ating
 [LLM API] Accumulated response: 890 chars content: ating
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 890}
 [ChatService] Streaming callback invoked {chunks: Array(1143), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 890}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 890 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated response: 894 chars content:  and
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 894}
 [ChatService] Streaming callback invoked {chunks: Array(1144), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 894}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 894 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " re"
 [LLM API] tagBuffer:  re
 [LLM API] Accumulated response: 897 chars content:  re
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 897}
 [ChatService] Streaming callback invoked {chunks: Array(1145), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 897}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 897 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "activ"
 [LLM API] tagBuffer: activ
 [LLM API] Accumulated response: 902 chars content: activ
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 902}
 [ChatService] Streaming callback invoked {chunks: Array(1146), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 902}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 902 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "ating"
 [LLM API] tagBuffer: ating
 [LLM API] Accumulated response: 907 chars content: ating
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 907}
 [ChatService] Streaming callback invoked {chunks: Array(1147), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 907}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 907 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 910 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 910}
 [ChatService] Streaming callback invoked {chunks: Array(1148), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 910}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 910 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 913 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 913}
 [ChatService] Streaming callback invoked {chunks: Array(1149), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 913}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 913 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 918 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 918}
 [ChatService] Streaming callback invoked {chunks: Array(1150), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 918}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 918 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated response: 925 chars content:  access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 925}
 [ChatService] Streaming callback invoked {chunks: Array(1151), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 925}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 925 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " status"
 [LLM API] tagBuffer:  status
 [LLM API] Accumulated response: 932 chars content:  status
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 932}
 [ChatService] Streaming callback invoked {chunks: Array(1152), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 932}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 932 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " ["
 [LLM API] tagBuffer:  [
 [LLM API] Accumulated response: 934 chars content:  [
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 934}
 [ChatService] Streaming callback invoked {chunks: Array(1153), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 934}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 934 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "Source"
 [LLM API] tagBuffer: Source
 [LLM API] Accumulated response: 940 chars content: Source
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 940}
 [ChatService] Streaming callback invoked {chunks: Array(1154), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 940}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 940 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ":"
 [LLM API] tagBuffer: :
 [LLM API] Accumulated response: 941 chars content: :
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 941}
 [ChatService] Streaming callback invoked {chunks: Array(1155), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 941}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 941 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " {"
 [LLM API] tagBuffer:  {
 [LLM API] Accumulated response: 943 chars content:  {
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 943}
 [ChatService] Streaming callback invoked {chunks: Array(1156), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 943}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 943 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "AF"
 [LLM API] tagBuffer: AF
 [LLM API] Accumulated response: 945 chars content: AF
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 945}
 [ChatService] Streaming callback invoked {chunks: Array(1157), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 945}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 945 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "8"
 [LLM API] tagBuffer: 8
 [LLM API] Accumulated response: 946 chars content: 8
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 946}
 [ChatService] Streaming callback invoked {chunks: Array(1158), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 946}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 946 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "14"
 [LLM API] tagBuffer: 14
 [LLM API] Accumulated response: 948 chars content: 14
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 948}
 [ChatService] Streaming callback invoked {chunks: Array(1159), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 948}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 948 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "6"
 [LLM API] tagBuffer: 6
 [LLM API] Accumulated response: 949 chars content: 6
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 949}
 [ChatService] Streaming callback invoked {chunks: Array(1160), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 949}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 949 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "CC"
 [LLM API] tagBuffer: CC
 [LLM API] Accumulated response: 951 chars content: CC
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 951}
 [ChatService] Streaming callback invoked {chunks: Array(1161), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 951}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 951 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "-E"
 [LLM API] tagBuffer: -E
 [LLM API] Accumulated response: 953 chars content: -E
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 953}
 [ChatService] Streaming callback invoked {chunks: Array(1162), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 953}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 953 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "8"
 [LLM API] tagBuffer: 8
 [LLM API] Accumulated response: 954 chars content: 8
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 954}
 [ChatService] Streaming callback invoked {chunks: Array(1163), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 954}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 954 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "D"
 [LLM API] tagBuffer: D
 [LLM API] Accumulated response: 955 chars content: D
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 955}
 [ChatService] Streaming callback invoked {chunks: Array(1164), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 955}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 955 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "5"
 [LLM API] tagBuffer: 5
 [LLM API] Accumulated response: 956 chars content: 5
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 956}
 [ChatService] Streaming callback invoked {chunks: Array(1165), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 956}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 956 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "-C"
 [LLM API] tagBuffer: -C
 [LLM API] Accumulated response: 958 chars content: -C
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 958}
 [ChatService] Streaming callback invoked {chunks: Array(1166), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 958}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 958 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "2"
 [LLM API] tagBuffer: 2
 [LLM API] Accumulated response: 959 chars content: 2
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 959}
 [ChatService] Streaming callback invoked {chunks: Array(1167), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 959}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 959 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "A"
 [LLM API] tagBuffer: A
 [LLM API] Accumulated response: 960 chars content: A
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 960}
 [ChatService] Streaming callback invoked {chunks: Array(1168), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 960}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 960 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "3"
 [LLM API] tagBuffer: 3
 [LLM API] Accumulated response: 961 chars content: 3
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 961}
 [ChatService] Streaming callback invoked {chunks: Array(1169), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 961}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 961 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 962 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 962}
 [ChatService] Streaming callback invoked {chunks: Array(1170), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 962}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 962 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "9"
 [LLM API] tagBuffer: 9
 [LLM API] Accumulated response: 963 chars content: 9
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 963}
 [ChatService] Streaming callback invoked {chunks: Array(1171), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 963}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 963 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "42"
 [LLM API] tagBuffer: 42
 [LLM API] Accumulated response: 965 chars content: 42
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 965}
 [ChatService] Streaming callback invoked {chunks: Array(1172), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 965}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 965 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "1"
 [LLM API] tagBuffer: 1
 [LLM API] Accumulated response: 966 chars content: 1
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 966}
 [ChatService] Streaming callback invoked {chunks: Array(1173), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 966}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 966 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 967 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 967}
 [ChatService] Streaming callback invoked {chunks: Array(1174), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 967}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 967 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "71"
 [LLM API] tagBuffer: 71
 [LLM API] Accumulated response: 969 chars content: 71
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 969}
 [ChatService] Streaming callback invoked {chunks: Array(1175), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 969}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 969 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "F"
 [LLM API] tagBuffer: F
 [LLM API] Accumulated response: 970 chars content: F
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 970}
 [ChatService] Streaming callback invoked {chunks: Array(1176), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 970}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 970 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "171"
 [LLM API] tagBuffer: 171
 [LLM API] Accumulated response: 973 chars content: 171
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 973}
 [ChatService] Streaming callback invoked {chunks: Array(1177), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 973}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 973 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "C"
 [LLM API] tagBuffer: C
 [LLM API] Accumulated response: 974 chars content: C
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 974}
 [ChatService] Streaming callback invoked {chunks: Array(1178), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 974}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 974 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "000"
 [LLM API] tagBuffer: 000
 [LLM API] Accumulated response: 977 chars content: 000
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 977}
 [ChatService] Streaming callback invoked {chunks: Array(1179), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 977}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 977 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "00"
 [LLM API] tagBuffer: 00
 [LLM API] Accumulated response: 979 chars content: 00
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 979}
 [ChatService] Streaming callback invoked {chunks: Array(1180), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 979}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 979 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "}"
 [LLM API] tagBuffer: }
 [LLM API] Accumulated response: 980 chars content: }
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 980}
 [ChatService] Streaming callback invoked {chunks: Array(1181), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 980}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 980 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "]\n\n"
 [LLM API] tagBuffer: ]


 [LLM API] Accumulated response: 983 chars content: ]


 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 983}
 [ChatService] Streaming callback invoked {chunks: Array(1182), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 983}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 983 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "3"
 [LLM API] tagBuffer: 3
 [LLM API] Accumulated response: 984 chars content: 3
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 984}
 [ChatService] Streaming callback invoked {chunks: Array(1183), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 984}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 984 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "."
 [LLM API] tagBuffer: .
 [LLM API] Accumulated response: 985 chars content: .
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 985}
 [ChatService] Streaming callback invoked {chunks: Array(1184), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 985}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 985 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " **"
 [LLM API] tagBuffer:  **
 [LLM API] Accumulated response: 988 chars content:  **
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 988}
 [ChatService] Streaming callback invoked {chunks: Array(1185), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 988}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 988 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "Int"
 [LLM API] tagBuffer: Int
 [LLM API] Accumulated response: 991 chars content: Int
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 991}
 [ChatService] Streaming callback invoked {chunks: Array(1186), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 991}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 991 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "egr"
 [LLM API] tagBuffer: egr
 [LLM API] Accumulated response: 994 chars content: egr
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 994}
 [ChatService] Streaming callback invoked {chunks: Array(1187), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 994}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 994 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "ates"
 [LLM API] tagBuffer: ates
 [LLM API] Accumulated response: 998 chars content: ates
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 998}
 [ChatService] Streaming callback invoked {chunks: Array(1188), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 998}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 998 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " with"
 [LLM API] tagBuffer:  with
 [LLM API] Accumulated response: 1003 chars content:  with
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1003}
 [ChatService] Streaming callback invoked {chunks: Array(1189), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1003}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1003 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " other"
 [LLM API] tagBuffer:  other
 [LLM API] Accumulated response: 1009 chars content:  other
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1009}
 [ChatService] Streaming callback invoked {chunks: Array(1190), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1009}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1009 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " security"
 [LLM API] tagBuffer:  security
 [LLM API] Accumulated response: 1018 chars content:  security
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1018}
 [ChatService] Streaming callback invoked {chunks: Array(1191), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1018}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1018 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " procedures"
 [LLM API] tagBuffer:  procedures
 [LLM API] Accumulated response: 1029 chars content:  procedures
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1029}
 [ChatService] Streaming callback invoked {chunks: Array(1192), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1029}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1029 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "**"
 [LLM API] tagBuffer: **
 [LLM API] Accumulated response: 1031 chars content: **
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1031}
 [ChatService] Streaming callback invoked {chunks: Array(1193), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1031}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1031 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated response: 1033 chars content:  -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1033}
 [ChatService] Streaming callback invoked {chunks: Array(1194), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1033}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1033 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " It"
 [LLM API] tagBuffer:  It
 [LLM API] Accumulated response: 1036 chars content:  It
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1036}
 [ChatService] Streaming callback invoked {chunks: Array(1195), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1036}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1036 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " references"
 [LLM API] tagBuffer:  references
 [LLM API] Accumulated response: 1047 chars content:  references
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1047}
 [ChatService] Streaming callback invoked {chunks: Array(1196), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1047}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1047 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated response: 1051 chars content:  and
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1051}
 [ChatService] Streaming callback invoked {chunks: Array(1197), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1051}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1051 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " coordinates"
 [LLM API] tagBuffer:  coordinates
 [LLM API] Accumulated response: 1063 chars content:  coordinates
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1063}
 [ChatService] Streaming callback invoked {chunks: Array(1198), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1063}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1063 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " with"
 [LLM API] tagBuffer:  with
 [LLM API] Accumulated response: 1068 chars content:  with
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1068}
 [ChatService] Streaming callback invoked {chunks: Array(1199), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1068}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1068 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated response: 1078 chars content:  procedure
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1078}
 [ChatService] Streaming callback invoked {chunks: Array(1200), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1078}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1078 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated response: 1079 chars content:  
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1079}
 [ChatService] Streaming callback invoked {chunks: Array(1201), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1079}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1079 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated response: 1081 chars content: 20
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1081}
 [ChatService] Streaming callback invoked {chunks: Array(1202), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1081}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1081 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated response: 1083 chars content: DP
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1083}
 [ChatService] Streaming callback invoked {chunks: Array(1203), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1083}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1083 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 1084 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1084}
 [ChatService] Streaming callback invoked {chunks: Array(1204), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1084}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1084 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated response: 1085 chars content: 0
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1085}
 [ChatService] Streaming callback invoked {chunks: Array(1205), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1085}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1085 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated response: 1087 chars content: SK
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1087}
 [ChatService] Streaming callback invoked {chunks: Array(1206), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1087}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1087 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "40"
 [LLM API] tagBuffer: 40
 [LLM API] Accumulated response: 1089 chars content: 40
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1089}
 [ChatService] Streaming callback invoked {chunks: Array(1207), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1089}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1089 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated response: 1091 chars content:  (
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1091}
 [ChatService] Streaming callback invoked {chunks: Array(1208), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1091}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1091 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "Access"
 [LLM API] tagBuffer: Access
 [LLM API] Accumulated response: 1097 chars content: Access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1097}
 [ChatService] Streaming callback invoked {chunks: Array(1209), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1097}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1097 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " Authorization"
 [LLM API] tagBuffer:  Authorization
 [LLM API] Accumulated response: 1111 chars content:  Authorization
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1111}
 [ChatService] Streaming callback invoked {chunks: Array(1210), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1111}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1111 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ")"
 [LLM API] tagBuffer: )
 [LLM API] Accumulated response: 1112 chars content: )
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1112}
 [ChatService] Streaming callback invoked {chunks: Array(1211), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1112}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1112 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated response: 1118 chars content:  which
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1118}
 [ChatService] Streaming callback invoked {chunks: Array(1212), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1118}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1118 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " handles"
 [LLM API] tagBuffer:  handles
 [LLM API] Accumulated response: 1126 chars content:  handles
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1126}
 [ChatService] Streaming callback invoked {chunks: Array(1213), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1126}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1126 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 1130 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1130}
 [ChatService] Streaming callback invoked {chunks: Array(1214), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1130}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1130 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " actual"
 [LLM API] tagBuffer:  actual
 [LLM API] Accumulated response: 1137 chars content:  actual
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1137}
 [ChatService] Streaming callback invoked {chunks: Array(1215), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1137}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1137 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " security"
 [LLM API] tagBuffer:  security
 [LLM API] Accumulated response: 1146 chars content:  security
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1146}
 [ChatService] Streaming callback invoked {chunks: Array(1216), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1146}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1146 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " screening"
 [LLM API] tagBuffer:  screening
 [LLM API] Accumulated response: 1156 chars content:  screening
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1156}
 [ChatService] Streaming callback invoked {chunks: Array(1217), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1156}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1156 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated response: 1160 chars content:  and
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1160}
 [ChatService] Streaming callback invoked {chunks: Array(1218), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1160}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1160 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " background"
 [LLM API] tagBuffer:  background
 [LLM API] Accumulated response: 1171 chars content:  background
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1171}
 [ChatService] Streaming callback invoked {chunks: Array(1219), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1171}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1171 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " investigations"
 [LLM API] tagBuffer:  investigations
 [LLM API] Accumulated response: 1186 chars content:  investigations
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1186}
 [ChatService] Streaming callback invoked {chunks: Array(1220), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1186}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1186 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " ["
 [LLM API] tagBuffer:  [
 [LLM API] Accumulated response: 1188 chars content:  [
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1188}
 [ChatService] Streaming callback invoked {chunks: Array(1221), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1188}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1188 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "Source"
 [LLM API] tagBuffer: Source
 [LLM API] Accumulated response: 1194 chars content: Source
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1194}
 [ChatService] Streaming callback invoked {chunks: Array(1222), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1194}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1194 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ":"
 [LLM API] tagBuffer: :
 [LLM API] Accumulated response: 1195 chars content: :
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1195}
 [ChatService] Streaming callback invoked {chunks: Array(1223), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1195}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1195 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " {"
 [LLM API] tagBuffer:  {
 [LLM API] Accumulated response: 1197 chars content:  {
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1197}
 [ChatService] Streaming callback invoked {chunks: Array(1224), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1197}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1197 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "D"
 [LLM API] tagBuffer: D
 [LLM API] Accumulated response: 1198 chars content: D
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1198}
 [ChatService] Streaming callback invoked {chunks: Array(1225), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1198}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1198 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "8"
 [LLM API] tagBuffer: 8
 [LLM API] Accumulated response: 1199 chars content: 8
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1199}
 [ChatService] Streaming callback invoked {chunks: Array(1226), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1199}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1199 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "80"
 [LLM API] tagBuffer: 80
 [LLM API] Accumulated response: 1201 chars content: 80
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1201}
 [ChatService] Streaming callback invoked {chunks: Array(1227), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1201}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1201 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "5"
 [LLM API] tagBuffer: 5
 [LLM API] Accumulated response: 1202 chars content: 5
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1202}
 [ChatService] Streaming callback invoked {chunks: Array(1228), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1202}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1202 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "DB"
 [LLM API] tagBuffer: DB
 [LLM API] Accumulated response: 1204 chars content: DB
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1204}
 [ChatService] Streaming callback invoked {chunks: Array(1229), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1204}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1204 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "E"
 [LLM API] tagBuffer: E
 [LLM API] Accumulated response: 1205 chars content: E
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1205}
 [ChatService] Streaming callback invoked {chunks: Array(1230), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1205}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1205 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 1206 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1206}
 [ChatService] Streaming callback invoked {chunks: Array(1231), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1206}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1206 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "6"
 [LLM API] tagBuffer: 6
 [LLM API] Accumulated response: 1207 chars content: 6
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1207}
 [ChatService] Streaming callback invoked {chunks: Array(1232), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1207}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1207 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "C"
 [LLM API] tagBuffer: C
 [LLM API] Accumulated response: 1208 chars content: C
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1208}
 [ChatService] Streaming callback invoked {chunks: Array(1233), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1208}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1208 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "3"
 [LLM API] tagBuffer: 3
 [LLM API] Accumulated response: 1209 chars content: 3
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1209}
 [ChatService] Streaming callback invoked {chunks: Array(1234), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1209}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1209 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "F"
 [LLM API] tagBuffer: F
 [LLM API] Accumulated response: 1210 chars content: F
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1210}
 [ChatService] Streaming callback invoked {chunks: Array(1235), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1210}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1210 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "-C"
 [LLM API] tagBuffer: -C
 [LLM API] Accumulated response: 1212 chars content: -C
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1212}
 [ChatService] Streaming callback invoked {chunks: Array(1236), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1212}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1212 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "4"
 [LLM API] tagBuffer: 4
 [LLM API] Accumulated response: 1213 chars content: 4
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1213}
 [ChatService] Streaming callback invoked {chunks: Array(1237), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1213}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1213 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "BE"
 [LLM API] tagBuffer: BE
 [LLM API] Accumulated response: 1215 chars content: BE
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1215}
 [ChatService] Streaming callback invoked {chunks: Array(1238), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1215}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1215 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 1216 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1216}
 [ChatService] Streaming callback invoked {chunks: Array(1239), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1216}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1216 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "9"
 [LLM API] tagBuffer: 9
 [LLM API] Accumulated response: 1217 chars content: 9
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1217}
 [ChatService] Streaming callback invoked {chunks: Array(1240), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1217}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1217 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "EF"
 [LLM API] tagBuffer: EF
 [LLM API] Accumulated response: 1219 chars content: EF
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1219}
 [ChatService] Streaming callback invoked {chunks: Array(1241), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1219}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1219 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "2"
 [LLM API] tagBuffer: 2
 [LLM API] Accumulated response: 1220 chars content: 2
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1220}
 [ChatService] Streaming callback invoked {chunks: Array(1242), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1220}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1220 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 1221 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1221}
 [ChatService] Streaming callback invoked {chunks: Array(1243), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1221}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1221 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "7"
 [LLM API] tagBuffer: 7
 [LLM API] Accumulated response: 1222 chars content: 7
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1222}
 [ChatService] Streaming callback invoked {chunks: Array(1244), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1222}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1222 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "B"
 [LLM API] tagBuffer: B
 [LLM API] Accumulated response: 1223 chars content: B
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1223}
 [ChatService] Streaming callback invoked {chunks: Array(1245), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1223}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1223 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "63"
 [LLM API] tagBuffer: 63
 [LLM API] Accumulated response: 1225 chars content: 63
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1225}
 [ChatService] Streaming callback invoked {chunks: Array(1246), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1225}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1225 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "C"
 [LLM API] tagBuffer: C
 [LLM API] Accumulated response: 1226 chars content: C
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1226}
 [ChatService] Streaming callback invoked {chunks: Array(1247), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1226}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1226 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "3"
 [LLM API] tagBuffer: 3
 [LLM API] Accumulated response: 1227 chars content: 3
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1227}
 [ChatService] Streaming callback invoked {chunks: Array(1248), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1227}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1227 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "90"
 [LLM API] tagBuffer: 90
 [LLM API] Accumulated response: 1229 chars content: 90
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1229}
 [ChatService] Streaming callback invoked {chunks: Array(1249), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1229}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1229 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "000"
 [LLM API] tagBuffer: 000
 [LLM API] Accumulated response: 1232 chars content: 000
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1232}
 [ChatService] Streaming callback invoked {chunks: Array(1250), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1232}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1232 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated response: 1233 chars content: 0
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1233}
 [ChatService] Streaming callback invoked {chunks: Array(1251), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1233}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1233 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "}"
 [LLM API] tagBuffer: }
 [LLM API] Accumulated response: 1234 chars content: }
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1234}
 [ChatService] Streaming callback invoked {chunks: Array(1252), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1234}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1234 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "]\n\n"
 [LLM API] tagBuffer: ]


 [LLM API] Accumulated response: 1237 chars content: ]


 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1237}
 [ChatService] Streaming callback invoked {chunks: Array(1253), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1237}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1237 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "The"
 [LLM API] tagBuffer: The
 [LLM API] Accumulated response: 1240 chars content: The
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1240}
 [ChatService] Streaming callback invoked {chunks: Array(1254), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1240}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1240 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " second"
 [LLM API] tagBuffer:  second
 [LLM API] Accumulated response: 1247 chars content:  second
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1247}
 [ChatService] Streaming callback invoked {chunks: Array(1255), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1247}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1247 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated response: 1257 chars content:  procedure
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1257}
 [ChatService] Streaming callback invoked {chunks: Array(1256), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1257}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1257 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " involved"
 [LLM API] tagBuffer:  involved
 [LLM API] Accumulated response: 1266 chars content:  involved
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1266}
 [ChatService] Streaming callback invoked {chunks: Array(1257), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1266}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1266 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated response: 1269 chars content:  is
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1269}
 [ChatService] Streaming callback invoked {chunks: Array(1258), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1269}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1269 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " **"
 [LLM API] tagBuffer:  **
 [LLM API] Accumulated response: 1272 chars content:  **
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1272}
 [ChatService] Streaming callback invoked {chunks: Array(1259), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1272}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1272 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated response: 1274 chars content: 20
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1274}
 [ChatService] Streaming callback invoked {chunks: Array(1260), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1274}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1274 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated response: 1276 chars content: DP
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1276}
 [ChatService] Streaming callback invoked {chunks: Array(1261), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1276}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1276 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 1277 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1277}
 [ChatService] Streaming callback invoked {chunks: Array(1262), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1277}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1277 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated response: 1278 chars content: 0
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1278}
 [ChatService] Streaming callback invoked {chunks: Array(1263), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1278}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1278 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated response: 1280 chars content: SK
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1280}
 [ChatService] Streaming callback invoked {chunks: Array(1264), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1280}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1280 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "40"
 [LLM API] tagBuffer: 40
 [LLM API] Accumulated response: 1282 chars content: 40
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1282}
 [ChatService] Streaming callback invoked {chunks: Array(1265), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1282}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1282 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated response: 1284 chars content:  (
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1284}
 [ChatService] Streaming callback invoked {chunks: Array(1266), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1284}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1284 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "PV"
 [LLM API] tagBuffer: PV
 [LLM API] Accumulated response: 1286 chars content: PV
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1286}
 [ChatService] Streaming callback invoked {chunks: Array(1267), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1286}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1286 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "NG"
 [LLM API] tagBuffer: NG
 [LLM API] Accumulated response: 1288 chars content: NG
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1288}
 [ChatService] Streaming callback invoked {chunks: Array(1268), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1288}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1288 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "S"
 [LLM API] tagBuffer: S
 [LLM API] Accumulated response: 1289 chars content: S
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1289}
 [ChatService] Streaming callback invoked {chunks: Array(1269), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1289}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1289 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " Un"
 [LLM API] tagBuffer:  Un
 [LLM API] Accumulated response: 1292 chars content:  Un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1292}
 [ChatService] Streaming callback invoked {chunks: Array(1270), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1292}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1292 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 1295 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1295}
 [ChatService] Streaming callback invoked {chunks: Array(1271), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1295}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1295 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 1300 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1300}
 [ChatService] Streaming callback invoked {chunks: Array(1272), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1300}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1300 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Access"
 [LLM API] tagBuffer:  Access
 [LLM API] Accumulated response: 1307 chars content:  Access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1307}
 [ChatService] Streaming callback invoked {chunks: Array(1273), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1307}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1307 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " Screening"
 [LLM API] tagBuffer:  Screening
 [LLM API] Accumulated response: 1317 chars content:  Screening
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1317}
 [ChatService] Streaming callback invoked {chunks: Array(1274), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1317}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1317 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ")**"
 [LLM API] tagBuffer: )**
 [LLM API] Accumulated response: 1320 chars content: )**
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1320}
 [ChatService] Streaming callback invoked {chunks: Array(1275), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1320}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1320 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " -"
 [LLM API] tagBuffer:  -
 [LLM API] Accumulated response: 1322 chars content:  -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1322}
 [ChatService] Streaming callback invoked {chunks: Array(1276), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1322}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1322 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " Rev"
 [LLM API] tagBuffer:  Rev
 [LLM API] Accumulated response: 1326 chars content:  Rev
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1326}
 [ChatService] Streaming callback invoked {chunks: Array(1277), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1326}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1326 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " "
 [LLM API] tagBuffer:  
 [LLM API] Accumulated response: 1327 chars content:  
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1327}
 [ChatService] Streaming callback invoked {chunks: Array(1278), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1327}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1327 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated response: 1328 chars content: 0
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1328}
 [ChatService] Streaming callback invoked {chunks: Array(1279), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1328}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1328 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "50"
 [LLM API] tagBuffer: 50
 [LLM API] Accumulated response: 1330 chars content: 50
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1330}
 [ChatService] Streaming callback invoked {chunks: Array(1280), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1330}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1330 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated response: 1331 chars content: ,
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1331}
 [ChatService] Streaming callback invoked {chunks: Array(1281), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1331}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1331 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated response: 1337 chars content:  which
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1337}
 [ChatService] Streaming callback invoked {chunks: Array(1282), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1337}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1337 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " focuses"
 [LLM API] tagBuffer:  focuses
 [LLM API] Accumulated response: 1345 chars content:  focuses
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1345}
 [ChatService] Streaming callback invoked {chunks: Array(1283), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1345}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1345 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " specifically"
 [LLM API] tagBuffer:  specifically
 [LLM API] Accumulated response: 1358 chars content:  specifically
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1358}
 [ChatService] Streaming callback invoked {chunks: Array(1284), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1358}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1358 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " on"
 [LLM API] tagBuffer:  on
 [LLM API] Accumulated response: 1361 chars content:  on
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1361}
 [ChatService] Streaming callback invoked {chunks: Array(1285), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1361}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1361 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 1365 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1365}
 [ChatService] Streaming callback invoked {chunks: Array(1286), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1365}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1365 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " security"
 [LLM API] tagBuffer:  security
 [LLM API] Accumulated response: 1374 chars content:  security
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1374}
 [ChatService] Streaming callback invoked {chunks: Array(1287), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1374}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1374 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " screening"
 [LLM API] tagBuffer:  screening
 [LLM API] Accumulated response: 1384 chars content:  screening
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1384}
 [ChatService] Streaming callback invoked {chunks: Array(1288), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1384}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1384 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " requirements"
 [LLM API] tagBuffer:  requirements
 [LLM API] Accumulated response: 1397 chars content:  requirements
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1397}
 [ChatService] Streaming callback invoked {chunks: Array(1289), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1397}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1397 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated response: 1398 chars content: ,
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1398}
 [ChatService] Streaming callback invoked {chunks: Array(1290), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1398}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1398 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " background"
 [LLM API] tagBuffer:  background
 [LLM API] Accumulated response: 1409 chars content:  background
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1409}
 [ChatService] Streaming callback invoked {chunks: Array(1291), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1409}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1409 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " investigations"
 [LLM API] tagBuffer:  investigations
 [LLM API] Accumulated response: 1424 chars content:  investigations
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1424}
 [ChatService] Streaming callback invoked {chunks: Array(1292), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1424}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1424 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated response: 1425 chars content: ,
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1425}
 [ChatService] Streaming callback invoked {chunks: Array(1293), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1425}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1425 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated response: 1429 chars content:  and
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1429}
 [ChatService] Streaming callback invoked {chunks: Array(1294), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1429}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1429 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " fitness"
 [LLM API] tagBuffer:  fitness
 [LLM API] Accumulated response: 1437 chars content:  fitness
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1437}
 [ChatService] Streaming callback invoked {chunks: Array(1295), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1437}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1437 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "-for"
 [LLM API] tagBuffer: -for
 [LLM API] Accumulated response: 1441 chars content: -for
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1441}
 [ChatService] Streaming callback invoked {chunks: Array(1296), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1441}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1441 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "-duty"
 [LLM API] tagBuffer: -duty
 [LLM API] Accumulated response: 1446 chars content: -duty
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1446}
 [ChatService] Streaming callback invoked {chunks: Array(1297), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1446}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1446 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " assessments"
 [LLM API] tagBuffer:  assessments
 [LLM API] Accumulated response: 1458 chars content:  assessments
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1458}
 [ChatService] Streaming callback invoked {chunks: Array(1298), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1458}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1458 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " that"
 [LLM API] tagBuffer:  that
 [LLM API] Accumulated response: 1463 chars content:  that
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1463}
 [ChatService] Streaming callback invoked {chunks: Array(1299), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1463}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1463 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " must"
 [LLM API] tagBuffer:  must
 [LLM API] Accumulated response: 1468 chars content:  must
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1468}
 [ChatService] Streaming callback invoked {chunks: Array(1300), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1468}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1468 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated response: 1471 chars content:  be
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1471}
 [ChatService] Streaming callback invoked {chunks: Array(1301), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1471}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1471 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " completed"
 [LLM API] tagBuffer:  completed
 [LLM API] Accumulated response: 1481 chars content:  completed
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1481}
 [ChatService] Streaming callback invoked {chunks: Array(1302), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1481}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1481 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " before"
 [LLM API] tagBuffer:  before
 [LLM API] Accumulated response: 1488 chars content:  before
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1488}
 [ChatService] Streaming callback invoked {chunks: Array(1303), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1488}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1488 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 1491 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1491}
 [ChatService] Streaming callback invoked {chunks: Array(1304), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1491}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1491 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 1494 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1494}
 [ChatService] Streaming callback invoked {chunks: Array(1305), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1494}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1494 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 1499 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1499}
 [ChatService] Streaming callback invoked {chunks: Array(1306), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1499}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1499 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated response: 1506 chars content:  access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1506}
 [ChatService] Streaming callback invoked {chunks: Array(1307), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1506}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1506 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " can"
 [LLM API] tagBuffer:  can
 [LLM API] Accumulated response: 1510 chars content:  can
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1510}
 [ChatService] Streaming callback invoked {chunks: Array(1308), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1510}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1510 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " be"
 [LLM API] tagBuffer:  be
 [LLM API] Accumulated response: 1513 chars content:  be
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1513}
 [ChatService] Streaming callback invoked {chunks: Array(1309), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1513}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1513 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " granted"
 [LLM API] tagBuffer:  granted
 [LLM API] Accumulated response: 1521 chars content:  granted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1521}
 [ChatService] Streaming callback invoked {chunks: Array(1310), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1521}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1521 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ".\n\n"
 [LLM API] tagBuffer: .


 [LLM API] Accumulated response: 1524 chars content: .


 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1524}
 [ChatService] Streaming callback invoked {chunks: Array(1311), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1524}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1524 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "However"
 [LLM API] tagBuffer: However
 [LLM API] Accumulated response: 1531 chars content: However
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1531}
 [ChatService] Streaming callback invoked {chunks: Array(1312), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1531}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1531 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated response: 1532 chars content: ,
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1532}
 [ChatService] Streaming callback invoked {chunks: Array(1313), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1532}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1532 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " when"
 [LLM API] tagBuffer:  when
 [LLM API] Accumulated response: 1537 chars content:  when
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1537}
 [ChatService] Streaming callback invoked {chunks: Array(1314), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1537}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1537 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " asking"
 [LLM API] tagBuffer:  asking
 [LLM API] Accumulated response: 1544 chars content:  asking
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1544}
 [ChatService] Streaming callback invoked {chunks: Array(1315), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1544}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1544 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " which"
 [LLM API] tagBuffer:  which
 [LLM API] Accumulated response: 1550 chars content:  which
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1550}
 [ChatService] Streaming callback invoked {chunks: Array(1316), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1550}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1550 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " procedure"
 [LLM API] tagBuffer:  procedure
 [LLM API] Accumulated response: 1560 chars content:  procedure
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1560}
 [ChatService] Streaming callback invoked {chunks: Array(1317), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1560}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1560 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " \""
 [LLM API] tagBuffer:  "
 [LLM API] Accumulated response: 1562 chars content:  "
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1562}
 [ChatService] Streaming callback invoked {chunks: Array(1318), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1562}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1562 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "man"
 [LLM API] tagBuffer: man
 [LLM API] Accumulated response: 1565 chars content: man
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1565}
 [ChatService] Streaming callback invoked {chunks: Array(1319), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1565}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1565 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "ages"
 [LLM API] tagBuffer: ages
 [LLM API] Accumulated response: 1569 chars content: ages
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1569}
 [ChatService] Streaming callback invoked {chunks: Array(1320), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1569}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1569 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "\""
 [LLM API] tagBuffer: "
 [LLM API] Accumulated response: 1570 chars content: "
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1570}
 [ChatService] Streaming callback invoked {chunks: Array(1321), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1570}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1570 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 1573 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1573}
 [ChatService] Streaming callback invoked {chunks: Array(1322), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1573}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1573 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 1576 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1576}
 [ChatService] Streaming callback invoked {chunks: Array(1323), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1576}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1576 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
 [LLM API] Accumulated response: 1581 chars content: orted
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1581}
 [ChatService] Streaming callback invoked {chunks: Array(1324), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1581}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1581 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " access"
 [LLM API] tagBuffer:  access
 [LLM API] Accumulated response: 1588 chars content:  access
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1588}
 [ChatService] Streaming callback invoked {chunks: Array(1325), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1588}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1588 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: ","
 [LLM API] tagBuffer: ,
 [LLM API] Accumulated response: 1589 chars content: ,
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1589}
 [ChatService] Streaming callback invoked {chunks: Array(1326), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1589}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1589 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 1593 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1593}
 [ChatService] Streaming callback invoked {chunks: Array(1327), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1593}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1593 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " answer"
 [LLM API] tagBuffer:  answer
 [LLM API] Accumulated response: 1600 chars content:  answer
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1600}
 [ChatService] Streaming callback invoked {chunks: Array(1328), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1600}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1600 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " is"
 [LLM API] tagBuffer:  is
 [LLM API] Accumulated response: 1603 chars content:  is
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1603}
 [ChatService] Streaming callback invoked {chunks: Array(1329), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1603}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1603 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 1607 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1607}
 [ChatService] Streaming callback invoked {chunks: Array(1330), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1607}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1607 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " **"
 [LLM API] tagBuffer:  **
 [LLM API] Accumulated response: 1610 chars content:  **
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1610}
 [ChatService] Streaming callback invoked {chunks: Array(1331), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1610}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1610 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "P"
 [LLM API] tagBuffer: P
 [LLM API] Accumulated response: 1611 chars content: P
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1611}
 [ChatService] Streaming callback invoked {chunks: Array(1332), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1611}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1611 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "alo"
 [LLM API] tagBuffer: alo
 [LLM API] Accumulated response: 1614 chars content: alo
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1614}
 [ChatService] Streaming callback invoked {chunks: Array(1333), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1614}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1614 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " Verde"
 [LLM API] tagBuffer:  Verde
 [LLM API] Accumulated response: 1620 chars content:  Verde
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1620}
 [ChatService] Streaming callback invoked {chunks: Array(1334), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1620}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1620 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Bad"
 [LLM API] tagBuffer:  Bad
 [LLM API] Accumulated response: 1624 chars content:  Bad
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1624}
 [ChatService] Streaming callback invoked {chunks: Array(1335), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1624}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1624 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "ging"
 [LLM API] tagBuffer: ging
 [LLM API] Accumulated response: 1628 chars content: ging
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1628}
 [ChatService] Streaming callback invoked {chunks: Array(1336), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1628}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1628 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " Procedure"
 [LLM API] tagBuffer:  Procedure
 [LLM API] Accumulated response: 1638 chars content:  Procedure
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1638}
 [ChatService] Streaming callback invoked {chunks: Array(1337), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1638}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1638 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " ("
 [LLM API] tagBuffer:  (
 [LLM API] Accumulated response: 1640 chars content:  (
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1640}
 [ChatService] Streaming callback invoked {chunks: Array(1338), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1640}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1640 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "20"
 [LLM API] tagBuffer: 20
 [LLM API] Accumulated response: 1642 chars content: 20
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1642}
 [ChatService] Streaming callback invoked {chunks: Array(1339), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1642}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1642 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "DP"
 [LLM API] tagBuffer: DP
 [LLM API] Accumulated response: 1644 chars content: DP
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1644}
 [ChatService] Streaming callback invoked {chunks: Array(1340), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1644}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1644 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "-"
 [LLM API] tagBuffer: -
 [LLM API] Accumulated response: 1645 chars content: -
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1645}
 [ChatService] Streaming callback invoked {chunks: Array(1341), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1645}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1645 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "0"
 [LLM API] tagBuffer: 0
 [LLM API] Accumulated response: 1646 chars content: 0
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1646}
 [ChatService] Streaming callback invoked {chunks: Array(1342), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1646}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1646 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "SK"
 [LLM API] tagBuffer: SK
 [LLM API] Accumulated response: 1648 chars content: SK
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1648}
 [ChatService] Streaming callback invoked {chunks: Array(1343), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1648}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1648 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "39"
 [LLM API] tagBuffer: 39
 [LLM API] Accumulated response: 1650 chars content: 39
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1650}
 [ChatService] Streaming callback invoked {chunks: Array(1344), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1650}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1650 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: ")**"
 [LLM API] tagBuffer: )**
 [LLM API] Accumulated response: 1653 chars content: )**
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1653}
 [ChatService] Streaming callback invoked {chunks: Array(1345), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1653}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1653 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " as"
 [LLM API] tagBuffer:  as
 [LLM API] Accumulated response: 1656 chars content:  as
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1656}
 [ChatService] Streaming callback invoked {chunks: Array(1346), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1656}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1656 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " it"
 [LLM API] tagBuffer:  it
 [LLM API] Accumulated response: 1659 chars content:  it
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1659}
 [ChatService] Streaming callback invoked {chunks: Array(1347), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1659}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1659 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " oversees"
 [LLM API] tagBuffer:  oversees
 [LLM API] Accumulated response: 1668 chars content:  oversees
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1668}
 [ChatService] Streaming callback invoked {chunks: Array(1348), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1668}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1668 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " the"
 [LLM API] tagBuffer:  the
 [LLM API] Accumulated response: 1672 chars content:  the
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1672}
 [ChatService] Streaming callback invoked {chunks: Array(1349), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1672}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1672 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " entire"
 [LLM API] tagBuffer:  entire
 [LLM API] Accumulated response: 1679 chars content:  entire
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1679}
 [ChatService] Streaming callback invoked {chunks: Array(1350), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1679}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1679 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " administrative"
 [LLM API] tagBuffer:  administrative
 [LLM API] Accumulated response: 1694 chars content:  administrative
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1694}
 [ChatService] Streaming callback invoked {chunks: Array(1351), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1694}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1694 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " process"
 [LLM API] tagBuffer:  process
 [LLM API] Accumulated response: 1702 chars content:  process
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1702}
 [ChatService] Streaming callback invoked {chunks: Array(1352), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1702}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1702 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " and"
 [LLM API] tagBuffer:  and
 [LLM API] Accumulated response: 1706 chars content:  and
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1706}
 [ChatService] Streaming callback invoked {chunks: Array(1353), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1706}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1706 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " lifecycle"
 [LLM API] tagBuffer:  lifecycle
 [LLM API] Accumulated response: 1716 chars content:  lifecycle
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1716}
 [ChatService] Streaming callback invoked {chunks: Array(1354), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1716}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1716 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " management"
 [LLM API] tagBuffer:  management
 [LLM API] Accumulated response: 1727 chars content:  management
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1727}
 [ChatService] Streaming callback invoked {chunks: Array(1355), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1727}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1727 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: " of"
 [LLM API] tagBuffer:  of
 [LLM API] Accumulated response: 1730 chars content:  of
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1730}
 [ChatService] Streaming callback invoked {chunks: Array(1356), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1730}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1730 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: " un"
 [LLM API] tagBuffer:  un
 [LLM API] Accumulated response: 1733 chars content:  un
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1733}
 [ChatService] Streaming callback invoked {chunks: Array(1357), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1733}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1733 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [LLM API] Processing line: data: "esc"
 [LLM API] tagBuffer: esc
 [LLM API] Accumulated response: 1736 chars content: esc
 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1736}
 [ChatService] Streaming callback invoked {chunks: Array(1358), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1736}
 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1736 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
 [MarkdownContent] RAG docs: 0
 [MarkdownContent] Citation metadata: undefined
 [MarkdownContent] Citation metadata keys: 0
 [MarkdownContent] Found 0 citation links to attach handlers to
 [LLM API] Processing line: data: "orted"
 [LLM API] tagBuffer: orted
llm-api.service.ts:280 [LLM API] Accumulated response: 1741 chars content: orted
llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1741}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1359), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1741}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1741 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
llm-api.service.ts:157 [LLM API] Processing line: data: " access"
llm-api.service.ts:174 [LLM API] tagBuffer:  access
llm-api.service.ts:280 [LLM API] Accumulated response: 1748 chars content:  access
llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1748}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1360), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1748}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1748 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
llm-api.service.ts:157 [LLM API] Processing line: data: " personnel"
llm-api.service.ts:174 [LLM API] tagBuffer:  personnel
llm-api.service.ts:280 [LLM API] Accumulated response: 1758 chars content:  personnel
llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1758}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1361), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1758}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1758 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:39 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:40 [MarkdownContent] RAG docs: 0
markdown-content.component.ts:41 [MarkdownContent] Citation metadata: undefined
markdown-content.component.ts:42 [MarkdownContent] Citation metadata keys: 0
markdown-content.component.ts:81 [MarkdownContent] Found 0 citation links to attach handlers to
llm-api.service.ts:157 [LLM API] Processing line: data: " at"
llm-api.service.ts:174 [LLM API] tagBuffer:  at
llm-api.service.ts:280 [LLM API] Accumulated response: 1761 chars content:  at
llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1761}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1362), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1761}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1761 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
llm-api.service.ts:157 [LLM API] Processing line: data: " Palo"
llm-api.service.ts:174 [LLM API] tagBuffer:  Palo
llm-api.service.ts:280 [LLM API] Accumulated response: 1766 chars content:  Palo
llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1766}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1363), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1766}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1766 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:39 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:40 [MarkdownContent] RAG docs: 0
markdown-content.component.ts:41 [MarkdownContent] Citation metadata: undefined
markdown-content.component.ts:42 [MarkdownContent] Citation metadata keys: 0
markdown-content.component.ts:81 [MarkdownContent] Found 0 citation links to attach handlers to
llm-api.service.ts:157 [LLM API] Processing line: data: " Verde"
llm-api.service.ts:174 [LLM API] tagBuffer:  Verde
llm-api.service.ts:280 [LLM API] Accumulated response: 1772 chars content:  Verde
llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1772}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1364), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1772}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1772 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
llm-api.service.ts:157 [LLM API] Processing line: data: ".\n"
llm-api.service.ts:174 [LLM API] tagBuffer: .

llm-api.service.ts:280 [LLM API] Accumulated response: 1774 chars content: .

llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1774}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1365), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1774}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1774 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:39 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:40 [MarkdownContent] RAG docs: 0
markdown-content.component.ts:41 [MarkdownContent] Citation metadata: undefined
markdown-content.component.ts:42 [MarkdownContent] Citation metadata keys: 0
markdown-content.component.ts:81 [MarkdownContent] Found 0 citation links to attach handlers to
llm-api.service.ts:157 [LLM API] Processing line: data: "</"
llm-api.service.ts:174 [LLM API] tagBuffer: </
llm-api.service.ts:280 [LLM API] Accumulated response: 1776 chars content: </
llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1776}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1366), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1776}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1776 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
llm-api.service.ts:157 [LLM API] Processing line: data: "response"
llm-api.service.ts:174 [LLM API] tagBuffer: response
llm-api.service.ts:280 [LLM API] Accumulated response: 1784 chars content: response
llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1784}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1367), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1784}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1784 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:39 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:40 [MarkdownContent] RAG docs: 0
markdown-content.component.ts:41 [MarkdownContent] Citation metadata: undefined
markdown-content.component.ts:42 [MarkdownContent] Citation metadata keys: 0
markdown-content.component.ts:81 [MarkdownContent] Found 0 citation links to attach handlers to
llm-api.service.ts:157 [LLM API] Processing line: data: ">"
llm-api.service.ts:174 [LLM API] tagBuffer: >
llm-api.service.ts:280 [LLM API] Accumulated response: 1785 chars content: >
llm-api.service.ts:303 [LLM API] Calling onChunk with: {thinkingLength: 3889, toolingLength: 0, responseLength: 1785}
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1368), currentChunk: {…}, isComplete: false, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1785}
chat.service.ts:324 [ChatService] updateMessageConent called, messageId: rex5dkm60 content length: 1785 first 100 chars: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:39 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:40 [MarkdownContent] RAG docs: 0
markdown-content.component.ts:41 [MarkdownContent] Citation metadata: undefined
markdown-content.component.ts:42 [MarkdownContent] Citation metadata keys: 0
markdown-content.component.ts:81 [MarkdownContent] Found 0 citation links to attach handlers to
llm-api.service.ts:157 [LLM API] Processing line: followup_and_topic_questions: "{\n    \"topic\": \
llm-api.service.ts:157 [LLM API] Processing line: metadata: {"{C387180F-5A45-4B5A-96C3-C26F0D11D6CB"
llm-api.service.ts:391 [LLM API] Received metadata with 22 documents
llm-api.service.ts:392 [LLM API] First 3 metadata keys: (3) ['{C387180F-5A45-4B5A-96C3-C26F0D11D6CB', '{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1', '{FA205FD1-7739-C377-84CE-7B63C3A00000']
chat.service.ts:98 [ChatService] Streaming callback invoked {chunks: Array(1369), currentChunk: {…}, isComplete: true, error: undefined, messageId: undefined}
chat.service.ts:118 [ChatService] Processing chunk: {thinkingLength: 3889, toolingLength: 0, responseLength: 1785}
chat.service.ts:142 [ChatService] Updating citation metadata with 22 keys
chat.service.ts:296 [ChatService] updateMessageCitationMetadata called for message rex5dkm60 with metadata keys: (22) ['{C387180F-5A45-4B5A-96C3-C26F0D11D6CB', '{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1', '{FA205FD1-7739-C377-84CE-7B63C3A00000', '{293EE29E-A38F-4044-BB23-2F20D35E2300', '{5745EE79-863B-4777-A1C1-3ECF6E4FCFF2', '{9F69364A-E6E6-4D01-A1AC-A7E9A1793E10', '{11AF8B8B-7F17-4101-9649-1094CE3909AA', '{C2DE6CFD-E6A0-4825-8F02-381BA55349B5', '{CBCD67CC-60E7-46EE-811F-D8EB5D07E72A', '{1402769A-E6A7-4DD9-9517-99C1340A645F', '{73FF30FA-FE19-C5C0-9F01-71F171C00000', '{393DDF27-812F-C387-9E24-828469F00000', '{4999779D-E7E3-4E66-A029-046AD2CED1B2', '{AF8146CC-E8D5-C2A3-9421-71F171C00000', '{EBA10DB5-52FE-47FE-87E6-B2ECA9F88F1D', '{D8805DBE-6C3F-C4BE-9EF2-7B63C3900000', '{4F007B96-2C13-44F6-BB0A-9569373B241D', '{93ADF86A-F974-4A6D-9C2C-B37C41626643', '{B73D5D0A-2118-C4E3-9CF9-784A42700000', '{2519F4E1-AEEF-4334-8219-B09E45982519', '{74CF9E8E-952F-4164-8764-F79B61ADEBC5', '{15FBA82B-35AB-47A8-AB1F-2071C02675B6']
chat.service.ts:307 [ChatService] Citation metadata updated in conversations
markdown-content.component.ts:39 [MarkdownContent] Content changed: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing une
markdown-content.component.ts:40 [MarkdownContent] RAG docs: 0
markdown-content.component.ts:41 [MarkdownContent] Citation metadata: {{C387180F-5A45-4B5A-96C3-C26F0D11D6CB: {…}, {CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1: {…}, {FA205FD1-7739-C377-84CE-7B63C3A00000: {…}, {293EE29E-A38F-4044-BB23-2F20D35E2300: {…}, {5745EE79-863B-4777-A1C1-3ECF6E4FCFF2: {…}, …}
markdown-content.component.ts:42 [MarkdownContent] Citation metadata keys: 22
source-citation.service.ts:147 [SourceCitationService] Processing content: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing unescorted access, but the primary procedure that manages the overall unescorted access process is:

**
source-citation.service.ts:148 [SourceCitationService] RAG documents count: 0
source-citation.service.ts:150 [SourceCitationService] Citation metadata: present with 22 keys: (3) ['{C387180F-5A45-4B5A-96C3-C26F0D11D6CB', '{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1', '{FA205FD1-7739-C377-84CE-7B63C3A00000']
source-citation.service.ts:174 [SourceCitationService] Found citation: [Source: {CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1}] identifiers: {CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1}
source-citation.service.ts:178 [SourceCitationService] hasUUIDs: true citationMetadata exists: true
source-citation.service.ts:195 [SourceCitationService] Looking up UUID: {CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1}
source-citation.service.ts:196 [SourceCitationService] Available keys: (5) ['{C387180F-5A45-4B5A-96C3-C26F0D11D6CB', '{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1', '{FA205FD1-7739-C377-84CE-7B63C3A00000', '{293EE29E-A38F-4044-BB23-2F20D35E2300', '{5745EE79-863B-4777-A1C1-3ECF6E4FCFF2']
source-citation.service.ts:197 [SourceCitationService] All available keys: (22) ['{C387180F-5A45-4B5A-96C3-C26F0D11D6CB', '{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1', '{FA205FD1-7739-C377-84CE-7B63C3A00000', '{293EE29E-A38F-4044-BB23-2F20D35E2300', '{5745EE79-863B-4777-A1C1-3ECF6E4FCFF2', '{9F69364A-E6E6-4D01-A1AC-A7E9A1793E10', '{11AF8B8B-7F17-4101-9649-1094CE3909AA', '{C2DE6CFD-E6A0-4825-8F02-381BA55349B5', '{CBCD67CC-60E7-46EE-811F-D8EB5D07E72A', '{1402769A-E6A7-4DD9-9517-99C1340A645F', '{73FF30FA-FE19-C5C0-9F01-71F171C00000', '{393DDF27-812F-C387-9E24-828469F00000', '{4999779D-E7E3-4E66-A029-046AD2CED1B2', '{AF8146CC-E8D5-C2A3-9421-71F171C00000', '{EBA10DB5-52FE-47FE-87E6-B2ECA9F88F1D', '{D8805DBE-6C3F-C4BE-9EF2-7B63C3900000', '{4F007B96-2C13-44F6-BB0A-9569373B241D', '{93ADF86A-F974-4A6D-9C2C-B37C41626643', '{B73D5D0A-2118-C4E3-9CF9-784A42700000', '{2519F4E1-AEEF-4334-8219-B09E45982519', '{74CF9E8E-952F-4164-8764-F79B61ADEBC5', '{15FBA82B-35AB-47A8-AB1F-2071C02675B6']
source-citation.service.ts:198 [SourceCitationService] citationMetadata type: object
source-citation.service.ts:199 [SourceCitationService] Full metadata object: {{C387180F-5A45-4B5A-96C3-C26F0D11D6CB: {…}, {CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1: {…}, {FA205FD1-7739-C377-84CE-7B63C3A00000: {…}, {293EE29E-A38F-4044-BB23-2F20D35E2300: {…}, {5745EE79-863B-4777-A1C1-3ECF6E4FCFF2: {…}, …}
source-citation.service.ts:205 [SourceCitationService] Direct lookup citationMetadata["{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1}"] = undefined
source-citation.service.ts:210 [SourceCitationService] Trying without braces: CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1
source-citation.service.ts:213 [SourceCitationService] Without braces citationMetadata["CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1"] = undefined
source-citation.service.ts:248  [SourceCitationService] No metadata found for UUID: {CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1}
(anonymous) @ source-citation.service.ts:248
replaceSourceCitationsWithHTML @ source-citation.service.ts:173
(anonymous) @ markdown-content.component.ts:48
run @ resource-DalzMB4W.mjs:205
runEffectsInView @ debug_node-DTOmNMDH.mjs:8756
refreshView @ debug_node-DTOmNMDH.mjs:8916
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInEmbeddedViews @ debug_node-DTOmNMDH.mjs:9025
refreshView @ debug_node-DTOmNMDH.mjs:8917
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
refreshView @ debug_node-DTOmNMDH.mjs:8943
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInEmbeddedViews @ debug_node-DTOmNMDH.mjs:9025
refreshView @ debug_node-DTOmNMDH.mjs:8917
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
detectChangesInView @ debug_node-DTOmNMDH.mjs:9120
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
detectChangesInView @ debug_node-DTOmNMDH.mjs:9120
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInEmbeddedViews @ debug_node-DTOmNMDH.mjs:9025
refreshView @ debug_node-DTOmNMDH.mjs:8917
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
refreshView @ debug_node-DTOmNMDH.mjs:8943
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewWhileDirty @ debug_node-DTOmNMDH.mjs:8797
detectChangesInternal @ debug_node-DTOmNMDH.mjs:8785
synchronizeOnce @ debug_node-DTOmNMDH.mjs:20117
synchronize @ debug_node-DTOmNMDH.mjs:20076
tickImpl @ debug_node-DTOmNMDH.mjs:20049
_tick @ debug_node-DTOmNMDH.mjs:20038
(anonymous) @ debug_node-DTOmNMDH.mjs:29476
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
run @ debug_node-DTOmNMDH.mjs:16342
next @ debug_node-DTOmNMDH.mjs:29473
ConsumerObserver2.next @ Subscriber.js:96
Subscriber2._next @ Subscriber.js:63
Subscriber2.next @ Subscriber.js:34
(anonymous) @ Subject.js:41
errorContext @ errorContext.js:19
Subject2.next @ Subject.js:31
emit @ debug_node-DTOmNMDH.mjs:16030
checkStable @ debug_node-DTOmNMDH.mjs:16410
onHasTask @ debug_node-DTOmNMDH.mjs:16524
hasTask @ zone.js:451
_updateTaskCount @ zone.js:471
_updateTaskCount @ zone.js:266
runTask @ zone.js:179
drainMicroTaskQueue @ zone.js:612
Promise.then
nativeScheduleMicroTask @ zone.js:588
scheduleMicroTask @ zone.js:599
scheduleTask @ zone.js:420
onScheduleTask @ debug_node-DTOmNMDH.mjs:16155
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
source-citation.service.ts:174 [SourceCitationService] Found citation: [Source: {AF8146CC-E8D5-C2A3-9421-71F171C00000}] identifiers: {AF8146CC-E8D5-C2A3-9421-71F171C00000}
source-citation.service.ts:178 [SourceCitationService] hasUUIDs: true citationMetadata exists: true
source-citation.service.ts:195 [SourceCitationService] Looking up UUID: {AF8146CC-E8D5-C2A3-9421-71F171C00000}
source-citation.service.ts:196 [SourceCitationService] Available keys: (5) ['{C387180F-5A45-4B5A-96C3-C26F0D11D6CB', '{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1', '{FA205FD1-7739-C377-84CE-7B63C3A00000', '{293EE29E-A38F-4044-BB23-2F20D35E2300', '{5745EE79-863B-4777-A1C1-3ECF6E4FCFF2']
source-citation.service.ts:197 [SourceCitationService] All available keys: (22) ['{C387180F-5A45-4B5A-96C3-C26F0D11D6CB', '{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1', '{FA205FD1-7739-C377-84CE-7B63C3A00000', '{293EE29E-A38F-4044-BB23-2F20D35E2300', '{5745EE79-863B-4777-A1C1-3ECF6E4FCFF2', '{9F69364A-E6E6-4D01-A1AC-A7E9A1793E10', '{11AF8B8B-7F17-4101-9649-1094CE3909AA', '{C2DE6CFD-E6A0-4825-8F02-381BA55349B5', '{CBCD67CC-60E7-46EE-811F-D8EB5D07E72A', '{1402769A-E6A7-4DD9-9517-99C1340A645F', '{73FF30FA-FE19-C5C0-9F01-71F171C00000', '{393DDF27-812F-C387-9E24-828469F00000', '{4999779D-E7E3-4E66-A029-046AD2CED1B2', '{AF8146CC-E8D5-C2A3-9421-71F171C00000', '{EBA10DB5-52FE-47FE-87E6-B2ECA9F88F1D', '{D8805DBE-6C3F-C4BE-9EF2-7B63C3900000', '{4F007B96-2C13-44F6-BB0A-9569373B241D', '{93ADF86A-F974-4A6D-9C2C-B37C41626643', '{B73D5D0A-2118-C4E3-9CF9-784A42700000', '{2519F4E1-AEEF-4334-8219-B09E45982519', '{74CF9E8E-952F-4164-8764-F79B61ADEBC5', '{15FBA82B-35AB-47A8-AB1F-2071C02675B6']
source-citation.service.ts:198 [SourceCitationService] citationMetadata type: object
source-citation.service.ts:199 [SourceCitationService] Full metadata object: {{C387180F-5A45-4B5A-96C3-C26F0D11D6CB: {…}, {CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1: {…}, {FA205FD1-7739-C377-84CE-7B63C3A00000: {…}, {293EE29E-A38F-4044-BB23-2F20D35E2300: {…}, {5745EE79-863B-4777-A1C1-3ECF6E4FCFF2: {…}, …}
source-citation.service.ts:205 [SourceCitationService] Direct lookup citationMetadata["{AF8146CC-E8D5-C2A3-9421-71F171C00000}"] = undefined
source-citation.service.ts:210 [SourceCitationService] Trying without braces: AF8146CC-E8D5-C2A3-9421-71F171C00000
source-citation.service.ts:213 [SourceCitationService] Without braces citationMetadata["AF8146CC-E8D5-C2A3-9421-71F171C00000"] = undefined
source-citation.service.ts:248  [SourceCitationService] No metadata found for UUID: {AF8146CC-E8D5-C2A3-9421-71F171C00000}
(anonymous) @ source-citation.service.ts:248
replaceSourceCitationsWithHTML @ source-citation.service.ts:173
(anonymous) @ markdown-content.component.ts:48
run @ resource-DalzMB4W.mjs:205
runEffectsInView @ debug_node-DTOmNMDH.mjs:8756
refreshView @ debug_node-DTOmNMDH.mjs:8916
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInEmbeddedViews @ debug_node-DTOmNMDH.mjs:9025
refreshView @ debug_node-DTOmNMDH.mjs:8917
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
refreshView @ debug_node-DTOmNMDH.mjs:8943
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInEmbeddedViews @ debug_node-DTOmNMDH.mjs:9025
refreshView @ debug_node-DTOmNMDH.mjs:8917
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
detectChangesInView @ debug_node-DTOmNMDH.mjs:9120
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
detectChangesInView @ debug_node-DTOmNMDH.mjs:9120
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInEmbeddedViews @ debug_node-DTOmNMDH.mjs:9025
refreshView @ debug_node-DTOmNMDH.mjs:8917
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
refreshView @ debug_node-DTOmNMDH.mjs:8943
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewWhileDirty @ debug_node-DTOmNMDH.mjs:8797
detectChangesInternal @ debug_node-DTOmNMDH.mjs:8785
synchronizeOnce @ debug_node-DTOmNMDH.mjs:20117
synchronize @ debug_node-DTOmNMDH.mjs:20076
tickImpl @ debug_node-DTOmNMDH.mjs:20049
_tick @ debug_node-DTOmNMDH.mjs:20038
(anonymous) @ debug_node-DTOmNMDH.mjs:29476
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
run @ debug_node-DTOmNMDH.mjs:16342
next @ debug_node-DTOmNMDH.mjs:29473
ConsumerObserver2.next @ Subscriber.js:96
Subscriber2._next @ Subscriber.js:63
Subscriber2.next @ Subscriber.js:34
(anonymous) @ Subject.js:41
errorContext @ errorContext.js:19
Subject2.next @ Subject.js:31
emit @ debug_node-DTOmNMDH.mjs:16030
checkStable @ debug_node-DTOmNMDH.mjs:16410
onHasTask @ debug_node-DTOmNMDH.mjs:16524
hasTask @ zone.js:451
_updateTaskCount @ zone.js:471
_updateTaskCount @ zone.js:266
runTask @ zone.js:179
drainMicroTaskQueue @ zone.js:612
Promise.then
nativeScheduleMicroTask @ zone.js:588
scheduleMicroTask @ zone.js:599
scheduleTask @ zone.js:420
onScheduleTask @ debug_node-DTOmNMDH.mjs:16155
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
source-citation.service.ts:174 [SourceCitationService] Found citation: [Source: {D8805DBE-6C3F-C4BE-9EF2-7B63C3900000}] identifiers: {D8805DBE-6C3F-C4BE-9EF2-7B63C3900000}
source-citation.service.ts:178 [SourceCitationService] hasUUIDs: true citationMetadata exists: true
source-citation.service.ts:195 [SourceCitationService] Looking up UUID: {D8805DBE-6C3F-C4BE-9EF2-7B63C3900000}
source-citation.service.ts:196 [SourceCitationService] Available keys: (5) ['{C387180F-5A45-4B5A-96C3-C26F0D11D6CB', '{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1', '{FA205FD1-7739-C377-84CE-7B63C3A00000', '{293EE29E-A38F-4044-BB23-2F20D35E2300', '{5745EE79-863B-4777-A1C1-3ECF6E4FCFF2']
source-citation.service.ts:197 [SourceCitationService] All available keys: (22) ['{C387180F-5A45-4B5A-96C3-C26F0D11D6CB', '{CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1', '{FA205FD1-7739-C377-84CE-7B63C3A00000', '{293EE29E-A38F-4044-BB23-2F20D35E2300', '{5745EE79-863B-4777-A1C1-3ECF6E4FCFF2', '{9F69364A-E6E6-4D01-A1AC-A7E9A1793E10', '{11AF8B8B-7F17-4101-9649-1094CE3909AA', '{C2DE6CFD-E6A0-4825-8F02-381BA55349B5', '{CBCD67CC-60E7-46EE-811F-D8EB5D07E72A', '{1402769A-E6A7-4DD9-9517-99C1340A645F', '{73FF30FA-FE19-C5C0-9F01-71F171C00000', '{393DDF27-812F-C387-9E24-828469F00000', '{4999779D-E7E3-4E66-A029-046AD2CED1B2', '{AF8146CC-E8D5-C2A3-9421-71F171C00000', '{EBA10DB5-52FE-47FE-87E6-B2ECA9F88F1D', '{D8805DBE-6C3F-C4BE-9EF2-7B63C3900000', '{4F007B96-2C13-44F6-BB0A-9569373B241D', '{93ADF86A-F974-4A6D-9C2C-B37C41626643', '{B73D5D0A-2118-C4E3-9CF9-784A42700000', '{2519F4E1-AEEF-4334-8219-B09E45982519', '{74CF9E8E-952F-4164-8764-F79B61ADEBC5', '{15FBA82B-35AB-47A8-AB1F-2071C02675B6']
source-citation.service.ts:198 [SourceCitationService] citationMetadata type: object
source-citation.service.ts:199 [SourceCitationService] Full metadata object: {{C387180F-5A45-4B5A-96C3-C26F0D11D6CB: {…}, {CB9F6C4E-4FF9-4AF7-B398-5A66F92758B1: {…}, {FA205FD1-7739-C377-84CE-7B63C3A00000: {…}, {293EE29E-A38F-4044-BB23-2F20D35E2300: {…}, {5745EE79-863B-4777-A1C1-3ECF6E4FCFF2: {…}, …}
source-citation.service.ts:205 [SourceCitationService] Direct lookup citationMetadata["{D8805DBE-6C3F-C4BE-9EF2-7B63C3900000}"] = undefined
source-citation.service.ts:210 [SourceCitationService] Trying without braces: D8805DBE-6C3F-C4BE-9EF2-7B63C3900000
source-citation.service.ts:213 [SourceCitationService] Without braces citationMetadata["D8805DBE-6C3F-C4BE-9EF2-7B63C3900000"] = undefined
source-citation.service.ts:248  [SourceCitationService] No metadata found for UUID: {D8805DBE-6C3F-C4BE-9EF2-7B63C3900000}
(anonymous) @ source-citation.service.ts:248
replaceSourceCitationsWithHTML @ source-citation.service.ts:173
(anonymous) @ markdown-content.component.ts:48
run @ resource-DalzMB4W.mjs:205
runEffectsInView @ debug_node-DTOmNMDH.mjs:8756
refreshView @ debug_node-DTOmNMDH.mjs:8916
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInEmbeddedViews @ debug_node-DTOmNMDH.mjs:9025
refreshView @ debug_node-DTOmNMDH.mjs:8917
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
refreshView @ debug_node-DTOmNMDH.mjs:8943
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInEmbeddedViews @ debug_node-DTOmNMDH.mjs:9025
refreshView @ debug_node-DTOmNMDH.mjs:8917
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
detectChangesInView @ debug_node-DTOmNMDH.mjs:9120
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
detectChangesInView @ debug_node-DTOmNMDH.mjs:9120
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInEmbeddedViews @ debug_node-DTOmNMDH.mjs:9025
refreshView @ debug_node-DTOmNMDH.mjs:8917
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewIfAttached @ debug_node-DTOmNMDH.mjs:9068
detectChangesInComponent @ debug_node-DTOmNMDH.mjs:9056
detectChangesInChildComponents @ debug_node-DTOmNMDH.mjs:9134
refreshView @ debug_node-DTOmNMDH.mjs:8943
detectChangesInView @ debug_node-DTOmNMDH.mjs:9108
detectChangesInViewWhileDirty @ debug_node-DTOmNMDH.mjs:8797
detectChangesInternal @ debug_node-DTOmNMDH.mjs:8785
synchronizeOnce @ debug_node-DTOmNMDH.mjs:20117
synchronize @ debug_node-DTOmNMDH.mjs:20076
tickImpl @ debug_node-DTOmNMDH.mjs:20049
_tick @ debug_node-DTOmNMDH.mjs:20038
(anonymous) @ debug_node-DTOmNMDH.mjs:29476
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
run @ debug_node-DTOmNMDH.mjs:16342
next @ debug_node-DTOmNMDH.mjs:29473
ConsumerObserver2.next @ Subscriber.js:96
Subscriber2._next @ Subscriber.js:63
Subscriber2.next @ Subscriber.js:34
(anonymous) @ Subject.js:41
errorContext @ errorContext.js:19
Subject2.next @ Subject.js:31
emit @ debug_node-DTOmNMDH.mjs:16030
checkStable @ debug_node-DTOmNMDH.mjs:16410
onHasTask @ debug_node-DTOmNMDH.mjs:16524
hasTask @ zone.js:451
_updateTaskCount @ zone.js:471
_updateTaskCount @ zone.js:266
runTask @ zone.js:179
drainMicroTaskQueue @ zone.js:612
Promise.then
nativeScheduleMicroTask @ zone.js:588
scheduleMicroTask @ zone.js:599
scheduleTask @ zone.js:420
onScheduleTask @ debug_node-DTOmNMDH.mjs:16155
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
Zone - Promise.then
onScheduleTask @ debug_node-DTOmNMDH.mjs:16154
scheduleTask @ zone.js:411
onScheduleTask @ zone.js:273
scheduleTask @ zone.js:411
scheduleTask @ zone.js:207
scheduleMicroTask @ zone.js:227
scheduleResolveOrReject @ zone.js:2527
resolvePromise @ zone.js:2461
(anonymous) @ zone.js:2369
(anonymous) @ zone.js:2385
Promise.then
(anonymous) @ zone.js:2779
ZoneAwarePromise @ zone.js:2701
Ctor.then @ zone.js:2778
resolvePromise @ zone.js:2422
resolve @ zone.js:2559
step @ main.js:40
fulfilled @ main.js:28
invoke @ zone.js:398
onInvoke @ debug_node-DTOmNMDH.mjs:16496
invoke @ zone.js:397
run @ zone.js:113
(anonymous) @ zone.js:2537
invokeTask @ zone.js:431
(anonymous) @ debug_node-DTOmNMDH.mjs:16160
onInvokeTask @ debug_node-DTOmNMDH.mjs:16160
invokeTask @ zone.js:430
onInvokeTask @ debug_node-DTOmNMDH.mjs:16483
invokeTask @ zone.js:430
runTask @ zone.js:161
drainMicroTaskQueue @ zone.js:612
source-citation.service.ts:345 [SourceCitationService] Processed result (first 200 chars): 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing unescorted access, but the primary procedure that manages the overall unescorted access process is:

**
markdown-content.component.ts:49 [MarkdownContent] After citation processing: 
Based on my search of Palo Verde procedures, there are two main procedures involved in managing unescorted access, but the primary procedure that manages the overall unescorted access process is:

**
markdown-content.component.ts:81 [MarkdownContent] Found 0 citation links to attach handlers to
[NEW] Explain Console errors by using Copilot in Edge: click
         
         to explain an error. 
        Learn more
        Don't show again
